{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Lines\n",
    "\n",
    "Let's use an autoencoder for the Lines dataset. \n",
    "\n",
    "The first experiment imports the 2D data points and casts them into a 1D vector. The (x,y) coordinates are mapped into successive inputs. They are hence correlated... This apparently is tremendously difficult to reconstruct for the autoencoder. Why? In the end it is just a difference between input and output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 784)\n",
      "(20, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import h5py\n",
    "#from matplotlib.mlab import griddata\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "filename = '/home/anne/data/h5/lines.h5'\n",
    "f = h5py.File(filename, 'r')\n",
    "#for key in f.keys():\n",
    "#    print(key)\n",
    "data = np.array(f[\"data\"])\n",
    "\n",
    "def pnts2grid(x, y, resX=28, resY=28):\n",
    "    xrange = np.linspace(0, resX - 1, 1)\n",
    "    yrange = np.linspace(0, resY - 1, 1)\n",
    "    Z = np.zeros((resX, resY))\n",
    "    for i in range(len(x)):\n",
    "        xpixel = round((x[i] - min(x)) / (max(x) - min(x)) * resX - 1).astype('int')\n",
    "        ypixel = round((y[i] - min(y)) / (max(y) - min(y)) * resY - 1).astype('int')\n",
    "        Z[xpixel,ypixel] = 1\n",
    "    return Z\n",
    "\n",
    "# Training and test set are 2D points\n",
    "N = 100\n",
    "resX = resY = 28\n",
    "z = np.zeros((N, resX, resY))\n",
    "for i in range(N):\n",
    "    x = data[i,:,0]\n",
    "    y = data[i,:,1]\n",
    "    z[i,:,:] = pnts2grid(x, y, resX, resY)\n",
    "\n",
    "x_train = z[0:80]\n",
    "x_test  = z[80:N]\n",
    "\n",
    "# Print one particular input to check if the discretization to a 28x28 grid went okay\n",
    "zfig=x_train[1]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(zfig, interpolation='nearest', cmap='Blues',\n",
    "    extent=(0.5,np.shape(zfig)[0]+0.5,0.5,np.shape(zfig)[1]+0.5))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "    \n",
    "# Make them into 32-bit floats\n",
    "#x_train = x_train.astype('float32') / 255.\n",
    "#x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32') \n",
    "\n",
    "# Create 1-dimensional vector of input, the coordinates are just fed into subsequent input nodes\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "input_size = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 784)               8624      \n",
      "=================================================================\n",
      "Total params: 16,474\n",
      "Trainable params: 16,474\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations (was 32)\n",
    "encoding_dim = 10\n",
    "batch_size = 100\n",
    "epochs = 1000\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(input_size,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(input_size, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of optimizers:\n",
    "#  adam\n",
    "#  adagrad\n",
    "#  sgd\n",
    "# types of losses:\n",
    "# - mean_absolute_error (gets all zeros)\n",
    "# - hinge, squared_hinge\n",
    "# - logcosh\n",
    "# - kullback_leibler_divergence (gest all ones)\n",
    "# - binary_crossentropy (adam/adagrad gets somewhere)\n",
    "# - poisson (as well)\n",
    "# - cosine_proximity (vague)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_logarithmic_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1000\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1567 - val_loss: 0.1566\n",
      "Epoch 2/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.1564 - val_loss: 0.1563\n",
      "Epoch 3/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.1561 - val_loss: 0.1560\n",
      "Epoch 4/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.1557 - val_loss: 0.1556\n",
      "Epoch 5/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.1553 - val_loss: 0.1552\n",
      "Epoch 6/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.1548 - val_loss: 0.1546\n",
      "Epoch 7/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.1542 - val_loss: 0.1540\n",
      "Epoch 8/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.1536 - val_loss: 0.1533\n",
      "Epoch 9/1000\n",
      "80/80 [==============================] - 0s 116us/step - loss: 0.1528 - val_loss: 0.1525\n",
      "Epoch 10/1000\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.1519 - val_loss: 0.1515\n",
      "Epoch 11/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.1509 - val_loss: 0.1504\n",
      "Epoch 12/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.1497 - val_loss: 0.1492\n",
      "Epoch 13/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.1485 - val_loss: 0.1479\n",
      "Epoch 14/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.1470 - val_loss: 0.1463\n",
      "Epoch 15/1000\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.1454 - val_loss: 0.1447\n",
      "Epoch 16/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.1437 - val_loss: 0.1428\n",
      "Epoch 17/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.1418 - val_loss: 0.1408\n",
      "Epoch 18/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.1397 - val_loss: 0.1387\n",
      "Epoch 19/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.1375 - val_loss: 0.1363\n",
      "Epoch 20/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.1350 - val_loss: 0.1338\n",
      "Epoch 21/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.1324 - val_loss: 0.1311\n",
      "Epoch 22/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.1296 - val_loss: 0.1282\n",
      "Epoch 23/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.1267 - val_loss: 0.1252\n",
      "Epoch 24/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.1236 - val_loss: 0.1220\n",
      "Epoch 25/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.1203 - val_loss: 0.1187\n",
      "Epoch 26/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.1169 - val_loss: 0.1153\n",
      "Epoch 27/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.1135 - val_loss: 0.1118\n",
      "Epoch 28/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.1099 - val_loss: 0.1081\n",
      "Epoch 29/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.1062 - val_loss: 0.1045\n",
      "Epoch 30/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.1025 - val_loss: 0.1008\n",
      "Epoch 31/1000\n",
      "80/80 [==============================] - 0s 102us/step - loss: 0.0988 - val_loss: 0.0971\n",
      "Epoch 32/1000\n",
      "80/80 [==============================] - 0s 121us/step - loss: 0.0951 - val_loss: 0.0935\n",
      "Epoch 33/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0914 - val_loss: 0.0899\n",
      "Epoch 34/1000\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.0878 - val_loss: 0.0863\n",
      "Epoch 35/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0843 - val_loss: 0.0829\n",
      "Epoch 36/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0809 - val_loss: 0.0796\n",
      "Epoch 37/1000\n",
      "80/80 [==============================] - 0s 122us/step - loss: 0.0776 - val_loss: 0.0764\n",
      "Epoch 38/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0744 - val_loss: 0.0733\n",
      "Epoch 39/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0714 - val_loss: 0.0704\n",
      "Epoch 40/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0685 - val_loss: 0.0677\n",
      "Epoch 41/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0658 - val_loss: 0.0651\n",
      "Epoch 42/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0633 - val_loss: 0.0628\n",
      "Epoch 43/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0610 - val_loss: 0.0606\n",
      "Epoch 44/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0589 - val_loss: 0.0586\n",
      "Epoch 45/1000\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.0570 - val_loss: 0.0567\n",
      "Epoch 46/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0552 - val_loss: 0.0551\n",
      "Epoch 47/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0536 - val_loss: 0.0536\n",
      "Epoch 48/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0522 - val_loss: 0.0522\n",
      "Epoch 49/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0509 - val_loss: 0.0510\n",
      "Epoch 50/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0497 - val_loss: 0.0499\n",
      "Epoch 51/1000\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.0487 - val_loss: 0.0490\n",
      "Epoch 52/1000\n",
      "80/80 [==============================] - 0s 109us/step - loss: 0.0478 - val_loss: 0.0481\n",
      "Epoch 53/1000\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0470 - val_loss: 0.0474\n",
      "Epoch 54/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0463 - val_loss: 0.0468\n",
      "Epoch 55/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0457 - val_loss: 0.0462\n",
      "Epoch 56/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0452 - val_loss: 0.0457\n",
      "Epoch 57/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0447 - val_loss: 0.0452\n",
      "Epoch 58/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0443 - val_loss: 0.0449\n",
      "Epoch 59/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0440 - val_loss: 0.0445\n",
      "Epoch 60/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0437 - val_loss: 0.0442\n",
      "Epoch 61/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0434 - val_loss: 0.0440\n",
      "Epoch 62/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0431 - val_loss: 0.0438\n",
      "Epoch 63/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0429 - val_loss: 0.0436\n",
      "Epoch 64/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0428 - val_loss: 0.0434\n",
      "Epoch 65/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0426 - val_loss: 0.0432\n",
      "Epoch 66/1000\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.0424 - val_loss: 0.0431\n",
      "Epoch 67/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0423 - val_loss: 0.0430\n",
      "Epoch 68/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0422 - val_loss: 0.0429\n",
      "Epoch 69/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0421 - val_loss: 0.0428\n",
      "Epoch 70/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0420 - val_loss: 0.0427\n",
      "Epoch 71/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0419 - val_loss: 0.0427\n",
      "Epoch 72/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0418 - val_loss: 0.0426\n",
      "Epoch 73/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0418 - val_loss: 0.0425\n",
      "Epoch 74/1000\n",
      "80/80 [==============================] - 0s 95us/step - loss: 0.0417 - val_loss: 0.0425\n",
      "Epoch 75/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0417 - val_loss: 0.0424\n",
      "Epoch 76/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0416 - val_loss: 0.0424\n",
      "Epoch 77/1000\n",
      "80/80 [==============================] - 0s 110us/step - loss: 0.0416 - val_loss: 0.0424\n",
      "Epoch 78/1000\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.0415 - val_loss: 0.0423\n",
      "Epoch 79/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0415 - val_loss: 0.0423\n",
      "Epoch 80/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0414 - val_loss: 0.0423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0414 - val_loss: 0.0422\n",
      "Epoch 82/1000\n",
      "80/80 [==============================] - 0s 81us/step - loss: 0.0413 - val_loss: 0.0422\n",
      "Epoch 83/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0413 - val_loss: 0.0422\n",
      "Epoch 84/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0413 - val_loss: 0.0422\n",
      "Epoch 85/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0413 - val_loss: 0.0422\n",
      "Epoch 86/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0412 - val_loss: 0.0421\n",
      "Epoch 87/1000\n",
      "80/80 [==============================] - 0s 81us/step - loss: 0.0412 - val_loss: 0.0421\n",
      "Epoch 88/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0412 - val_loss: 0.0421\n",
      "Epoch 89/1000\n",
      "80/80 [==============================] - 0s 89us/step - loss: 0.0411 - val_loss: 0.0421\n",
      "Epoch 90/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.0411 - val_loss: 0.0421\n",
      "Epoch 91/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0411 - val_loss: 0.0421\n",
      "Epoch 92/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0411 - val_loss: 0.0421\n",
      "Epoch 93/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0410 - val_loss: 0.0420\n",
      "Epoch 94/1000\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.0410 - val_loss: 0.0420\n",
      "Epoch 95/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0410 - val_loss: 0.0420\n",
      "Epoch 96/1000\n",
      "80/80 [==============================] - 0s 110us/step - loss: 0.0410 - val_loss: 0.0420\n",
      "Epoch 97/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0410 - val_loss: 0.0420\n",
      "Epoch 98/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0409 - val_loss: 0.0420\n",
      "Epoch 99/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0409 - val_loss: 0.0420\n",
      "Epoch 100/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0409 - val_loss: 0.0420\n",
      "Epoch 101/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0409 - val_loss: 0.0420\n",
      "Epoch 102/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0409 - val_loss: 0.0420\n",
      "Epoch 103/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0409 - val_loss: 0.0420\n",
      "Epoch 104/1000\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.0408 - val_loss: 0.0420\n",
      "Epoch 105/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0408 - val_loss: 0.0420\n",
      "Epoch 106/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0408 - val_loss: 0.0420\n",
      "Epoch 107/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0408 - val_loss: 0.0420\n",
      "Epoch 108/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0408 - val_loss: 0.0420\n",
      "Epoch 109/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.0408 - val_loss: 0.0420\n",
      "Epoch 110/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0408 - val_loss: 0.0420\n",
      "Epoch 111/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0407 - val_loss: 0.0420\n",
      "Epoch 112/1000\n",
      "80/80 [==============================] - 0s 108us/step - loss: 0.0407 - val_loss: 0.0420\n",
      "Epoch 113/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0407 - val_loss: 0.0420\n",
      "Epoch 114/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.0407 - val_loss: 0.0420\n",
      "Epoch 115/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0407 - val_loss: 0.0420\n",
      "Epoch 116/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0407 - val_loss: 0.0420\n",
      "Epoch 117/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0407 - val_loss: 0.0420\n",
      "Epoch 118/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0407 - val_loss: 0.0420\n",
      "Epoch 119/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.0406 - val_loss: 0.0420\n",
      "Epoch 120/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0406 - val_loss: 0.0420\n",
      "Epoch 121/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0406 - val_loss: 0.0420\n",
      "Epoch 122/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0406 - val_loss: 0.0420\n",
      "Epoch 123/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0406 - val_loss: 0.0420\n",
      "Epoch 124/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0406 - val_loss: 0.0420\n",
      "Epoch 125/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0406 - val_loss: 0.0420\n",
      "Epoch 126/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0406 - val_loss: 0.0420\n",
      "Epoch 127/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.0406 - val_loss: 0.0420\n",
      "Epoch 128/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0406 - val_loss: 0.0420\n",
      "Epoch 129/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 130/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 131/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 132/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 133/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 134/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 135/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 136/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 137/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 138/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 139/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 140/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 141/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 142/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 143/1000\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 144/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 145/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 146/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 147/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 148/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 149/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 150/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 151/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0404 - val_loss: 0.0419\n",
      "Epoch 152/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 153/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 154/1000\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 155/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 156/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 157/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 158/1000\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 159/1000\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 160/1000\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0403 - val_loss: 0.0419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/1000\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 162/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 163/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 164/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0403 - val_loss: 0.0419\n",
      "Epoch 165/1000\n",
      "80/80 [==============================] - 0s 160us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 166/1000\n",
      "80/80 [==============================] - 0s 131us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 167/1000\n",
      "80/80 [==============================] - 0s 107us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 168/1000\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 169/1000\n",
      "80/80 [==============================] - 0s 81us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 170/1000\n",
      "80/80 [==============================] - 0s 121us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 171/1000\n",
      "80/80 [==============================] - 0s 165us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 172/1000\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 173/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 174/1000\n",
      "80/80 [==============================] - 0s 132us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 175/1000\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 176/1000\n",
      "80/80 [==============================] - 0s 132us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 177/1000\n",
      "80/80 [==============================] - 0s 113us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 178/1000\n",
      "80/80 [==============================] - 0s 81us/step - loss: 0.0402 - val_loss: 0.0419\n",
      "Epoch 179/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 180/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 181/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 182/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 183/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 184/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 185/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 186/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 187/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 188/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 189/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 190/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 191/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 192/1000\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 193/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0401 - val_loss: 0.0419\n",
      "Epoch 194/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 195/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 196/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 197/1000\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 198/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 199/1000\n",
      "80/80 [==============================] - 0s 47us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 200/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 201/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 202/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 203/1000\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 204/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 205/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 206/1000\n",
      "80/80 [==============================] - 0s 46us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 207/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 208/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 209/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 210/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 211/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 212/1000\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 213/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 214/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 215/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 216/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 217/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 218/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 219/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 220/1000\n",
      "80/80 [==============================] - 0s 101us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 221/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 222/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 223/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 224/1000\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.0399 - val_loss: 0.0419\n",
      "Epoch 225/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 226/1000\n",
      "80/80 [==============================] - 0s 209us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 227/1000\n",
      "80/80 [==============================] - 0s 107us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 228/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 229/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 230/1000\n",
      "80/80 [==============================] - 0s 117us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 231/1000\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 232/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 233/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 234/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 235/1000\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 236/1000\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 237/1000\n",
      "80/80 [==============================] - 0s 135us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 238/1000\n",
      "80/80 [==============================] - 0s 186us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 239/1000\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.0398 - val_loss: 0.0419\n",
      "Epoch 240/1000\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.0397 - val_loss: 0.0419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 242/1000\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 243/1000\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 244/1000\n",
      "80/80 [==============================] - 0s 174us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 245/1000\n",
      "80/80 [==============================] - 0s 120us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 246/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 247/1000\n",
      "80/80 [==============================] - 0s 192us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 248/1000\n",
      "80/80 [==============================] - 0s 141us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 249/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 250/1000\n",
      "80/80 [==============================] - 0s 147us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 251/1000\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 252/1000\n",
      "80/80 [==============================] - 0s 171us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 253/1000\n",
      "80/80 [==============================] - 0s 164us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 254/1000\n",
      "80/80 [==============================] - 0s 114us/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 255/1000\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 256/1000\n",
      "80/80 [==============================] - 0s 96us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 257/1000\n",
      "80/80 [==============================] - 0s 180us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 258/1000\n",
      "80/80 [==============================] - 0s 218us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 259/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 260/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 261/1000\n",
      "80/80 [==============================] - 0s 117us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 262/1000\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 263/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 264/1000\n",
      "80/80 [==============================] - 0s 129us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 265/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 266/1000\n",
      "80/80 [==============================] - 0s 116us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 267/1000\n",
      "80/80 [==============================] - 0s 132us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 268/1000\n",
      "80/80 [==============================] - 0s 90us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 269/1000\n",
      "80/80 [==============================] - 0s 307us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 270/1000\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 271/1000\n",
      "80/80 [==============================] - 0s 105us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 272/1000\n",
      "80/80 [==============================] - 0s 159us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 273/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 274/1000\n",
      "80/80 [==============================] - 0s 116us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 275/1000\n",
      "80/80 [==============================] - 0s 253us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 276/1000\n",
      "80/80 [==============================] - 0s 168us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 277/1000\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 278/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 279/1000\n",
      "80/80 [==============================] - 0s 126us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 280/1000\n",
      "80/80 [==============================] - 0s 136us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 281/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 282/1000\n",
      "80/80 [==============================] - 0s 95us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 283/1000\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 284/1000\n",
      "80/80 [==============================] - 0s 208us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 285/1000\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 286/1000\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 287/1000\n",
      "80/80 [==============================] - 0s 120us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 288/1000\n",
      "80/80 [==============================] - 0s 108us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 289/1000\n",
      "80/80 [==============================] - 0s 186us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 290/1000\n",
      "80/80 [==============================] - 0s 153us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 291/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 292/1000\n",
      "80/80 [==============================] - 0s 168us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 293/1000\n",
      "80/80 [==============================] - 0s 116us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 294/1000\n",
      "80/80 [==============================] - 0s 172us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 295/1000\n",
      "80/80 [==============================] - 0s 167us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 296/1000\n",
      "80/80 [==============================] - 0s 121us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 297/1000\n",
      "80/80 [==============================] - 0s 104us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 298/1000\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 299/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 300/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 301/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 302/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 303/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 304/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 305/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 306/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 307/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 308/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 309/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 310/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0393 - val_loss: 0.0419\n",
      "Epoch 311/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 312/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 313/1000\n",
      "80/80 [==============================] - 0s 110us/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 314/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 315/1000\n",
      "80/80 [==============================] - 0s 46us/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 316/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 317/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 318/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 319/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 320/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0392 - val_loss: 0.0419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 322/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 323/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0391 - val_loss: 0.0419\n",
      "Epoch 324/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0391 - val_loss: 0.0419\n",
      "Epoch 325/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0391 - val_loss: 0.0419\n",
      "Epoch 326/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: 0.0391 - val_loss: 0.0419\n",
      "Epoch 327/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0391 - val_loss: 0.0419\n",
      "Epoch 328/1000\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.0391 - val_loss: 0.0419\n",
      "Epoch 329/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0391 - val_loss: 0.0419\n",
      "Epoch 330/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0391 - val_loss: 0.0419\n",
      "Epoch 331/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0391 - val_loss: 0.0419\n",
      "Epoch 332/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0391 - val_loss: 0.0419\n",
      "Epoch 333/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0391 - val_loss: 0.0419\n",
      "Epoch 334/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0390 - val_loss: 0.0419\n",
      "Epoch 335/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0390 - val_loss: 0.0419\n",
      "Epoch 336/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0390 - val_loss: 0.0419\n",
      "Epoch 337/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0390 - val_loss: 0.0419\n",
      "Epoch 338/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0390 - val_loss: 0.0419\n",
      "Epoch 339/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0390 - val_loss: 0.0419\n",
      "Epoch 340/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0390 - val_loss: 0.0419\n",
      "Epoch 341/1000\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.0390 - val_loss: 0.0419\n",
      "Epoch 342/1000\n",
      "80/80 [==============================] - 0s 95us/step - loss: 0.0390 - val_loss: 0.0419\n",
      "Epoch 343/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0390 - val_loss: 0.0419\n",
      "Epoch 344/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0390 - val_loss: 0.0419\n",
      "Epoch 345/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 346/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 347/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 348/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 349/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 350/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 351/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 352/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 353/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 354/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 355/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0388 - val_loss: 0.0419\n",
      "Epoch 356/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0388 - val_loss: 0.0419\n",
      "Epoch 357/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0388 - val_loss: 0.0419\n",
      "Epoch 358/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0388 - val_loss: 0.0419\n",
      "Epoch 359/1000\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.0388 - val_loss: 0.0419\n",
      "Epoch 360/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0388 - val_loss: 0.0419\n",
      "Epoch 361/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0388 - val_loss: 0.0419\n",
      "Epoch 362/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0388 - val_loss: 0.0419\n",
      "Epoch 363/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0388 - val_loss: 0.0419\n",
      "Epoch 364/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0388 - val_loss: 0.0419\n",
      "Epoch 365/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0387 - val_loss: 0.0419\n",
      "Epoch 366/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0387 - val_loss: 0.0419\n",
      "Epoch 367/1000\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0387 - val_loss: 0.0419\n",
      "Epoch 368/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0387 - val_loss: 0.0419\n",
      "Epoch 369/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0387 - val_loss: 0.0419\n",
      "Epoch 370/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0387 - val_loss: 0.0419\n",
      "Epoch 371/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0387 - val_loss: 0.0419\n",
      "Epoch 372/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0387 - val_loss: 0.0419\n",
      "Epoch 373/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0387 - val_loss: 0.0419\n",
      "Epoch 374/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: 0.0386 - val_loss: 0.0419\n",
      "Epoch 375/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0386 - val_loss: 0.0419\n",
      "Epoch 376/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0386 - val_loss: 0.0419\n",
      "Epoch 377/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0386 - val_loss: 0.0419\n",
      "Epoch 378/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0386 - val_loss: 0.0419\n",
      "Epoch 379/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0386 - val_loss: 0.0419\n",
      "Epoch 380/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0386 - val_loss: 0.0419\n",
      "Epoch 381/1000\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.0386 - val_loss: 0.0419\n",
      "Epoch 382/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0385 - val_loss: 0.0419\n",
      "Epoch 383/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0385 - val_loss: 0.0419\n",
      "Epoch 384/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0385 - val_loss: 0.0419\n",
      "Epoch 385/1000\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.0385 - val_loss: 0.0419\n",
      "Epoch 386/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0385 - val_loss: 0.0419\n",
      "Epoch 387/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0385 - val_loss: 0.0419\n",
      "Epoch 388/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0385 - val_loss: 0.0419\n",
      "Epoch 389/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0385 - val_loss: 0.0419\n",
      "Epoch 390/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0384 - val_loss: 0.0419\n",
      "Epoch 391/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0384 - val_loss: 0.0419\n",
      "Epoch 392/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0384 - val_loss: 0.0419\n",
      "Epoch 393/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0384 - val_loss: 0.0419\n",
      "Epoch 394/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0384 - val_loss: 0.0419\n",
      "Epoch 395/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0384 - val_loss: 0.0419\n",
      "Epoch 396/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0384 - val_loss: 0.0419\n",
      "Epoch 397/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0384 - val_loss: 0.0419\n",
      "Epoch 398/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0383 - val_loss: 0.0419\n",
      "Epoch 399/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0383 - val_loss: 0.0419\n",
      "Epoch 400/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0383 - val_loss: 0.0419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0383 - val_loss: 0.0419\n",
      "Epoch 402/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0383 - val_loss: 0.0419\n",
      "Epoch 403/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0383 - val_loss: 0.0419\n",
      "Epoch 404/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0383 - val_loss: 0.0419\n",
      "Epoch 405/1000\n",
      "80/80 [==============================] - 0s 193us/step - loss: 0.0383 - val_loss: 0.0419\n",
      "Epoch 406/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0382 - val_loss: 0.0419\n",
      "Epoch 407/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0382 - val_loss: 0.0419\n",
      "Epoch 408/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0382 - val_loss: 0.0419\n",
      "Epoch 409/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0382 - val_loss: 0.0419\n",
      "Epoch 410/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0382 - val_loss: 0.0419\n",
      "Epoch 411/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0382 - val_loss: 0.0419\n",
      "Epoch 412/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0382 - val_loss: 0.0419\n",
      "Epoch 413/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0381 - val_loss: 0.0419\n",
      "Epoch 414/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0381 - val_loss: 0.0419\n",
      "Epoch 415/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0381 - val_loss: 0.0419\n",
      "Epoch 416/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0381 - val_loss: 0.0419\n",
      "Epoch 417/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0381 - val_loss: 0.0419\n",
      "Epoch 418/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0381 - val_loss: 0.0419\n",
      "Epoch 419/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0381 - val_loss: 0.0419\n",
      "Epoch 420/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0380 - val_loss: 0.0419\n",
      "Epoch 421/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0380 - val_loss: 0.0419\n",
      "Epoch 422/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0380 - val_loss: 0.0419\n",
      "Epoch 423/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0380 - val_loss: 0.0419\n",
      "Epoch 424/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0380 - val_loss: 0.0419\n",
      "Epoch 425/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0380 - val_loss: 0.0419\n",
      "Epoch 426/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0380 - val_loss: 0.0419\n",
      "Epoch 427/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0379 - val_loss: 0.0419\n",
      "Epoch 428/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0379 - val_loss: 0.0419\n",
      "Epoch 429/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0379 - val_loss: 0.0419\n",
      "Epoch 430/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0379 - val_loss: 0.0419\n",
      "Epoch 431/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0379 - val_loss: 0.0419\n",
      "Epoch 432/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0379 - val_loss: 0.0419\n",
      "Epoch 433/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0378 - val_loss: 0.0419\n",
      "Epoch 434/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0378 - val_loss: 0.0419\n",
      "Epoch 435/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0378 - val_loss: 0.0419\n",
      "Epoch 436/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0378 - val_loss: 0.0419\n",
      "Epoch 437/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0378 - val_loss: 0.0419\n",
      "Epoch 438/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0378 - val_loss: 0.0419\n",
      "Epoch 439/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0378 - val_loss: 0.0419\n",
      "Epoch 440/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0377 - val_loss: 0.0419\n",
      "Epoch 441/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0377 - val_loss: 0.0419\n",
      "Epoch 442/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0377 - val_loss: 0.0419\n",
      "Epoch 443/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0377 - val_loss: 0.0419\n",
      "Epoch 444/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0377 - val_loss: 0.0419\n",
      "Epoch 445/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0377 - val_loss: 0.0419\n",
      "Epoch 446/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0376 - val_loss: 0.0419\n",
      "Epoch 447/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0376 - val_loss: 0.0419\n",
      "Epoch 448/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0376 - val_loss: 0.0419\n",
      "Epoch 449/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0376 - val_loss: 0.0419\n",
      "Epoch 450/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0376 - val_loss: 0.0419\n",
      "Epoch 451/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0376 - val_loss: 0.0419\n",
      "Epoch 452/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0375 - val_loss: 0.0419\n",
      "Epoch 453/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0375 - val_loss: 0.0419\n",
      "Epoch 454/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0375 - val_loss: 0.0419\n",
      "Epoch 455/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0375 - val_loss: 0.0419\n",
      "Epoch 456/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0375 - val_loss: 0.0419\n",
      "Epoch 457/1000\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0375 - val_loss: 0.0419\n",
      "Epoch 458/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0374 - val_loss: 0.0419\n",
      "Epoch 459/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0374 - val_loss: 0.0419\n",
      "Epoch 460/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0374 - val_loss: 0.0419\n",
      "Epoch 461/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0374 - val_loss: 0.0419\n",
      "Epoch 462/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0374 - val_loss: 0.0419\n",
      "Epoch 463/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0374 - val_loss: 0.0419\n",
      "Epoch 464/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0373 - val_loss: 0.0419\n",
      "Epoch 465/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0373 - val_loss: 0.0419\n",
      "Epoch 466/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0373 - val_loss: 0.0419\n",
      "Epoch 467/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0373 - val_loss: 0.0419\n",
      "Epoch 468/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0373 - val_loss: 0.0419\n",
      "Epoch 469/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0373 - val_loss: 0.0419\n",
      "Epoch 470/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0372 - val_loss: 0.0419\n",
      "Epoch 471/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0372 - val_loss: 0.0419\n",
      "Epoch 472/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0372 - val_loss: 0.0419\n",
      "Epoch 473/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0372 - val_loss: 0.0419\n",
      "Epoch 474/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0372 - val_loss: 0.0419\n",
      "Epoch 475/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0371 - val_loss: 0.0419\n",
      "Epoch 476/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0371 - val_loss: 0.0419\n",
      "Epoch 477/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0371 - val_loss: 0.0419\n",
      "Epoch 478/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0371 - val_loss: 0.0419\n",
      "Epoch 479/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0371 - val_loss: 0.0419\n",
      "Epoch 480/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0371 - val_loss: 0.0419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0370 - val_loss: 0.0419\n",
      "Epoch 482/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0370 - val_loss: 0.0418\n",
      "Epoch 483/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0370 - val_loss: 0.0418\n",
      "Epoch 484/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0370 - val_loss: 0.0418\n",
      "Epoch 485/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0370 - val_loss: 0.0418\n",
      "Epoch 486/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0369 - val_loss: 0.0418\n",
      "Epoch 487/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0369 - val_loss: 0.0418\n",
      "Epoch 488/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0369 - val_loss: 0.0418\n",
      "Epoch 489/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0369 - val_loss: 0.0418\n",
      "Epoch 490/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0369 - val_loss: 0.0418\n",
      "Epoch 491/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0368 - val_loss: 0.0418\n",
      "Epoch 492/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: 0.0368 - val_loss: 0.0418\n",
      "Epoch 493/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0368 - val_loss: 0.0418\n",
      "Epoch 494/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0368 - val_loss: 0.0418\n",
      "Epoch 495/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0368 - val_loss: 0.0418\n",
      "Epoch 496/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0368 - val_loss: 0.0418\n",
      "Epoch 497/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0367 - val_loss: 0.0418\n",
      "Epoch 498/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0367 - val_loss: 0.0418\n",
      "Epoch 499/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0367 - val_loss: 0.0418\n",
      "Epoch 500/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0367 - val_loss: 0.0418\n",
      "Epoch 501/1000\n",
      "80/80 [==============================] - 0s 98us/step - loss: 0.0367 - val_loss: 0.0418\n",
      "Epoch 502/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0366 - val_loss: 0.0418\n",
      "Epoch 503/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0366 - val_loss: 0.0418\n",
      "Epoch 504/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0366 - val_loss: 0.0418\n",
      "Epoch 505/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0366 - val_loss: 0.0418\n",
      "Epoch 506/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0366 - val_loss: 0.0418\n",
      "Epoch 507/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0365 - val_loss: 0.0418\n",
      "Epoch 508/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0365 - val_loss: 0.0418\n",
      "Epoch 509/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0365 - val_loss: 0.0418\n",
      "Epoch 510/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0365 - val_loss: 0.0418\n",
      "Epoch 511/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0365 - val_loss: 0.0418\n",
      "Epoch 512/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0364 - val_loss: 0.0418\n",
      "Epoch 513/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0364 - val_loss: 0.0418\n",
      "Epoch 514/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0364 - val_loss: 0.0418\n",
      "Epoch 515/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0364 - val_loss: 0.0417\n",
      "Epoch 516/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0364 - val_loss: 0.0417\n",
      "Epoch 517/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0363 - val_loss: 0.0417\n",
      "Epoch 518/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0363 - val_loss: 0.0417\n",
      "Epoch 519/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0363 - val_loss: 0.0417\n",
      "Epoch 520/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0363 - val_loss: 0.0417\n",
      "Epoch 521/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0363 - val_loss: 0.0417\n",
      "Epoch 522/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0362 - val_loss: 0.0417\n",
      "Epoch 523/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0362 - val_loss: 0.0417\n",
      "Epoch 524/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0362 - val_loss: 0.0417\n",
      "Epoch 525/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0362 - val_loss: 0.0417\n",
      "Epoch 526/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0362 - val_loss: 0.0417\n",
      "Epoch 527/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0361 - val_loss: 0.0417\n",
      "Epoch 528/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0361 - val_loss: 0.0417\n",
      "Epoch 529/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0361 - val_loss: 0.0417\n",
      "Epoch 530/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0361 - val_loss: 0.0417\n",
      "Epoch 531/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0361 - val_loss: 0.0417\n",
      "Epoch 532/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0360 - val_loss: 0.0417\n",
      "Epoch 533/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0360 - val_loss: 0.0417\n",
      "Epoch 534/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0360 - val_loss: 0.0417\n",
      "Epoch 535/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0360 - val_loss: 0.0417\n",
      "Epoch 536/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0359 - val_loss: 0.0417\n",
      "Epoch 537/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0359 - val_loss: 0.0417\n",
      "Epoch 538/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0359 - val_loss: 0.0417\n",
      "Epoch 539/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0359 - val_loss: 0.0417\n",
      "Epoch 540/1000\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.0359 - val_loss: 0.0416\n",
      "Epoch 541/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0358 - val_loss: 0.0416\n",
      "Epoch 542/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0358 - val_loss: 0.0416\n",
      "Epoch 543/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0358 - val_loss: 0.0416\n",
      "Epoch 544/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0358 - val_loss: 0.0416\n",
      "Epoch 545/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0358 - val_loss: 0.0416\n",
      "Epoch 546/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0357 - val_loss: 0.0416\n",
      "Epoch 547/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0357 - val_loss: 0.0416\n",
      "Epoch 548/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0357 - val_loss: 0.0416\n",
      "Epoch 549/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0357 - val_loss: 0.0416\n",
      "Epoch 550/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0357 - val_loss: 0.0416\n",
      "Epoch 551/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0356 - val_loss: 0.0416\n",
      "Epoch 552/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0356 - val_loss: 0.0416\n",
      "Epoch 553/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0356 - val_loss: 0.0416\n",
      "Epoch 554/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0356 - val_loss: 0.0416\n",
      "Epoch 555/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0355 - val_loss: 0.0416\n",
      "Epoch 556/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0355 - val_loss: 0.0416\n",
      "Epoch 557/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0355 - val_loss: 0.0416\n",
      "Epoch 558/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0355 - val_loss: 0.0416\n",
      "Epoch 559/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0355 - val_loss: 0.0416\n",
      "Epoch 560/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0354 - val_loss: 0.0416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 561/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0354 - val_loss: 0.0416\n",
      "Epoch 562/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0354 - val_loss: 0.0416\n",
      "Epoch 563/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0354 - val_loss: 0.0416\n",
      "Epoch 564/1000\n",
      "80/80 [==============================] - 0s 111us/step - loss: 0.0353 - val_loss: 0.0415\n",
      "Epoch 565/1000\n",
      "80/80 [==============================] - 0s 109us/step - loss: 0.0353 - val_loss: 0.0415\n",
      "Epoch 566/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0353 - val_loss: 0.0415\n",
      "Epoch 567/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0353 - val_loss: 0.0415\n",
      "Epoch 568/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0353 - val_loss: 0.0415\n",
      "Epoch 569/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0352 - val_loss: 0.0415\n",
      "Epoch 570/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: 0.0352 - val_loss: 0.0415\n",
      "Epoch 571/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0352 - val_loss: 0.0415\n",
      "Epoch 572/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0352 - val_loss: 0.0415\n",
      "Epoch 573/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0352 - val_loss: 0.0415\n",
      "Epoch 574/1000\n",
      "80/80 [==============================] - 0s 46us/step - loss: 0.0351 - val_loss: 0.0415\n",
      "Epoch 575/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0351 - val_loss: 0.0415\n",
      "Epoch 576/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0351 - val_loss: 0.0415\n",
      "Epoch 577/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0351 - val_loss: 0.0415\n",
      "Epoch 578/1000\n",
      "80/80 [==============================] - 0s 190us/step - loss: 0.0350 - val_loss: 0.0415\n",
      "Epoch 579/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0350 - val_loss: 0.0415\n",
      "Epoch 580/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0350 - val_loss: 0.0415\n",
      "Epoch 581/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0350 - val_loss: 0.0415\n",
      "Epoch 582/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0350 - val_loss: 0.0415\n",
      "Epoch 583/1000\n",
      "80/80 [==============================] - 0s 95us/step - loss: 0.0349 - val_loss: 0.0415\n",
      "Epoch 584/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0349 - val_loss: 0.0415\n",
      "Epoch 585/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0349 - val_loss: 0.0415\n",
      "Epoch 586/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0349 - val_loss: 0.0415\n",
      "Epoch 587/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0348 - val_loss: 0.0414\n",
      "Epoch 588/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0348 - val_loss: 0.0414\n",
      "Epoch 589/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0348 - val_loss: 0.0414\n",
      "Epoch 590/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0348 - val_loss: 0.0414\n",
      "Epoch 591/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0347 - val_loss: 0.0414\n",
      "Epoch 592/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0347 - val_loss: 0.0414\n",
      "Epoch 593/1000\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.0347 - val_loss: 0.0414\n",
      "Epoch 594/1000\n",
      "80/80 [==============================] - 0s 81us/step - loss: 0.0347 - val_loss: 0.0414\n",
      "Epoch 595/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0347 - val_loss: 0.0414\n",
      "Epoch 596/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0346 - val_loss: 0.0414\n",
      "Epoch 597/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0346 - val_loss: 0.0414\n",
      "Epoch 598/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0346 - val_loss: 0.0414\n",
      "Epoch 599/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0346 - val_loss: 0.0414\n",
      "Epoch 600/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0345 - val_loss: 0.0414\n",
      "Epoch 601/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0345 - val_loss: 0.0414\n",
      "Epoch 602/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0345 - val_loss: 0.0414\n",
      "Epoch 603/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0345 - val_loss: 0.0414\n",
      "Epoch 604/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.0345 - val_loss: 0.0414\n",
      "Epoch 605/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0344 - val_loss: 0.0414\n",
      "Epoch 606/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0344 - val_loss: 0.0414\n",
      "Epoch 607/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0344 - val_loss: 0.0414\n",
      "Epoch 608/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0344 - val_loss: 0.0414\n",
      "Epoch 609/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0343 - val_loss: 0.0414\n",
      "Epoch 610/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0343 - val_loss: 0.0414\n",
      "Epoch 611/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.0343 - val_loss: 0.0413\n",
      "Epoch 612/1000\n",
      "80/80 [==============================] - 0s 109us/step - loss: 0.0343 - val_loss: 0.0413\n",
      "Epoch 613/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0342 - val_loss: 0.0413\n",
      "Epoch 614/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0342 - val_loss: 0.0413\n",
      "Epoch 615/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0342 - val_loss: 0.0413\n",
      "Epoch 616/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0342 - val_loss: 0.0413\n",
      "Epoch 617/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0342 - val_loss: 0.0413\n",
      "Epoch 618/1000\n",
      "80/80 [==============================] - 0s 107us/step - loss: 0.0341 - val_loss: 0.0413\n",
      "Epoch 619/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0341 - val_loss: 0.0413\n",
      "Epoch 620/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0341 - val_loss: 0.0413\n",
      "Epoch 621/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0341 - val_loss: 0.0413\n",
      "Epoch 622/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0340 - val_loss: 0.0413\n",
      "Epoch 623/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: 0.0340 - val_loss: 0.0413\n",
      "Epoch 624/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0340 - val_loss: 0.0413\n",
      "Epoch 625/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.0340 - val_loss: 0.0413\n",
      "Epoch 626/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0339 - val_loss: 0.0413\n",
      "Epoch 627/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0339 - val_loss: 0.0413\n",
      "Epoch 628/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0339 - val_loss: 0.0413\n",
      "Epoch 629/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0339 - val_loss: 0.0413\n",
      "Epoch 630/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0339 - val_loss: 0.0413\n",
      "Epoch 631/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0338 - val_loss: 0.0413\n",
      "Epoch 632/1000\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.0338 - val_loss: 0.0413\n",
      "Epoch 633/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0338 - val_loss: 0.0413\n",
      "Epoch 634/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0338 - val_loss: 0.0413\n",
      "Epoch 635/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0337 - val_loss: 0.0412\n",
      "Epoch 636/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0337 - val_loss: 0.0412\n",
      "Epoch 637/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0337 - val_loss: 0.0412\n",
      "Epoch 638/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0337 - val_loss: 0.0412\n",
      "Epoch 639/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0336 - val_loss: 0.0412\n",
      "Epoch 640/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0336 - val_loss: 0.0412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 641/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0336 - val_loss: 0.0412\n",
      "Epoch 642/1000\n",
      "80/80 [==============================] - 0s 118us/step - loss: 0.0336 - val_loss: 0.0412\n",
      "Epoch 643/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0335 - val_loss: 0.0412\n",
      "Epoch 644/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.0335 - val_loss: 0.0412\n",
      "Epoch 645/1000\n",
      "80/80 [==============================] - 0s 129us/step - loss: 0.0335 - val_loss: 0.0412\n",
      "Epoch 646/1000\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.0335 - val_loss: 0.0412\n",
      "Epoch 647/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0335 - val_loss: 0.0412\n",
      "Epoch 648/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0334 - val_loss: 0.0412\n",
      "Epoch 649/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0334 - val_loss: 0.0412\n",
      "Epoch 650/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0334 - val_loss: 0.0412\n",
      "Epoch 651/1000\n",
      "80/80 [==============================] - 0s 120us/step - loss: 0.0334 - val_loss: 0.0412\n",
      "Epoch 652/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0333 - val_loss: 0.0412\n",
      "Epoch 653/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0333 - val_loss: 0.0412\n",
      "Epoch 654/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0333 - val_loss: 0.0412\n",
      "Epoch 655/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0333 - val_loss: 0.0412\n",
      "Epoch 656/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0332 - val_loss: 0.0412\n",
      "Epoch 657/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0332 - val_loss: 0.0412\n",
      "Epoch 658/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0332 - val_loss: 0.0412\n",
      "Epoch 659/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0332 - val_loss: 0.0411\n",
      "Epoch 660/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0331 - val_loss: 0.0411\n",
      "Epoch 661/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0331 - val_loss: 0.0411\n",
      "Epoch 662/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0331 - val_loss: 0.0411\n",
      "Epoch 663/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0331 - val_loss: 0.0411\n",
      "Epoch 664/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0331 - val_loss: 0.0411\n",
      "Epoch 665/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0330 - val_loss: 0.0411\n",
      "Epoch 666/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0330 - val_loss: 0.0411\n",
      "Epoch 667/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0330 - val_loss: 0.0411\n",
      "Epoch 668/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0330 - val_loss: 0.0411\n",
      "Epoch 669/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0329 - val_loss: 0.0411\n",
      "Epoch 670/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0329 - val_loss: 0.0411\n",
      "Epoch 671/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0329 - val_loss: 0.0411\n",
      "Epoch 672/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0329 - val_loss: 0.0411\n",
      "Epoch 673/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0328 - val_loss: 0.0411\n",
      "Epoch 674/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0328 - val_loss: 0.0411\n",
      "Epoch 675/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0328 - val_loss: 0.0411\n",
      "Epoch 676/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0328 - val_loss: 0.0411\n",
      "Epoch 677/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0327 - val_loss: 0.0411\n",
      "Epoch 678/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0327 - val_loss: 0.0411\n",
      "Epoch 679/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0327 - val_loss: 0.0411\n",
      "Epoch 680/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0327 - val_loss: 0.0411\n",
      "Epoch 681/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0326 - val_loss: 0.0411\n",
      "Epoch 682/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0326 - val_loss: 0.0411\n",
      "Epoch 683/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0326 - val_loss: 0.0411\n",
      "Epoch 684/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0326 - val_loss: 0.0411\n",
      "Epoch 685/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0326 - val_loss: 0.0410\n",
      "Epoch 686/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0325 - val_loss: 0.0410\n",
      "Epoch 687/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0325 - val_loss: 0.0410\n",
      "Epoch 688/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0325 - val_loss: 0.0410\n",
      "Epoch 689/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0325 - val_loss: 0.0410\n",
      "Epoch 690/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0324 - val_loss: 0.0410\n",
      "Epoch 691/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0324 - val_loss: 0.0410\n",
      "Epoch 692/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: 0.0324 - val_loss: 0.0410\n",
      "Epoch 693/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0324 - val_loss: 0.0410\n",
      "Epoch 694/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0323 - val_loss: 0.0410\n",
      "Epoch 695/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0323 - val_loss: 0.0410\n",
      "Epoch 696/1000\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.0323 - val_loss: 0.0410\n",
      "Epoch 697/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0323 - val_loss: 0.0410\n",
      "Epoch 698/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0322 - val_loss: 0.0410\n",
      "Epoch 699/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0322 - val_loss: 0.0410\n",
      "Epoch 700/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0322 - val_loss: 0.0410\n",
      "Epoch 701/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0322 - val_loss: 0.0410\n",
      "Epoch 702/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0321 - val_loss: 0.0410\n",
      "Epoch 703/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0321 - val_loss: 0.0410\n",
      "Epoch 704/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0321 - val_loss: 0.0410\n",
      "Epoch 705/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0321 - val_loss: 0.0410\n",
      "Epoch 706/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0321 - val_loss: 0.0410\n",
      "Epoch 707/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0320 - val_loss: 0.0410\n",
      "Epoch 708/1000\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.0320 - val_loss: 0.0410\n",
      "Epoch 709/1000\n",
      "80/80 [==============================] - 0s 89us/step - loss: 0.0320 - val_loss: 0.0410\n",
      "Epoch 710/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.0320 - val_loss: 0.0410\n",
      "Epoch 711/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0319 - val_loss: 0.0410\n",
      "Epoch 712/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0319 - val_loss: 0.0410\n",
      "Epoch 713/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0319 - val_loss: 0.0409\n",
      "Epoch 714/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0319 - val_loss: 0.0409\n",
      "Epoch 715/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0318 - val_loss: 0.0409\n",
      "Epoch 716/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0318 - val_loss: 0.0409\n",
      "Epoch 717/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0318 - val_loss: 0.0409\n",
      "Epoch 718/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0318 - val_loss: 0.0409\n",
      "Epoch 719/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0317 - val_loss: 0.0409\n",
      "Epoch 720/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0317 - val_loss: 0.0409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 721/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0317 - val_loss: 0.0409\n",
      "Epoch 722/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0317 - val_loss: 0.0409\n",
      "Epoch 723/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0317 - val_loss: 0.0409\n",
      "Epoch 724/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0316 - val_loss: 0.0409\n",
      "Epoch 725/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0316 - val_loss: 0.0409\n",
      "Epoch 726/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0316 - val_loss: 0.0409\n",
      "Epoch 727/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0316 - val_loss: 0.0409\n",
      "Epoch 728/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: 0.0315 - val_loss: 0.0409\n",
      "Epoch 729/1000\n",
      "80/80 [==============================] - 0s 95us/step - loss: 0.0315 - val_loss: 0.0409\n",
      "Epoch 730/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0315 - val_loss: 0.0409\n",
      "Epoch 731/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0315 - val_loss: 0.0409\n",
      "Epoch 732/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0314 - val_loss: 0.0409\n",
      "Epoch 733/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0314 - val_loss: 0.0409\n",
      "Epoch 734/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0314 - val_loss: 0.0409\n",
      "Epoch 735/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0314 - val_loss: 0.0409\n",
      "Epoch 736/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0313 - val_loss: 0.0409\n",
      "Epoch 737/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.0313 - val_loss: 0.0409\n",
      "Epoch 738/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0313 - val_loss: 0.0409\n",
      "Epoch 739/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0313 - val_loss: 0.0409\n",
      "Epoch 740/1000\n",
      "80/80 [==============================] - 0s 205us/step - loss: 0.0313 - val_loss: 0.0409\n",
      "Epoch 741/1000\n",
      "80/80 [==============================] - 0s 152us/step - loss: 0.0312 - val_loss: 0.0409\n",
      "Epoch 742/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0312 - val_loss: 0.0409\n",
      "Epoch 743/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0312 - val_loss: 0.0408\n",
      "Epoch 744/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0312 - val_loss: 0.0408\n",
      "Epoch 745/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0311 - val_loss: 0.0408\n",
      "Epoch 746/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0311 - val_loss: 0.0408\n",
      "Epoch 747/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0311 - val_loss: 0.0408\n",
      "Epoch 748/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0311 - val_loss: 0.0408\n",
      "Epoch 749/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0310 - val_loss: 0.0408\n",
      "Epoch 750/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0310 - val_loss: 0.0408\n",
      "Epoch 751/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0310 - val_loss: 0.0408\n",
      "Epoch 752/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0310 - val_loss: 0.0408\n",
      "Epoch 753/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0310 - val_loss: 0.0408\n",
      "Epoch 754/1000\n",
      "80/80 [==============================] - 0s 90us/step - loss: 0.0309 - val_loss: 0.0408\n",
      "Epoch 755/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0309 - val_loss: 0.0408\n",
      "Epoch 756/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0309 - val_loss: 0.0408\n",
      "Epoch 757/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0309 - val_loss: 0.0408\n",
      "Epoch 758/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0308 - val_loss: 0.0408\n",
      "Epoch 759/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0308 - val_loss: 0.0408\n",
      "Epoch 760/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0308 - val_loss: 0.0408\n",
      "Epoch 761/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0308 - val_loss: 0.0408\n",
      "Epoch 762/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0308 - val_loss: 0.0408\n",
      "Epoch 763/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.0307 - val_loss: 0.0408\n",
      "Epoch 764/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0307 - val_loss: 0.0408\n",
      "Epoch 765/1000\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.0307 - val_loss: 0.0408\n",
      "Epoch 766/1000\n",
      "80/80 [==============================] - 0s 115us/step - loss: 0.0307 - val_loss: 0.0408\n",
      "Epoch 767/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0306 - val_loss: 0.0408\n",
      "Epoch 768/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0306 - val_loss: 0.0408\n",
      "Epoch 769/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0306 - val_loss: 0.0408\n",
      "Epoch 770/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0306 - val_loss: 0.0408\n",
      "Epoch 771/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0305 - val_loss: 0.0408\n",
      "Epoch 772/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0305 - val_loss: 0.0408\n",
      "Epoch 773/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0305 - val_loss: 0.0408\n",
      "Epoch 774/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0305 - val_loss: 0.0408\n",
      "Epoch 775/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0305 - val_loss: 0.0408\n",
      "Epoch 776/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0304 - val_loss: 0.0408\n",
      "Epoch 777/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0304 - val_loss: 0.0408\n",
      "Epoch 778/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0304 - val_loss: 0.0408\n",
      "Epoch 779/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0304 - val_loss: 0.0408\n",
      "Epoch 780/1000\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.0303 - val_loss: 0.0407\n",
      "Epoch 781/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0303 - val_loss: 0.0407\n",
      "Epoch 782/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0303 - val_loss: 0.0407\n",
      "Epoch 783/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0303 - val_loss: 0.0407\n",
      "Epoch 784/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0303 - val_loss: 0.0407\n",
      "Epoch 785/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0302 - val_loss: 0.0407\n",
      "Epoch 786/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0302 - val_loss: 0.0407\n",
      "Epoch 787/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0302 - val_loss: 0.0407\n",
      "Epoch 788/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0302 - val_loss: 0.0407\n",
      "Epoch 789/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0301 - val_loss: 0.0407\n",
      "Epoch 790/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0301 - val_loss: 0.0407\n",
      "Epoch 791/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0301 - val_loss: 0.0407\n",
      "Epoch 792/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0301 - val_loss: 0.0407\n",
      "Epoch 793/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0301 - val_loss: 0.0407\n",
      "Epoch 794/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0300 - val_loss: 0.0407\n",
      "Epoch 795/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0300 - val_loss: 0.0407\n",
      "Epoch 796/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0300 - val_loss: 0.0407\n",
      "Epoch 797/1000\n",
      "80/80 [==============================] - 0s 47us/step - loss: 0.0300 - val_loss: 0.0407\n",
      "Epoch 798/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0299 - val_loss: 0.0407\n",
      "Epoch 799/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0299 - val_loss: 0.0407\n",
      "Epoch 800/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0299 - val_loss: 0.0407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 801/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0299 - val_loss: 0.0407\n",
      "Epoch 802/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0299 - val_loss: 0.0407\n",
      "Epoch 803/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0298 - val_loss: 0.0407\n",
      "Epoch 804/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0298 - val_loss: 0.0407\n",
      "Epoch 805/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0298 - val_loss: 0.0407\n",
      "Epoch 806/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0298 - val_loss: 0.0407\n",
      "Epoch 807/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0297 - val_loss: 0.0407\n",
      "Epoch 808/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0297 - val_loss: 0.0407\n",
      "Epoch 809/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0297 - val_loss: 0.0407\n",
      "Epoch 810/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0297 - val_loss: 0.0407\n",
      "Epoch 811/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0297 - val_loss: 0.0407\n",
      "Epoch 812/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0296 - val_loss: 0.0407\n",
      "Epoch 813/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0296 - val_loss: 0.0407\n",
      "Epoch 814/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0296 - val_loss: 0.0407\n",
      "Epoch 815/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0296 - val_loss: 0.0407\n",
      "Epoch 816/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0296 - val_loss: 0.0407\n",
      "Epoch 817/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0295 - val_loss: 0.0407\n",
      "Epoch 818/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0295 - val_loss: 0.0407\n",
      "Epoch 819/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0295 - val_loss: 0.0407\n",
      "Epoch 820/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0295 - val_loss: 0.0407\n",
      "Epoch 821/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0294 - val_loss: 0.0407\n",
      "Epoch 822/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: 0.0294 - val_loss: 0.0407\n",
      "Epoch 823/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0294 - val_loss: 0.0407\n",
      "Epoch 824/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.0294 - val_loss: 0.0407\n",
      "Epoch 825/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0294 - val_loss: 0.0407\n",
      "Epoch 826/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0293 - val_loss: 0.0407\n",
      "Epoch 827/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0293 - val_loss: 0.0406\n",
      "Epoch 828/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0293 - val_loss: 0.0406\n",
      "Epoch 829/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0293 - val_loss: 0.0406\n",
      "Epoch 830/1000\n",
      "80/80 [==============================] - 0s 95us/step - loss: 0.0293 - val_loss: 0.0406\n",
      "Epoch 831/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0292 - val_loss: 0.0406\n",
      "Epoch 832/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0292 - val_loss: 0.0406\n",
      "Epoch 833/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0292 - val_loss: 0.0406\n",
      "Epoch 834/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0292 - val_loss: 0.0406\n",
      "Epoch 835/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0291 - val_loss: 0.0406\n",
      "Epoch 836/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0291 - val_loss: 0.0406\n",
      "Epoch 837/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0291 - val_loss: 0.0406\n",
      "Epoch 838/1000\n",
      "80/80 [==============================] - 0s 104us/step - loss: 0.0291 - val_loss: 0.0406\n",
      "Epoch 839/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0291 - val_loss: 0.0406\n",
      "Epoch 840/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0290 - val_loss: 0.0406\n",
      "Epoch 841/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0290 - val_loss: 0.0406\n",
      "Epoch 842/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0290 - val_loss: 0.0406\n",
      "Epoch 843/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0290 - val_loss: 0.0406\n",
      "Epoch 844/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0290 - val_loss: 0.0406\n",
      "Epoch 845/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0289 - val_loss: 0.0406\n",
      "Epoch 846/1000\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.0289 - val_loss: 0.0406\n",
      "Epoch 847/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0289 - val_loss: 0.0406\n",
      "Epoch 848/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0289 - val_loss: 0.0406\n",
      "Epoch 849/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0289 - val_loss: 0.0406\n",
      "Epoch 850/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0288 - val_loss: 0.0406\n",
      "Epoch 851/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0288 - val_loss: 0.0406\n",
      "Epoch 852/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0288 - val_loss: 0.0406\n",
      "Epoch 853/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0288 - val_loss: 0.0406\n",
      "Epoch 854/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0287 - val_loss: 0.0406\n",
      "Epoch 855/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0287 - val_loss: 0.0406\n",
      "Epoch 856/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0287 - val_loss: 0.0406\n",
      "Epoch 857/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0287 - val_loss: 0.0406\n",
      "Epoch 858/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0287 - val_loss: 0.0406\n",
      "Epoch 859/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0286 - val_loss: 0.0406\n",
      "Epoch 860/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0286 - val_loss: 0.0406\n",
      "Epoch 861/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0286 - val_loss: 0.0406\n",
      "Epoch 862/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0286 - val_loss: 0.0406\n",
      "Epoch 863/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0286 - val_loss: 0.0406\n",
      "Epoch 864/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0285 - val_loss: 0.0406\n",
      "Epoch 865/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0285 - val_loss: 0.0406\n",
      "Epoch 866/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0285 - val_loss: 0.0406\n",
      "Epoch 867/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0285 - val_loss: 0.0406\n",
      "Epoch 868/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0285 - val_loss: 0.0406\n",
      "Epoch 869/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0284 - val_loss: 0.0406\n",
      "Epoch 870/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0284 - val_loss: 0.0406\n",
      "Epoch 871/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0284 - val_loss: 0.0406\n",
      "Epoch 872/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0284 - val_loss: 0.0406\n",
      "Epoch 873/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0284 - val_loss: 0.0406\n",
      "Epoch 874/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0283 - val_loss: 0.0406\n",
      "Epoch 875/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0283 - val_loss: 0.0406\n",
      "Epoch 876/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0283 - val_loss: 0.0406\n",
      "Epoch 877/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0283 - val_loss: 0.0406\n",
      "Epoch 878/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0283 - val_loss: 0.0406\n",
      "Epoch 879/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0282 - val_loss: 0.0406\n",
      "Epoch 880/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: 0.0282 - val_loss: 0.0406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 881/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0282 - val_loss: 0.0406\n",
      "Epoch 882/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0282 - val_loss: 0.0406\n",
      "Epoch 883/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0282 - val_loss: 0.0406\n",
      "Epoch 884/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0281 - val_loss: 0.0406\n",
      "Epoch 885/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0281 - val_loss: 0.0406\n",
      "Epoch 886/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0281 - val_loss: 0.0406\n",
      "Epoch 887/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0281 - val_loss: 0.0406\n",
      "Epoch 888/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0281 - val_loss: 0.0406\n",
      "Epoch 889/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0280 - val_loss: 0.0406\n",
      "Epoch 890/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0280 - val_loss: 0.0406\n",
      "Epoch 891/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0280 - val_loss: 0.0406\n",
      "Epoch 892/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0280 - val_loss: 0.0406\n",
      "Epoch 893/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0280 - val_loss: 0.0406\n",
      "Epoch 894/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0279 - val_loss: 0.0406\n",
      "Epoch 895/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0279 - val_loss: 0.0406\n",
      "Epoch 896/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0279 - val_loss: 0.0406\n",
      "Epoch 897/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0279 - val_loss: 0.0406\n",
      "Epoch 898/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0279 - val_loss: 0.0405\n",
      "Epoch 899/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0278 - val_loss: 0.0405\n",
      "Epoch 900/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0278 - val_loss: 0.0405\n",
      "Epoch 901/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0278 - val_loss: 0.0405\n",
      "Epoch 902/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0278 - val_loss: 0.0405\n",
      "Epoch 903/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0278 - val_loss: 0.0405\n",
      "Epoch 904/1000\n",
      "80/80 [==============================] - 0s 122us/step - loss: 0.0278 - val_loss: 0.0405\n",
      "Epoch 905/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0277 - val_loss: 0.0405\n",
      "Epoch 906/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0277 - val_loss: 0.0405\n",
      "Epoch 907/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0277 - val_loss: 0.0405\n",
      "Epoch 908/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0277 - val_loss: 0.0405\n",
      "Epoch 909/1000\n",
      "80/80 [==============================] - 0s 115us/step - loss: 0.0277 - val_loss: 0.0405\n",
      "Epoch 910/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0276 - val_loss: 0.0405\n",
      "Epoch 911/1000\n",
      "80/80 [==============================] - 0s 127us/step - loss: 0.0276 - val_loss: 0.0405\n",
      "Epoch 912/1000\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.0276 - val_loss: 0.0405\n",
      "Epoch 913/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0276 - val_loss: 0.0405\n",
      "Epoch 914/1000\n",
      "80/80 [==============================] - 0s 101us/step - loss: 0.0276 - val_loss: 0.0405\n",
      "Epoch 915/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0275 - val_loss: 0.0405\n",
      "Epoch 916/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0275 - val_loss: 0.0405\n",
      "Epoch 917/1000\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.0275 - val_loss: 0.0405\n",
      "Epoch 918/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0275 - val_loss: 0.0405\n",
      "Epoch 919/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0275 - val_loss: 0.0405\n",
      "Epoch 920/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0274 - val_loss: 0.0405\n",
      "Epoch 921/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0274 - val_loss: 0.0405\n",
      "Epoch 922/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0274 - val_loss: 0.0405\n",
      "Epoch 923/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0274 - val_loss: 0.0405\n",
      "Epoch 924/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0274 - val_loss: 0.0405\n",
      "Epoch 925/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0274 - val_loss: 0.0405\n",
      "Epoch 926/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0273 - val_loss: 0.0405\n",
      "Epoch 927/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0273 - val_loss: 0.0405\n",
      "Epoch 928/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0273 - val_loss: 0.0405\n",
      "Epoch 929/1000\n",
      "80/80 [==============================] - 0s 90us/step - loss: 0.0273 - val_loss: 0.0405\n",
      "Epoch 930/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0273 - val_loss: 0.0405\n",
      "Epoch 931/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0272 - val_loss: 0.0405\n",
      "Epoch 932/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0272 - val_loss: 0.0405\n",
      "Epoch 933/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0272 - val_loss: 0.0405\n",
      "Epoch 934/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0272 - val_loss: 0.0405\n",
      "Epoch 935/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0272 - val_loss: 0.0405\n",
      "Epoch 936/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0272 - val_loss: 0.0405\n",
      "Epoch 937/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0271 - val_loss: 0.0405\n",
      "Epoch 938/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0271 - val_loss: 0.0405\n",
      "Epoch 939/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0271 - val_loss: 0.0405\n",
      "Epoch 940/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0271 - val_loss: 0.0405\n",
      "Epoch 941/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0271 - val_loss: 0.0405\n",
      "Epoch 942/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.0270 - val_loss: 0.0405\n",
      "Epoch 943/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0270 - val_loss: 0.0405\n",
      "Epoch 944/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0270 - val_loss: 0.0405\n",
      "Epoch 945/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0270 - val_loss: 0.0405\n",
      "Epoch 946/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0270 - val_loss: 0.0405\n",
      "Epoch 947/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0270 - val_loss: 0.0405\n",
      "Epoch 948/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0269 - val_loss: 0.0405\n",
      "Epoch 949/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0269 - val_loss: 0.0405\n",
      "Epoch 950/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0269 - val_loss: 0.0405\n",
      "Epoch 951/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: 0.0269 - val_loss: 0.0405\n",
      "Epoch 952/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0269 - val_loss: 0.0405\n",
      "Epoch 953/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0268 - val_loss: 0.0405\n",
      "Epoch 954/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0268 - val_loss: 0.0405\n",
      "Epoch 955/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0268 - val_loss: 0.0405\n",
      "Epoch 956/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0268 - val_loss: 0.0405\n",
      "Epoch 957/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0268 - val_loss: 0.0405\n",
      "Epoch 958/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0268 - val_loss: 0.0405\n",
      "Epoch 959/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0267 - val_loss: 0.0405\n",
      "Epoch 960/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0267 - val_loss: 0.0405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 961/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0267 - val_loss: 0.0405\n",
      "Epoch 962/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0267 - val_loss: 0.0405\n",
      "Epoch 963/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0267 - val_loss: 0.0405\n",
      "Epoch 964/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0267 - val_loss: 0.0405\n",
      "Epoch 965/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0266 - val_loss: 0.0405\n",
      "Epoch 966/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0266 - val_loss: 0.0405\n",
      "Epoch 967/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0266 - val_loss: 0.0405\n",
      "Epoch 968/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0266 - val_loss: 0.0405\n",
      "Epoch 969/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0266 - val_loss: 0.0405\n",
      "Epoch 970/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0265 - val_loss: 0.0405\n",
      "Epoch 971/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0265 - val_loss: 0.0405\n",
      "Epoch 972/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0265 - val_loss: 0.0405\n",
      "Epoch 973/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0265 - val_loss: 0.0405\n",
      "Epoch 974/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0265 - val_loss: 0.0405\n",
      "Epoch 975/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0265 - val_loss: 0.0405\n",
      "Epoch 976/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0264 - val_loss: 0.0405\n",
      "Epoch 977/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0264 - val_loss: 0.0405\n",
      "Epoch 978/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: 0.0264 - val_loss: 0.0405\n",
      "Epoch 979/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0264 - val_loss: 0.0405\n",
      "Epoch 980/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0264 - val_loss: 0.0405\n",
      "Epoch 981/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0264 - val_loss: 0.0405\n",
      "Epoch 982/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0263 - val_loss: 0.0405\n",
      "Epoch 983/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.0263 - val_loss: 0.0405\n",
      "Epoch 984/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0263 - val_loss: 0.0405\n",
      "Epoch 985/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0263 - val_loss: 0.0405\n",
      "Epoch 986/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: 0.0263 - val_loss: 0.0405\n",
      "Epoch 987/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0263 - val_loss: 0.0405\n",
      "Epoch 988/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0262 - val_loss: 0.0405\n",
      "Epoch 989/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0262 - val_loss: 0.0405\n",
      "Epoch 990/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0262 - val_loss: 0.0405\n",
      "Epoch 991/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0262 - val_loss: 0.0405\n",
      "Epoch 992/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0262 - val_loss: 0.0405\n",
      "Epoch 993/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.0262 - val_loss: 0.0405\n",
      "Epoch 994/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0261 - val_loss: 0.0405\n",
      "Epoch 995/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0261 - val_loss: 0.0405\n",
      "Epoch 996/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: 0.0261 - val_loss: 0.0405\n",
      "Epoch 997/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0261 - val_loss: 0.0405\n",
      "Epoch 998/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0261 - val_loss: 0.0405\n",
      "Epoch 999/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0261 - val_loss: 0.0405\n",
      "Epoch 1000/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0260 - val_loss: 0.0405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f747fc5a208>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the autoencoder should learn to map the x_train input data to x_train as output\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode lines\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACEUAAAHICAYAAACiF8beAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzs3X2spNd9H/bfuXff+brkyrJMcknZphspNgIZXMmFkNqwLYROAfEPGwltpLULp24Nb+vGSQAZDZRCQYDaRV20XSYpm6pKDSSKEyDFFmaiuKnswETELG3KLyTLmKXNN7+QSy1f93339A9e2VfLe849O/eZe+bMfD7CADv3zPM6w/N9zjM/nUk55wAAAAAAAAAAWDZrvXcAAAAAAAAAAGAeFEUAAAAAAAAAAEtJUQQAAAAAAAAAsJQURQAAAAAAAAAAS0lRBAAAAAAAAACwlBRFAAAAAAAAAABLSVEELIGU0mdTSq+klH670J5SSv9TSunZlNJvppS+fbf3EQAWhdwEgHZyEwDayU0AaLPbmakoApbD5yLi/kr790XEvRuPH4uIv7sL+wQAi+pzITcBoNXnQm4CQKvPhdwEgBafi13MTEURsARyzv86Ir5SeckDEfF/5Hd9KSJuTSl9YHf2DgAWi9wEgHZyEwDayU0AaLPbmbmn1njkyJF89933zLpuWEq//uu/djrn/L4p1rV+8905Xz637evyuVefjIjzm/70cM754evY1B0R8eKm5y9t/O0PrmMdwDbk5vSeePqFYttHPnR0YdZJmdwESuaRm/p4Rvb8878Xp0+fTlOtT27CcjHeXF2ub7Y2ZW7uYmZGyE2YO5nJqqpdM+Rzr678PdpqUcTdd98Tjz72+CzrhaV1cG96fqp15cvnYv+/9xe2fd35Lz90Pud831TbBeZDbk7v8LHjxbZHHzuxMOukTG4CJfPITX08I/v4x6aNLrkJy8V4c3W5vtnalLkpM2G5yExWVe2a4fyXH1r5e7TVoghgzlKKWFvfjS29HBF3bXp+58bfAGAcchMA2slNAGize5kZITcBGN2gY821He8OsDNpbfvHzp2MiP84ves7IuKNnLMp2QAYj9wEgHZyEwDa7E5mRshNAJbBgGNNM0VAb2nnP32XUvpHEfFdEXEkpfRSRPzNiNgbEZFz/nsR8UhE/PmIeDYizkbEf7LjjQJAD3ITANrJTQBoM0FmvrsauQnAChhwrKkoArpKk1RL5Zx/cJv2HBE/seMNAUBXchMA2slNAGgzTWZGyE0AVsGYY01FEdBTit38vToAGJvcBIB2chMA2shMAGg3aG7OXBRx+NjxYtuZUydmXS2smDTZ1GwAy6h2TTHrtYjrlJHJTaBOHw+byU0AaCMzYZk88fQLxfuGxowss9rn++Dehybc0pi5aaYI6G2iqdkAYCXITQBoJzcBoI3MBIB2A+amogjoKg05xQwA9CE3AaCd3ASANjITANqNmZuKIqCnFENOMQMAXchNAGgnNwGgjcwEgHaD5qaiCOhtwClmAKAbuQkA7eQmALSRmQDQbsDcVBQBXaUhOw4A6ENuAkA7uQkAbWQmALQbMzcVRUBPKSLWx/vdHQDoQm4CQDu5CQBtZCYAtBs0N2cuijhz6sSU+8GSOHzs+MzLruxnasDf3QGAbuQmLI0nnn6hOH5Y2bEBK6H0ub/wzAvTb0xuArusdm9Qvs/GedslMhMA2g2Ym2aKgK7GnGIGAPqQmwDQTm4CQBuZCQDtxsxNRRHQ29p4U8wAQDdyEwDayU0AaCMzAaDdgLmpKAJ6SmnIKWYAoAu5CQDt5CYAtJGZANBu0NxUFAG9DTjFDAB0IzcBoJ3cBIA2MhMA2g2Ym4oioKs05BQzANCH3ASAdnITANrITABoN2ZuKoqA3gacYgYAupGbANBObgJAG5kJAO0GzE1FERWHjx0vtp05dWIX92Sx1M5LzSqfs6IUQ04xAwBdyE1YGcZiMAG5CXQgp2fj2qczmQkrQ38LExg0NxVFQFdpyI4DAPqQmwDQTm4CQBuZCQDtxsxNRRHQ24C/uwMA3chNAGgnNwGgjcwEgHYD5qaiCOhtwN/dAYBu5CYAtJObANBGZgJAuwFzU1EE9JTGnGIGALqQmwDQTm4CQBuZCQDtBs1NRRHQWVobr+MAgF7kJgC0k5sA0EZmAkC7EXNTUQR0lCIiDTjFDAD0IDcBoJ3cBIA2MhMA2o2am4oiKs6cOtF7F7o5fOz4TMut8jmbSdp4ADCpWo7JqoHJTSD08Yyv9Dn9+Me+NO2G5CYAtJGZsFQ+8qGj8ehjW19zz/rdF6tru8/MSt6HGDQ3FUVAVynWBpxiBgD6kJsA0E5uAkAbmQkA7cbMTUUR0NmIU8wAQC9yEwDayU0AaCMzAaDdiLmpKAI6G7HjAIBe5CYAtJObANBGZgJAuxFzU1EE9DTo7+4AQBdyEwDayU0AaCMzAaDdoLmpKAI6SoP+7g4A9CA3AaCd3ASANjITANqNmpuKIqCzEaeYAYBe5CYAtJObANBGZgJAuxFzU1HECjt87PhMy505dWLiPVltI3YcAItAHq0muQnL4yMfOhqPPrZ1Xz7rWAX4WnITgNGVrgsvPPPCpNuRmQCrq3YPwj3orY2Ym4oioKcUkdbG6zgAoAu5CQDt5CYAtJGZANBu0Nwc7wc/YImkSJHS9o+mdaV0f0rpmZTSsymlT23RfjSl9MWU0hMppd9MKf35yQ8IAOZoqtyUmQCsArkJAG3cowWAdqPmpqII6Gyim1TrEfFQRHxfRHw4In4wpfTha172NyLiF3LOH4mIByPi70x8KAAwdzvNTZkJwCqRmwDQxj1aAGg3Ym4qioDeUsNjex+NiGdzzs/lnC9GxOcj4oFrXpMj4uaNf98SEb+/010HgF2389yUmQCsDrkJAG3cowWAdgPm5p5ZFwQmkCLW1ppqk46klB7f9PzhnPPDm57fEREvbnr+UkR87Jp1/DcR8S9TSv9FRNwQEd97/TsMAB1Nk5syE4DVIDcBoI17tADQbtDcVBQBnTX+rs7pnPN9O9zUD0bE53LO/31K6d+PiJ9PKX1rzvnqDtcLALtml3JTZgKwFOQmALRxjxYA2o2Ym4oiltzhY8dnWu7MqRMT7wlbSdH2uzoNXo6IuzY9v3Pjb5v9aETcHxGRc/43KaUDEXEkIl6ZYgcAFkkt/2TcuCbKTZkJA5i1r9b/w5+Qm7Bcnnj6hWLOrXrGjZL/o+znKnKPFojQT68y7+/1GTU3m+a2AOYkRaS1tO2jwamIuDel9MGU0r6IeDAiTl7zmhci4nsiIlJKH4qIAxHx6oRHAwDzNU1uykwAVoPcBIA27tECQLtBc9NMEdDZFNVUOefLKaXjEfGFiFiPiM/mnJ9MKX0mIh7POZ+MiL8aEf9rSumvRESOiB/JOecdbxwAdtFOc1NmArBK5CYAtHGPFgDajZibiiKgs4mmmImc8yMR8cg1f/v0pn8/FREfn2RjANDJRBfcMhOAlSA3AaCNe7QA0G7E3FQUAZ01TiEDAITcBIDrITcBoI3MBIB2I+amogjoKKU0WTUVACw7uQkA7eQmALSRmQDQbtTcVBQBnY3YcQBAL3ITANrJTQBoIzMBoN2IuTlMUcThY8eLbWdOndjFPXnXIu1PbV9qepw33mvEjgNgZIuU4Vw/uQnM2o/r/1lFchNWw7LcG5w1q1c5/5f9+CLKx/Hxj31p0u3ITFgNs2YG8LVGzM1hiiJgWY34uzsA0IvcBIB2chMA2shMAGg3Ym4qioCe0pjVVADQhdwEgHZyEwDayEwAaDdobiqKgI5SRAzYbwBAF3ITANrJTQBoIzMBoN2ouakoArpKsTbgFDMA0IfcBIB2chMA2shMAGg3Zm4qioDORpxiBgB6kZsA0E5uAkAbmQkA7UbMTUUR0FMac4oZAOhCbgJAO7kJAG1kJgC0GzQ3FUVARyliyClmAKAHuQkA7eQmALSRmQDQbtTcHKYo4sypE8W2w8eOz2W981iutq/zOMZZ95PdM2LHAbDo5N/ykpvAPPr4WcdpvdYLreQmLI+PfOhoPPrY9WdHLYsWLafmsc3dPo6d3KeeheuJ6chMoGbRMhN6GzE3hymKgKU06BQzANCF3ASAdnITANrITABoN2huKoqAjlJEpBF7DgDoQG4CQDu5CQBtZCYAtBs1NxVFQFdpyClmAKAPuQkA7eQmALSRmQDQbszcVBQBnY1YTQUAvchNAGgnNwGgjcwEgHYj5qaiCOhp0N/dAYAu5CYAtJObANBGZgJAu0FzU1EEdJQihpxiBgB6kJsA0E5uAkAbmQkA7UbNzaUoijhz6kSx7fCx43PZZm29tf2ZdV9ryzG2EaeYAVh0MnV5yU1YHk88/UKxv55HXz3rWGwnmSJz6E1uAvPIv1m318OsOT6PMeWinRu+lswEenzXyPhW9T70iLm5FEURMLIB+w0A6EZuAkA7uQkAbWQmALQbMTcVRUBPacxqKgDoQm4CQDu5CQBtZCYAtBs0NxVFQEcp0pC/uwMAPchNAGgnNwGgjcwEgHaj5qaiCOhswGIqAOhGbgJAO7kJAG1kJgC0GzE3FUVAZyNOMQMAvchNAGgnNwGgjcwEgHYj5qaiCOgopRhyihkA6EFuAkA7uQkAbWQmALQbNTeXvijizKkT1fbDx47PtOx26516e7Mux+IbsZoKYBHI1NUkN2F5fORDR+PRxxaj3501N2ptO1mvPGIqchOomcf9zR75tl0ez0IWrx6ZCcAsVvWaYcTcXPqiCFh0A/YbANCN3ASAdnITANrITABoN2JuKoqAzkaspgKAXuQmALSTmwDQRmYCQLsRc1NRBHSUUhryd3cAoAe5CQDt5CYAtJGZANBu1NxUFAGdDVhMBQDdyE0AaCc3AaCNzASAdiPm5lrvHYBVt5bSto8WKaX7U0rPpJSeTSl9qvCav5BSeiql9GRK6R9OeiAAsAumyE2ZCcCqkJsA0MY9WgBoN2JumikCOkopJpliJqW0HhEPRcQnIuKliDiVUjqZc35q02vujYifjoiP55zPpJS+bscbBoBdNEVuykwAVoXcBIA27tECQLtRc3PliyLOnDox03KHjx3f1e3NutxIdvucLoqJfnbnoxHxbM75uYiIlNLnI+KBiHhq02v+04h4KOd8JiIi5/zKJFsGGMzoubFTs+ZtzW6e0wlyU2YC7zGPvnG79a56HrE75CYwq3ncp9tJ3tbWK1OZgnu0QM2s+Tav7yBlH72NmJt+PgM6Sylt+4iIIymlxzc9fuya1dwRES9uev7Sxt82+5aI+JaU0qMppS+llO6f31EBwHxMkJsyE4CVITcBoI17tADQbsTcXPmZIqCnFNH6uzqnc8737XBzeyLi3oj4roi4MyL+dUrp23LOr+9wvQCwK3YxN2UmAMOTmwDQxj1aAGg3am6aKQI6W0vbPxq8HBF3bXp+58bfNnspIk7mnC/lnH83Iv5dvNuRAMAwJshNmQnAypCbANDGPVoAaDdibiqKgJ4appdJbdVWpyLi3pTSB1NK+yLiwYg4ec1r/s94t5IqUkpH4t0pZ56b7mAAYM6myU2ZCcBqkJsA0MY9WgBoN2huKoqAzlLa/rGdnPPliDgeEV+IiKcj4hdyzk+mlD6TUvrkxsu+EBGvpZSeiogvRsRfzzm/Np+jAoD52GluykwAVoncBIA27tECQLsRc3PPLAsB00gRsd44h8x2cs6PRMQj1/zt05v+nSPipzYeADCcqXJTZgKwCuQmALRxjxYA2o2am4oi5uDMqRO9d2FIq3reGqeQAWAXHD52vNi22zlV25ceFiWn5SYwD/Pq42p9+SJlDstLbsLyeOLpF4rZMY/c6LHOWXNzJ9uchQxfTjITWCTyZHo98nvR7u9OacTcVBQBHbVOIQMAyE0AuB5yEwDayEwAaDdqbiqKgM7WR+w5AKATuQkA7eQmALSRmQDQbsTcVBQBnY04xQwA9CI3AaCd3ASANjITANqNmJuKIqCjFBFr4/UbANCF3ASAdnITANrITABoN2puKoqAnlKKtRF7DgDoQW4CQDu5CQBtZCYAtBs0NxVFQGcjTjEDAL3ITQBoJzcBoI3MBIB2I+bmyhdFHD52fKblzpw6savrnFVtX+axPa7PqFPMAKtrkTJukXJs1vNSs0jHtyjkJjCaWceNxnFMQW7CcvnIh47Go48tdwbMmnEylZ2SmcC8yKHdNes92nnc293OPK57dsuoubnyRRHQ24jVVADQi9wEgHZyEwDayEwAaDdibiqKgI5SilgfsOMAgB7kJgC0k5sA0EZmAkC7UXNTUQR0NmC/AQDdyE0AaCc3AaCNzASAdiPmpqII6GzEKWYAoBe5CQDt5CYAtJGZANBuxNxUFAEdpUixvjZexwEAPchNAGgnNwGgjcwEgHaj5qaiCOgpjTnFDAB0ITcBoJ3cBIA2MhMA2g2am0tfFHH42PGZlz1z6sSuLjcPi7QvbG3EKWaAsfXIxkUy6/HPutwynLNFIjeBRbJdNtQyoNZWW2+tbZTMWYZjGIXcBFbBrNlhjMdmMhOY1axju1WwSMe/LPk963Ec3PvQpPsxYm4ufVEELLq13jsAAAORmwDQTm4CQBuZCQDtRsxNRRHQUYoY8nd3AKAHuQkA7eQmALSRmQDQbtTcVBQBnQ3YbwBAN3ITANrJTQBoIzMBoN2IuakoAjpKaczf3QGAHuQmALSTmwDQRmYCQLtRc1NRBHS2PuIP7wBAJ3ITANrJTQBoIzMBoN2IuakoAjpKEbE2YDUVAPQgNwGgndwEgDYyEwDajZqbS1EUcfjY8ZmXPXPqxIR78q7a/sxje4xtwGIqYHCLlkU7yfFZLNrxc33kJrBItsuUWceGtbbaOkcZi856fPOySOdmanIToGweeTvr9uhPZgLLoseYahZycWwj5uZSFEXAqFJKsb42XjUVAPQgNwGgndwEgDYyEwDajZqbiiKgswFnmAGAbuQmALSTmwDQRmYCQLsRc1NRBHQ2YDEVAHQjNwGgndwEgDYyEwDajZibiiKgoxQRayOWUwFAB3ITANrJTQBoIzMBoN2ouakoAnpKEetrvXcCAAYhNwGgndwEgDYyEwDaDZqbiiKgsxTjVVMBQC9yEwDayU0AaCMzAaDdiLk5TFHE4WPHZ1ruzKkTE+9Jn22OdPy0e3eKmd57AUzliadfKPbXi9Qfz5opEfXj2Ml6Z7FI57Smdl5GOYZFITeB0cyam7XlZl3nKHm0SPuynUU/p3ITYD7mkcWzbo9pyEygh92+lxohU5jGqLk5TFEELKv1EXsOAOhEbgJAO7kJAG1kJgC0GzE3FUVAR6NWUwFAD3ITANrJTQBoIzMBoN2oubnWewdgpaWI1PBoWlVK96eUnkkpPZtS+lTldd+fUsoppfumOgwA2BUT5abMBGAlyE0AaOMeLQC0GzQ3zRQBna219gwVKaX1iHgoIj4RES9FxKmU0smc81PXvO6miPjJiHhsxxsFgA52mpsyE4BVIjcBoI17tADQbsTcNFMEdJQiYn1t+0eDj0bEsznn53LOFyPi8xHxwBav+1sR8TMRcX6qYwCA3TJRbspMAFaC3ASANu7RAkC7UXNTUQR0lWKt4RERR1JKj296/Ng1K7ojIl7c9Pyljb/9yZZS+vaIuCvn/ItzPSQAmJtJclNmArAi5CYAtHGPFgDajZmbC/XzGYePHZ9puTOnTky8J4undoyznjf6S9H8uzqnc84z/05OSmktIn4uIn5k1nUA2/vIh47Go48tfibtJFNk9dZq52XZjz1i965FdiM3ZSZwPbbr/2oZMI98MG7cXbO+h6X34sIzL+xkd95DbgLUzWMcN3U2bNe2CuPN3eAeLTAvs47R9O8sslFzc6GKImDlpIg9azv/3Z2IeDki7tr0/M6Nv33VTRHxrRHxy+ndnurrI+JkSumTOefHp9gBAJi7aXJTZgKwGuQmALRxjxYA2g2am4oioKPrqKbazqmIuDel9MF4t8N4MCJ+6KuNOec3IuLIH283pV+OiL/mYhuAkUyUmzITgJUgNwGgjXu0ANBu1NxUFAGdrU3Qc+ScL6eUjkfEFyJiPSI+m3N+MqX0mYh4POd8cscbAYAFsNPclJkArBK5CQBt3KMFgHYj5qaiCOgoRcT6NNVUkXN+JCIeueZvny689rum2SoA7J6pclNmArAK5CYAtHGPFgDajZqbiiKgpxSRJppjBgCWntwEgHZyEwDayEwAaDdobiqKgM7G6zYAoB+5CQDt5CYAtJGZANBuxNzc9aKIw8eOz7TcmVMnJt6T+akd4zyOY9Z1rsJ7sehSTPO7O8DymrWvnpU+nlnUPjcH9z402XbkJsDsan31bo9hV13pnH78Y1+adDtyE2A+5pGb7u/2JTOBRbNd/64fp6dRc9NMEdDZ2nj9BgB0IzcBoJ3cBIA2MhMA2o2Ym4oioKs05O/uAEAfchMA2slNAGgjMwGg3Zi5qSgCOkoRsdZ7JwBgEHITANrJTQBoIzMBoN2ouakoAjob8Xd3AKAXuQkA7eQmALSRmQDQbsTcVBQBPaUYcooZAOhCbgJAO7kJAG1kJgC0GzQ3FUVAR6NOMQMAPchNAGgnNwGgjcwEgHaj5uZciiIOHztebDtz6sQ8NrlQase4SOdm1v1kWiNWUwG7ZxVycx4WKW+ZltwEFslImTJrNsrUsclNgOktUv7NmuE7ufe7SMc/JZkJ7Dbf0e0uY9tpjZibZoqAztbG6zcAoBu5CQDt5CYAtJGZANBuxNxUFAEdvTvFzIA9BwB0IDcBoJ3cBIA2MhMA2o2am4oioLMBZ5gBgG7kJgC0k5sA0EZmAkC7EXNTUQR0lWJtxJ4DALqQmwDQTm4CQBuZCQDtxsxNRRHQ0ahTzABAD3ITANrJTQBoIzMBoN2ouakoAnpKY04xAwBdyE0AaCc3AaCNzASAdoPm5sxFEYePHZ9yP1bGmVMneu9Ck9p+1t77UY5vkYw4xQzAolukPFqF3NzN60K5Cay6WXNlHpmzChk3OrkJrLpVzqqdHF/tvC3rOZWZwCLZrj9d1r54npyXaY2Ym2aKgI5SRKyN128AQBdyEwDayU0AaCMzAaDdqLmpKAI6SwP+7g4A9CI3AaCd3ASANjITANqNmJuKIqCzAWeYAYBu5CYAtJObANBGZgJAuxFzU1EEdJQiYn3EngMAOpCbANBObgJAG5kJAO1GzU1FEdBVGnKKGQDoQ24CQDu5CQBtZCYAtBszNxVFQE9pzClmAKALuQkA7eQmALSRmQDQbtDcrBZFPPH0C3H42PHd2hcGcebUiWJb7fNSW26ny45q1ClmALay2/34IuVGj5yaxzXarBm/W+QmsExmzbF5jMfmsdwi5fSqkpsAzGrV8l9mAqNZpHEhq2fU3DRTBHQ2XrcBAP3ITQBoJzcBoI3MBIB2I+amogjobcSeAwB6kZsA0E5uAkAbmQkA7QbMTUUR0NnagFPMAEAvchMA2slNAGgjMwGg3Yi5qSgCOhuv2wCAfuQmALSTmwDQRmYCQLsRc1NRBPQ2Ys8BAL3ITQBoJzcBoI3MBIB2A+amogjoKEVEGrHnAIAO5CYAtJObANBGZgJAu1Fzc633DsBKSxFrDY+mVaV0f0rpmZTSsymlT23R/lMppadSSr+ZUvpXKaW7pz4cAJiriXJTZgKwEuQmALRxjxYA2g2am9WZIj7yoaPx6GMnZl03Wzh87Hix7cyp8c/1To6htuxun7fa9iY3QTFVSmk9Ih6KiE9ExEsRcSqldDLn/NSmlz0REfflnM+mlH48In42Iv7izrcO8K5lyLF5mDVTtjufu32+Z93ewb0PTbsjO8xNmQksinn047s9ppp1e8s+Ll4ochNgYSxL/o20r9fFPVpgILs9voP3GDA3zRQBXaWm/zX4aEQ8m3N+Lud8MSI+HxEPbH5BzvmLOeezG0+/FBF3TnooADB3k+SmzARgRchNAGjjHi0AtBszNxVFQEcpJpti5o6IeHHT85c2/lbyoxHxz2fecQDoYKLclJkArAS5CQBt3KMFgHaj5mb15zOAXdDWMRxJKT2+6fnDOeeHZ9pcSn8pIu6LiO+cZXkA6GoXc1NmAjA8uQkAbdyjBYB2A+amogjorHEKmdM55/sq7S9HxF2bnt+58bev3VZK3xsR/3VEfGfO+cL17CcALIIJclNmArAy5CYAtHGPFgDajZibfj4DOktp+0eDUxFxb0rpgymlfRHxYESc/NrtpI9ExP8SEZ/MOb8y9XEAwG6YIDdlJgArQ24CQBv3aAGg3Yi5aaYI6Km9Y6jKOV9OKR2PiC9ExHpEfDbn/GRK6TMR8XjO+WRE/HcRcWNE/JP07kZfyDl/cudbB4BdMkFuykwAVobcBIA27tECQLtBc3PliyIOHztebDtz6sTk25vHOpdF7b3YbbX36eDehybdVuMUM9vKOT8SEY9c87dPb/r3906yIYAZLFIfP+u+zJrhsn9aU+SmzASYv1r+1bJ4t8foy05uAqtglOxYpH3hvdyjBZbFKLnI2EbMzZUvioCeUkxTTQUAq0BuAkA7uQkAbWQmALQbNTcVRUBnI3YcANCL3ASAdnITANrITABoN2JuKoqAzqaaYgYAVoHcBIB2chMA2shMAGg3Ym4qioDORqymAoBe5CYAtJObANBGZgJAuxFzU1EEdDZixwEAvchNAGgnNwGgjcwEgHYj5qaiCOgoxZhTzABAD3ITANrJTQBoIzMBoN2oubnyRRFnTp3Y1e0dPna82Lbb+7JoZj3+oc9pGrOaClhdtT53HubRjy98NlAmNwFmVsu/WcdUsy436zqHHvv1IDeBFSED2DGZCQxm1jEVTGLQ3Fz5ogjobcB+AwC6kZsA0E5uAkAbmQkA7UbMTUUR0FWKNGI5FQB0ITcBoJ3cBIA2MhMA2o2Zm4oioLMB+w0A6EZuAkA7uQkAbWQmALQbMTcVRUBHKcacYgYAepCbANBObgJAG5kJAO1GzU1FEdDZiFPMAEAvchMA2slNAGgjMwGg3Yi5qSgCOhuw3wCAbuQmALSTmwDQRmYCQLsRc1NRBEvt8LHjxbYzp07s4p6UDdhvAIPbSd/74NiEAAAgAElEQVS4KH3ndkbo/5mN3ARW3TwyrrZcbXvzMOu+bLefq5r/chNYBbNmo3Ejm8lMYFnIvq2t8rHPw4i5qSgCekpjVlMBQBdyEwDayU0AaCMzAaDdoLmpKAI6SjHm7+4AQA9yEwDayU0AaCMzAaDdqLmpKAI6G6/bAIB+5CYAtJObANBGZgJAuxFzU1EEdDZgMRUAdCM3AaCd3ASANjITANqNmJuKIqCzEaeYAYBe5CYAtJObANBGZgJAuxFzU1EEdDZetwEA/chNAGgnNwGgjcwEgHYj5qaiiF125tSJYtvhY8dnWm7VzXpuFuF8pzTmFDPA2FYhU0Y5xkXIopHITYDFyofd3pdZx9OrSm4Cq2LWPFqkTKUvmQmw/OT+dEbNTUUR0NmIU8wAQC9yEwDayU0AaCMzAaDdiLmpKAI6G6/bAIB+5CYAtJObANBGZgJAuxFzU1EEdDZgMRUAdCM3AaCd3ASANjITANqNmJuKIqCrFGnIeioA6EFuAkA7uQkAbWQmALQbMzcVRUBHKcaspgKAHuQmALSTmwDQRmYCQLtRc1NRBHQ2YscBAL3ITQBoJzcBoI3MBIB2I+bmzEURh48dL7adOXVi1tUuzPZ6WJbjGEXtfNc+b1MbcYoZgN0yj/xfpGsK2X/95CZQs0h9/LKYddwkUxeD3ASWxSJlziJxXqYjMwGWm8yc1oi5aaYI6CiliLXx+g0A6EJuAkA7uQkAbWQmALQbNTcVRUBvA3YcANCN3ASAdnITANrITABoN2BuKoqAzkacYgYAepGbANBObgJAG5kJAO1GzM213jsAqy6l7R9t60n3p5SeSSk9m1L61Bbt+1NK/3ij/bGU0j3THgkAzN8UuSkzAVgVchMA2rhHCwDtRsxNRRHQ2UQ3qdYj4qGI+L6I+HBE/GBK6cPXvOxHI+JMzvmbI+J/iIifmfZIAGD+dpqbMhOAVSI3AaCNe7QA0G7E3FQUAZ2lhv81+GhEPJtzfi7nfDEiPh8RD1zzmgci4h9s/PufRsT3pNRaqwUAi2GC3JSZAKwMuQkAbdyjBYB2I+ZmyjmXG1N6NSKen2XFsMTuzjm/b4oVpZT+RUQcaXjpgYg4v+n5wznnhzet5wci4v6c81/eeP4fRcTHcs7HN73mtzde89LG8/9v4zWnd34kQITchIKFyk2ZCYtDbsJ7TJaZEXITlo3chPdYqLHmxnrkJiwAmQlbWvnc3FNrnHIwDrxXzvn+3vsATEduwnzJTVguchPmS27CcpGbMD8yE5aLzIT5GjU3/XwGLIeXI+KuTc/v3Pjblq9JKe2JiFsi4rVd2TsAWBwyEwDayU0AaCc3AaDdruamoghYDqci4t6U0gdTSvsi4sGIOHnNa05GxA9v/PsHIuL/ybXfzwGA5SQzAaCd3ASAdnITANrtam5Wfz4DGEPO+XJK6XhEfCEi1iPisznnJ1NKn4mIx3POJyPif4uIn08pPRsRX4l3OxcAWCkyEwDayU0AaCc3AaDdbudmUoQIAAAAAAAAACwjP58BAAAAAAAAACwlRREAAAAAAAAAwFJSFAEAAAAAAAAALCVFEQAAAAAAAADAUlIUAQAAAAAAAAAsJUURAAAAAAAAAMBSUhQBAAAAAAAAACwlRREAAAAAAAAAwFJSFAEAAAAAAAAALCVFEQAAAAAAAADAUlIUAQAAAAAAAAAsJUURAAAAAAAAAMBSUhQBAAAAAAAAACwlRREAAAAAAAAAwFJSFAEAAAAAAAAALCVFEQAAAAAAAADAUlIUAQAAAAAAAAAsJUURAAAAAAAAAMBSUhQBAAAAAAAAACwlRREAAAAAAAAAwFJSFAEAAAAAAAAALCVFEQAAAAAAAADAUlIUAQAAAAAAAAAsJUURAAAAAAAAAMBSUhQBAAAAAAAAACwlRREAAAAAAAAAwFJSFAEAAAAAAAAALCVFEQAAAAAAAADAUlIUAQAAAAAAAAAsJUURAAAAAAAAAMBSUhQBAAAAAAAAACwlRREAAAAAAAAAwFJSFAEAAAAAAAAALCVFEQAAAAAAAADAUtpTa7z99iP5rrvv3rIt5/Jy62up2HblannBWtue9fI6a9bSbMtFRFy+UjnIymprW6wd474909eoXK28Ualybq5Ujr22ztox1JaLqL9XtWVnXa7m4qWrxbanf/vLp3PO75tpxddYv/nunC+f2/Z1+dyrX8g53z/FNoH5uf3IkXz07nu2bqx0R5XYjEpsVNXir9Y3ru8gN2v7Ouv+zJrjsx7FjKe7vtyM52Ve26x93mrrrEVq7Tie+PVfk5vAlm697fb8gTuObtlWG1fMOt6s93GzjVMP7F2vrLXu4uXymGPvevn4q8vtKZ+bnYyNS2pj5lw547PuS+29327slypXB9Vroxm3WTs3pfsbLz7/fLz22unJ3ii5Ccvl9iNH8tGj91z3crXr/yu1+4Yzj6pm25ftXJqhX42YfRxTM+thlBM8Zh431q5Tahk2L7UtzjreLF1TvPjC8/Ha6WlyU2bCcrn99iP5zqNbf7dZM+tYszq+qTRdqoztdvJ9YW0sslZZba0vrh3jHIaa1bHW1Uqg1valNpY+uK88tt/ua8ZZv4ec9fNW3155nb/9G7++8vdoq0URd919d/zSr3xpy7baG3LTwb3FttfPXiq2vX3+crHtyI37im21t/9Q5YMcUf8P5NU3LxTb1iof1tp//29VjvGOwwcn3965i1eKbXsrneqs79PR28vHcKHS4URE7K/sT62zqt2IPH+pfPy1L+9+79V3im3HvvHW58tLXp98+Xzs/1MPbvu680/8z0em2iYwP0fvvid+5dF/u2Vb7WJl1n6s1v/XLlTPVdZ5cyXDt1Pb11ofX8uqWb8UqxX+1Vy+Us+qklqm1L9MqxUT1rc5a6Fp7SL/amW5WhbX3oubD67LTWBLH7jjaPzv/+yLW7bddeRQcbnbbihn1Vfevlhsq3WrFypF0W+eK4+NvuUDN1XWWs/qF147W2z7wK0Him3Pny4vd8dt5fFYLYtnLTQ//Vb5fF+eMYtqd5xq1ynbjTf3VLKqtuzNB8u3TWrj+zPvlD83pc/wJ77zO4rLzEJuwnI5evSe+OKvPrZ1YyVwauPNsxfK/VhtTDXjdz+xfwfFhK9U7tMePlTOh9rYqHZvtFZPMOt4szamqv6fBSs7c7Yynq5lWG1726l9F1M7p7Xxdq3opVS8+t1/9mPlHblOMhOWy51H745/Wfhus1YQXxtv1K79a/f3apnx0lfKXyrfXRkTb+e1yrj4YGVfa2O42vVELadq323WvFO5RqmN32vbe6Eylv62u24uttUyKmKb7yEr5/TWG8rfe9e+h62dm9rn7Rvfd3Dl79FWiyKAOUsRsTb7gBAAVorcBIB2chMA2shMAGg3aG4qioDe5jG3EAAsK7kJAO3kJgC0kZkA0G7A3FQUAV2liDT7b0MBwGqRmwDQTm4CQBuZCQDtxsxNRRHQ24DVVADQjdwEgHZyEwDayEwAaDdgblaLIlKk2Ltn60qPtSu5uNzvnzlXbLvpQHmTr79zsdh2+crVYtvNB/cW2w7tq/+myZvnLhfbUuUNfft8ebn33by/2Pba2+VjvHi5fIwHKsfx+6+fL7Z9XWVfLlW2V3ufDu4tV//UjiHKH5mIiHjnwpVi2w37y8d/NZdXfO5ieZ3ra+X390N33Fxsm1RKQ/7uDrC1FBHlrqXc59T64/VKFl25Wu7/rlT6xj2V/m87tW3W9vXCpfIxrlWWO1vpx/cXrlEiIvbvLfettWuKWjZcrlz75Mr5rr5PlbbtXK0su3d9tvf4YuXcrFXOza5dA8tNWCp71tfi/bce2LLtK5VxU62Pu1zpG285VB43nn7rbLHtyE3lMdV2/fib5y4V226t7M8rb14ottUy7o2z5e3Vxoa1MdUblTFz7Rh+99V3im21cVrtOqWW09u+F5XrrRv2l8e/tfXWrm9qn9O961tfw6TK9eJM5CYsl1S+Jq/147XxT/Uav7IrlypjowOV+4a1cVNE/b7iLQfLfXVt3Fi7x1kb/9TGt3sqXWstN2pD8UuV5c5V9nNPJW/OV8fh5X2JqN8Xr2V17T2uXafVljtQuPap3Uu4bjITlkqOiEuFvrN0LR5R78NrXU7tnuHePbP1mdtlZi37YsbvzGrftZbOZ0TE1avlY6x9t/nOhfJYs5Y1b1f25ezZ8vEduWlfsa323e12Y83aOPwbDh8sttXe4wuVa6L6/sx+r/m6DJqbZoqA3gacYgYAupGbANBObgJAG5kJAO0GzE1FEdDbgFPMAEA3chMA2slNAGgjMwGg3YC5qSgCehp0ihkA6EJuAkA7uQkAbWQmALQbNDcVRUBvA04xAwDdyE0AaCc3AaCNzASAdgPmpqII6CoN2XEAQB9yEwDayU0AaCMzAaDdmLmpKAJ6ShGxPt4UMwDQhdwEgHZyEwDayEwAaDdobm5fFJG3/vP5y1eLi3z9LQeKbV9552Kx7cDe8gl8+Y1zxbZc2MeIiP1765UqNx0on4I3z10qtq2vpWLb+Uvlc3Pkpv3Fttq+Xqqc7xv2l89brpycNyrHd9sN+4pta5Vj37enfAxrqbxcRPGjtrFsue2F18qfjTsOlz+LqbI/r58tn5vJbXNegLGU8uHixSvFZQ7uK/fjl6+Ue8eLV8rZsHe93B/XMuxcZT8j6v18rTurtdXy4fLF8vEf2lde7urV8nKVplirtNWOYX0OfXktpyIi9u4pt9fe49o1xf7q+1te59vnLxfbJic3YWnsWUtx66G9W7a99taF6nIlhyqZ+vo75Wv8IzeVxz+1TDld2c/t9qc2/qmNcWt9/DsXyjn+/OmzxbZvOHyw2Hb2QrmPv7EyFq2t8zdeer3Y9qe+/uZiW+WyKG4pfJa+6lJl4QOVsXjt2ujwDeVt1t6nl76y9Rj2cuXabmZyE5ZGioi961v/N/32+XJftW9/+d5n7X7bhUvlddbGcBcq90UPVHIxop6NX3m7fE/5cOU+Zq0brF1T1NTGlLUxfG08/XYlb2vjtFre1I6vdn83ov7ZqI0N5zHevFhYZ+3zMhOZCUtjz1oqfsf1h2+cLy5Xu0e7r3Kv9UKl71uvhMb7bi5/X/jKm/Wx5gduLX/39QdnysdYGzfVMqU2K0D9uqB8PVHLjAMHy/tZG9vfXDm+K5X3oprR29z3vLVyHXLzwfJ12Ktvla9tSvdKIiLeKn8lWv0sTm7A3DRTBHQ15hQzANCH3ASAdnITANrITABoN2ZuKoqA3gaspgKAbuQmALSTmwDQRmYCQLsBc1NRBPSUUsTaeL+7AwBdyE0AaCc3AaCNzASAdoPmpqII6G3AKWYAoBu5CQDt5CYAtJGZANBuwNwcb49h2aS0/QMAeJfcBIB2chMA2kyUmSml+1NKz6SUnk0pfWqL9h9JKb2aUvryxuMvT34sADBvA441zRQBXY05xQwA9CE3AaCd3ASANtNkZkppPSIeiohPRMRLEXEqpXQy5/zUNS/9xznn4zveIAB0MeZYs1oUkVLE/r1bTyaxZ71c4fHGucvFtguXrhbbvvnrbyy2/dZvvVFs+/lf//1i29/5gW8rtkVEvHnuUrHt7MUrxbabDpRP3XOvvFNsu/t9NxTbXj9b3pfLV3Kx7ZZDe4ttFy+Xz3ftvbhQWe6Nyn4evqG8Lwf31f8DuXy1fIyXKvtz520Hi22197f2Hu6tfL4nlWLIKWaAraWISIUKyAN7y33g2QvlvNm3p9xH1PqxNytZXMvw2n5GRLxzobzeg5Vl96yXj6OWVYcq2VE7xlsr2ZhzOW8quxK1ZKhl2NXK9mp5W3t/I+rXBrVj3Fv5TL1zvnxOD+0vvxe1z+mk5CYslbVUHiN8w+HyNf6zf1Qeb91Q6avuqKzzncrY7+ZD5f54u1HD0y+/VWy76/by/txaGVddqvT/tXHTW5U+/kolx2rjxt968c1i25Gb9hXbbjtYbjtfeS/W1spn/OZtcvMPXz9fbKud79q1yDuVa7gbK/tz241bH//61ONQuQlLpTberN1ze3vGMdyh/eV+7Pylcv+3p9JXr1faIiLeOFteb22seq6SHbXxWG1fz1fyr9bH17Z36Up5nW9XcvqWyjXMxco6L+RyW+2aKaI+Vq29i7WxYW0Mu6eyO8XrlPLqrt90mfnRiHg25/xcRERK6fMR8UBEXFsUAcxRivLYodZPvfja2WLbXbcfKrbVvk+q9dP7K/uSKzkcEfFHb1woth25eX+xrTr2q/T9teVq1wW1+5u164JaZtYmIHjtrfJ5qeX3758pjxe/6f3l73Uj6veoq98XHyl/pmrfs9f29X2V935Sg441zRQBXaUhOw4A6ENuAkA7uQkAbZoz80hK6fFNzx/OOT+86fkdEfHipucvRcTHtljP96eU/oOI+HcR8Vdyzi9u8RoAWFBjjjUVRUBvC/i7OgCwsOQmALSTmwDQpi0zT+ec79vhlv6viPhHOecLKaX/LCL+QUR89w7XCQC7a8CxpqII6G3A390BgG7kJgC0k5sA0GaazHw5Iu7a9PzOjb/9sZzza5ue/v2I+NkpNgwAu2rAseZ4c1vAMkkbU8xs9wAA5CYAXA+5CQBtpsvMUxFxb0rpgymlfRHxYESc/NpNpQ9sevrJiHh6suMAgN0w4VgzpXR/SumZlNKzKaVPbdH+n6eUfiul9OWU0q+mlD68qe2nN5Z7JqX057bblpkioLcBp5gBgG7kJgC0k5sA0GaCzMw5X04pHY+IL0TEekR8Nuf8ZErpMxHxeM75ZET8lymlT0bE5Yj4SkT8yI43DAC7bYLcTCmtR8RDEfGJiHgpIk6llE7mnJ/a9LJ/mHP+exuv/2RE/FxE3L9RHPFgRPzpiPiGiPi/U0rfknO+UtqeogjoKEXE2pr/Zw4AtJCbANBObgJAmykzM+f8SEQ8cs3fPr3p3z8dET89ycYAoIMJc/OjEfFszvm5iIiU0ucj4oGI+OOiiJzzm5tef0NE5I1/PxARn885X4iI300pPbuxvn9T2ti2RRFXruYt/54qFSAXL18ttt12475i26Ur5eX+9JGbi23/75GzxbZffPoPim0REX/m/bcW2+4+cqjY9vKZc8W2e953Q7Ht4L7yb6y8de5Ssa30PkREXLhULHqJK+XF4uaD5bd/fa38/t51+8HKvpTfw+1cqezsjQfK+3qpstxNleXOXSyft33ru3TjKG08gKWQI+JyIcv2VPqVPevljmDWgsv9eyvbq/TxtSyOqPePtX3NudxX79tTWWdlX14/O1turlV2tBKbcbVyDHur72H5+Gr7uZ21yvtYy/HaNmufm53s62TkJiyVS1dyvPLG+S3bbr9pf3G5ShcXv/dqeWz4cpTHcLVM+TNHbym2nd9m/HNof3n89/aF8njk9r3l5W7cX+6rz1fGOL/ywuli2y2H9hbbar3/N7+/PPatHd+tN5TvCxyp3DO4ULnXcK4yLo6o5+YtB8vHX9vmG5XPTe266HdfeWfrbe1gPL31ToTchCVyNcr3XGvjkdr473LlGn9vpf+4XLkXd6hy7/OdC5fLK91GLVNnPsbK+Pa1ty8W22r3d2vjpncq2XjH4cr91koW1c53fYxebouon9Pa/Y2rleOv3W+4fPX6v2uojdGvm8yEpXL5ai5eq99aGfvUuoHa93e1769qPdX+yrhvu9tw5yvjnxsqmVnrp6v7U1nurfPlfK/d9511rFUbvz7/WvmewDe9/8ZiW+181r67jKjfo/jGryuPmWvHf6ZyHfL6hXLb0X3l77Un1Z6bR1JKj296/nDO+eFNz++IiBc3PX8pIj72ns2l9BMR8VMRsS8ivnvTsl+6Ztk7ajtjpgjoKlULjACAzeQmALSTmwDQRmYCQLvm3Dydc75vp1vLOT8UEQ+llH4oIv5GRPzwLOtRFAGdmc4UANrJTQBoJzcBoI3MBIB2E+XmyxFx16bnd278reTzEfF3Z1w2JD10llLa9gEAvEtuAkA7uQkAbWQmALSbKDdPRcS9KaUPppT2RcSDEXHymu3cu+npfxgRv7Px75MR8WBKaX9K6YMRcW9E/NvaxhRFQE+p8QEATJabKaX7U0rPpJSeTSl9qvK6708p5ZTSjqd5A4BdZ7wJAG1kJgC0myg3c86XI+J4RHwhIp6OiF/IOT+ZUvpMSumTGy87nlJ6MqX05Yj4qdj46Yyc85MR8QsR8VRE/IuI+Imc85Xa9vx8BnSUJvy9upTS/RHxP0bEekT8/Zzzf1t43fdHxD+NiGM558cn2TgA7IIpcjOltB4RD0XEJyLipYg4lVI6mXN+6prX3RQRPxkRj+1ogwDQyZTjTQBYZjITANpNmZs550ci4pFr/vbpTf/+ycqyfzsi/nbrtswUAZ2tra1t+9jOpi94vi8iPhwRP5hS+vAWr/MFDwBDmyA3PxoRz+acn8s5X4x3f4vugS1e97ci4mci4vy0RwAAu2ei8WZ1hqWU0k+llJ5KKf1mSulfpZTu3tT2wyml39l4/PDEhwcAk5kiMwFgVYyYm9WZInKOuHI1F9q2/ntExJEb9xXbZi0cOXr7oWLbj3/HPcW25199p7reu24/WGy7WjnG9bXygbzy5oVi222Vc/PmucvFtqNHysdfU9uXy1euFttuPFD+aNSqfy5V1vnm+XJbRMQN+9eLbWcvlGc8uVjZZuHjGxER5y+W17l3z+79xzpRNdUff8Gzsc6vfsHz1DWv++oXPH99io0C71X6b7rW5+6r9DmXrpQ7slxrq/R/Nfu36f9q/WpNLR9qm6wdf02ta714ebb34vzlyvmu7MuBveV11q6nzlZyKiJi33p5vW+fL19T1I6xlkh71sutlcuiyTXm5pGU0ubZkB7OOT+88e87IuLFTW0vRcTHrtnGt0fEXTnnX0wpyUyYk7W1iBsK447X3iqPY245tLfYVhtv1fqx337ljWLbr/zqa8W2/+rPflOxLaI+bqzl0e/84dvFttrYsLa9O27eX2yr9aylewIR9fyrefnMuWLbB249UGyrHd9vvFB+DyMivuFweb2/9WJ52dpn6vkz5fsNKd1QbLvjtq3vQ8xjHLpLMyw9ERH35ZzPppR+PCJ+NiL+Ykrptoj4mxFxX7z7cfm1jWXP7GinYEWlKF9318ZNB/aW77fV7qlV+//KOOadyj282r2/iPoYpzamfKsydqqNjfceKm/vwqXK/cbKuamNt96unJs/qtzDrd2nrV3fnK8cw1vnLxXbIiJuu6F8vXWucr7PXiiPRQ/sK7//71TGsDcf3Pr41yae2cFMEbA8UirfG6z1YTcdLPd9Z965WGx7/Wy5T63laS2jD1b6zIiI85cq9w0r2ffa2+Xj+LrKmPHyjNcFNbXv/Wrv06tvlzPzrtvK4+XaOp/8gzeLbafPl7cXEfE93/x1xbbfrIxTa/n+3JnyPYH3HyqPbRfwHu1CWbwyDVgl7b+7cySl9Pimx49ds6atvuC542s2tekLnjkcCQDMX3tuns4537fp8fDWK9xiEymtRcTPRcRfnXbnAWCXTfM7r9vOsJRz/mLO+ezG0y9FxJ0b//5zEfFLOeevbBRC/FJE3L/DowKA6U302+gAsBIGzc3qTBHAfKVIrVPInM453zfzdv7kC54fmXUdANDbdeRmzcsRcdem53du/O2rboqIb42IX96oeP76iDiZUvpkznnz7BMAsNCuIzd3NMPSNX40Iv55Zdk73rMEAHQ20VgTAFbCqLmpKAI6m2iKGV/wALASJsjNUxFxb0rpg/FuVj4YET/01cac8xsRcWTT9n45Iv6avARgRI25uaMi/E3b+kvx7k9lfOdO1wUAu23EacABoJcRc3O8Mg5YNtNMMfPHX/CklPbFu1/wnPxqY875jZzzkZzzPTnne/5/9u48zq7zrvP891e3bt3aN1Vply0vcmKbODGW7bDZLHZQgNjMYDqOX3mRQNIsg7vThDCdDGCIoeksDBkIHrAbPMPQHQwJJKOZVtqEzsKSTkaOHSd4i2XF1mJJllRS7ctdnvmjyk5FXb/fPbo60tUtfd5+1cuq+tW5Z7t6vud5zqNztfhIUyZEAABazxnmZkqpIuluSQ9LekrSX6WUnjCze83strO34QAANMGZ9zfrTcBfXI3ZLZJ+VYv9zPnTWRYAgPNCCz4GHACApmnB3ORJEUAzWT6zqVJKFTN7+QZPQdKDL9/gkfRISmln/AoAALSA/HJzl6Rdp/zsHud3v/+MVwgAQDPkk5vhE5YkycyulXS/pB0ppZeWlR6W9DtmNrT0/Rskve9MNwgAgNzl1NcEAOCC0KK5yaQIoMny+twdbvAAAC4Erfh5dQAANMuZ5mbGCfgfltQr6eNLA2P7Ukq3pZTGzOy3tDixQpLuTSmNndEGAQBwltDXBAAgu1bMzXBSRFJSpZpO+0WL7f6BGJ8pu7VoUslcudbQa24e7vJfVNK//+wet/ZzN1zk1jYMdrq1qbkptzbS29FQrVLzz0OhzT9wKTW23PGpBbc2HGzn1HzVrfWUCm5Nis9xR8Hf1qm5iltbP+Cfp+AlVQ2Od55M1pKzqQD4ona3EfNlv11tL/h5GzTxYaZErynFbefsgr+tHcG1QaNZNdpfcmvlip8pxWAfo+Y/yoaZYN9rZ3C8I9H2dHXEmeuJzlOl6h/Tc5Vl5CawuqQkzTt9gJ5Ov6sa9n+C9ZWDvu1lg71u7XUbhtzaXJDTkrR1tMetHRybdWttXf7+z8z7/Z+vvHjCrUUZ9+RL427t8uE+t3Z0Yt6t/crOJ9zaR3/iGrcWZfhCkEXR+0KS2oP6JWv98zQd9HGvv3jYrR084Z9fT8q5H5pXbtabgJ9SuiVY9kFJD57xRgBQSnLHaaO/6lE7EI37Rq/ZFrSpJ4Nx2vaoQ6m4r1ZeaKz/E2XVUE/RrUXjn9NB/68n2Jaof//82Ixbu36rfzqGmYsAACAASURBVC0S9QujaBzq9vddkhaCPI6GPXqDa7jo/Eb74Y1TV2r+Np4u+prA6pKSPx4XjYt2Fv02POpPRG1YNNZ2ctq/D7emzx/3lKSBLr8dj8Yp1wT5dmTcz8y1A/72RPdLj036+xhdT0zM+tcT/3TA7/feEdwTPD7p719ncJ5u2jri1iRp3zE/w6PsG+zxz8WWSrdb6y75WTsx648X5KlVc5MnRQDN1nrtBgAAzUNuAgCQHbkJAEA2ZCYAANm1YG4yKQJoJmvNR8wAANAU5CYAANmRmwAAZENmAgCQXYvmJpMigCZrxUfMAADQLOQmAADZkZsAAGRDZgIAkF0r5iaTIoBma712AwCA5iE3AQDIjtwEACAbMhMAgOxaMDeZFAE0kZm15CNmAABoBnITAIDsyE0AALIhMwEAyK5Vc5NJEUCTteIjZgAAaBZyEwCA7MhNAACyITMBAMiuFXMznBSRklSppRVrxYK/swfHZt3ahsFOtzZbrrq19jZ/fcM9Rbc2Nl12a5L0ju1b3No17/qEW/uDf3Ozv9zaQbd2YsbfnomgNl+pubXoXPR2+qe4v8s/bjPzFbc2u+Cfp9G+DrdW7y9ItM6F6srvQ0ka7vXXWU3+cgPd/v5HxztvrdhwAFhZlJvloF1pC9qBqB0vB23jscl5tzYQtP/Vgv+aklSp+vvRWWxsdmiwG4qayP3HZ9za2v6SW2sPXrTmnD9Jag/ytqcUZGqQm8WCWwqvfSQpOlO1IP8KDeZOe8E/v3PBNVzeyE1g9UhJWnDy8cj4nLtc1B5tHPL7m9Pzflt1PMjNKPvq9Rui+pY1XW7NOy6S9PX9E27tuo1Dbu3X/8szbu0t125wa0NBf/upw5Nu7UNvutqtrQ/GBeaDTNl/3B9riPrFknRscsGtRbkZXW8NdPnXaZet63Vrew5POdvhLtIwchNYPZKS215Z8PziqO3cNOxnUTnIv0rQNvYFfdipOX/sT5IGg7G66HWjpq4YXDdUg4a31O4vt+uJg27tJ16z2a2dCMapv/Mifzy5M+g4RrkZZVihTn8zGqftCI5Nkl/rCO5EROPU3rloz/lfqJKZwOpRS9JseeUcGw/uw0X3Nret96/vo/Z2bMrvh0T3qOaC8UQpHqeMxpMjG4LMjK4LxoJjOhlk/0hwP7G75O/D+j7/uEV96bE5/1xs6veviR5/cdytSdLmYNloP/Yd88e2Lx7pdmtRH/3wuD+2kbdWzE2eFAE0W+u1GwAANA+5CQBAduQmAADZkJkAAGTXgrnJpAigmUwt+bk7AAA0BbkJAEB25CYAANmQmQAAZNeiucmkCKCJTPEj/gAAwLeQmwAAZEduAgCQDZkJAEB2rZqbrTeNA1hVTGb1vwAAgERuAgBwOshNAACyITMBAMguv9w0sx1m9oyZ7TGz965Qf7eZPWlmXzOz/2pmFy+rVc3sq0tfO+uti0kRQJO1tVndLwAAsIjcBAAgO3ITAIBs8srMejd3lv3eT5hZMrPtue0EAADnSB65aWYFSfdJeqOkqyS9xcyuOuXXHpO0PaV0jaRPSPrQstpsSul1S1+31d3mrDsH4CywxUfM1PsCAAAiNwEAOB3kJgAA2eSUmRlv7sjM+iS9S9KX890RAADOgfz6mjdI2pNS2ptSWpD0kKTbl/9CSulzKaWZpW+/JGlzo5vNpAigiUz8yx0AALIiNwEAyI7cBAAgmxwzs+7NnSW/JemDkuZy2wkAAM6R08jNETN7ZNnXz57yUpsk7V/2/YGln3neIenTy77vXHrdL5nZj9fb7vao2GamzuLK8yYKwUVAtZbcWrlac2vjM+Voc4LX9Ne3fqAULrsQLPvn//ZWt3b9RUNu7YVjM26tPThuteRvyx9+6QW39i+v8yfFROeit+Sf/oHuoltrC6b3+GuTpuYqQVXqaPfn6HQWG3u/FYJtrQTnfnq+6tbyxiAUsHqYSR2FlduyUtDGRW1OEA2h3k6/jY/anXotUpT/MwuNtZ1dxYJbi7a13TnWUnxtkJJ/LVIMzlMluIaJcirK28hcnSyKcjOqBbGp1OAbrhici7yRm8DqUSiYhns7Vqx5P5ekb7407dai9mih4vc354M2fv/JWbdWz0if3x89dNIfA4+a4ys29Lq1KP9ue82oW7tui9+/7ezwc/q6i/3lakHgRHlTCZYbDPqpH/1vz7s1Sdpx+Yhbu2bTgL89wXsjuhaJlhvqWXk/2gv5Zxy5CawebWYqBX0nTzTeOD3vj9U12noUg7asvRBvfzQ2HEScRoLrhmj/o7HBqP3sDNr/yVl/H7aOdru1qK9drviZEn1ed6XmLxede0nqCvK/v8vP42icNhJdGzQ4LHLaMmbmiJk9suz7B1JKDyz7fqWbOzcufwEz+05JW1JK/9nMfqXR7QXgK7RJA10rj8etCTLjyQMTbu3w+Lxbi8Z9u0t+exq1/fVEfcaJIIuiMcOoDe8OciF6zah/Fy23ebjLrX1mr59hv/Spf3Zr/+HO17m1v3nykFvbOtDp1iSpOxj7XdvvjwnMl/2x3yiHo+uX6L2Yt4y5eSyllMvHRJnZWyVtl3Tzsh9fnFI6aGaXSvqsmX09pfSc9xqNjdIDyAePKwUAIDtyEwCA7MhNAACyyZ6ZZ3Rzx8zaJP2epLc3+hoAADRdfn3Ng5K2LPt+89LPvn11ZrdI+lVJN6eUXpmhlFI6uPT/vWb2eUnXSnInRfDxGUATmRZndNf7AgAA5CYAAKeD3AQAIJscM7PezZ0+Sd8h6fNm9ryk10vaaWa5/CtaAADOhRxzc7ekbWZ2iZl1SLpT0s5vW5fZtZLul3RbSumlZT8fMrPS0p9HJH2PpCejlfGkCKCpGIQCACA7chMAgOzITQAAssktM1+5uaPFyRB3Srrr5WJKaVzSK5/ptfSvWt+TUnpEAAC0jHxyM6VUMbO7JT0sqSDpwZTSE2Z2r6RHUko7JX1YUq+kjy+tc19K6TZJV0q638xqWnwIxAdSSkyKAM5nfMYrAADZkZsAAGRHbgIAkE0emZnx5g4AAC0vr75mSmmXpF2n/OyeZX++xVnui5JeczrrYlIE0Ex8xisAANmRmwAAZEduAgCQTY6ZWe/mzik///581goAwDnUon1NJkUATfTy5+4AAID6yE0AALIjNwEAyIbMBAAgu1bNzXBShJnUXmg77RddP9jp1srVmlubmK24tQ3Ba86Vq26tt7PxeR/fd/mIW3vshZNu7V0fe8yt3f9T291aueYfmx+8fNCtXbmp360dnZh3a9H7tVJLbq0QLHckWN+6/pK/YJ3tKQSPYVmI3lPTZbcW7GL4Ps0bjzMFVg+T/3c6Jb/R6eoouLWgqQrbxq6i/5oLFb+NqwXbKcUXO4Wg9o3DU27tsnU9bq0juA7pLfn7GCkVT//aRqqTm0FuROepUvWPd0d7vJ2l4BxHLDjHMwv+NVV7sB9RpuaN3ARWD5PfRpaDrBoN+hVRxo30dri1YtDJifpUv/v3e92aJBWCHLvtSr+/WQ4a1ltftc6tfejz33Brb3nNRrfWFoRceGnQYB9u//FZtzYUnKcoi/+nGy/2i5J6guuGKFqia5+ZeX8MI3ovNprhjSA3gdXF+ysdXY9HY6pR+z8dtHHR+qK2OurfSfE49GAwxjsb9GOGevxcmQr2sbfkr++NV25wa5GxqQW3FvW3jk/7y3UG4wmloE85OOC/L6R4bLTReyDV4I0T1c4VMhNYPdrM3GvuqL3ZOtrt1qJ+QXSPMmrepub8HCr1+flVT9T+Hxjz+2Ibh7oaWl90bIaDHJ4Njtuj+0+4ta8d8MeZNw375/Bf/83X3dqDd13r1uqlw6GTc24tup6KrlGivmZ3cI0SjW3krRVzkydFAE3WgpOpAABoGnITAIDsyE0AALIhMwEAyK4Vc5NJEUAzWWs+YgYAgKYgNwEAyI7cBAAgGzITAIDsWjQ3mRQBNNHi5+40eysAAGgN5CYAANmRmwAAZENmAgCQXavmJpMigKaylvzcHQAAmoPcBAAgO3ITAIBsyEwAALJrzdxkUgTQZK34iBkAAJqF3AQAIDtyEwCAbMhMAACya8XcZFIE0EzWmo+YAQCgKchNAACyIzcBAMiGzAQAILsWzc1wUkSlmnR0Yn7F2mBP0V0uemJGIThKo/0ltzY9X3Fra3o7/BXWMT1fdWtfPzju1rZvHXJrD/709W7ttg/+V7f2hffvcGtD3f4+/vN+fzs3DXe5teRW4vN0Yqbs1g6cmHFra4PzK0ltwTonZv3zf2R8zq1F+z+34J/7ge54W/Niktra2s7JugCcfbXkty3V5Le67UFwBoup2O63H1Etav/ria51nn5x0q31dfmXHPuPz7q1FByAgW7/WqSro+DWJuf8TJkr19xaR3BMO4Irqtkgb9oL/mtG7wtJKlf8bY0uSis1/5gWgnUGi2km2Mc8kZvA6hL1N6P2qFz1279akBs9Jb+xHu7x+1tRFr96fY9flPS9m4fd2qFpP/+ePub3q65e6y/32g3+9nzv//C/uLU9n/s9tzYf5M1fPn7Ard100Rq3Fp2L3fuOu7VLh3rdWpS3krS2v8+tLQTvqe4gq6PHhU4FYw3Hp1Y+h9F7uxHkJrC6pCTNO/2VKP+itipaLupTVYPOQTjeWKeP01Py13kyGI8cCPqbU0H/72+fPezWvu/iUbfW3+X3RaNxyv/4+Itu7e//2d+Wv/rZ17u1sakFt7Z+wB/fjPraktQR5F+dRXPnjVPneTOGzARWl4VKTfuPr9yn6g7yLbqGjzKsq+i3H0PBvdTpIKPqNXFRG3h80s+G4eB+6pe/OebWtg53u7Xo3l90HfIfH9vv1u75pY+4tXs/8ktu7Qe3+vn96OETbu2l8ZXHJqT43rUUXxdF9z2novveff55mgmWi8bL89SqucmTIoAma8XZVAAANAu5CQBAduQmAADZkJkAAGTXirnZetM4gFXGzOp+ZXydHWb2jJntMbP3rlD/eTP7upl91cz+0cyuyn1nAAA4y/LITTITAHChyKu/CQDAakdmAgCQXSvmJpMigCYyM7W11f/K8DoFSfdJeqOkqyS9ZYUbOB9LKb0mpfQ6SR+S5D8jFwCA81AeuUlmAgAuFDn2N+tNJrzJzB41s4qZ3XFKrbo0yfCrZrYzx90DACA3eWUmAAAXglbNTT4+A2iynCZL3SBpT0pp7+Jr2kOSbpf05Mu/kFKaWPb7PYo/5hEAgPNSDrlJZgIALhhnmpvLJhPeKumApN1mtjOl9OSyX9sn6e2S3rPCS8wuTTIEAOC8dh7+g1YAAM5brZibTIoAmqwtW8sxYmaPLPv+gZTSA8u+3yRp/7LvD0i68dQXMbNflPRuSR2SfvD0txYAgObKITfJTADABSNjbkayTCZ8fqlWO9OVAQDQLDlkJgAAF4xWzE0mRQBNlrHdOJZS2n6m60op3SfpPjO7S9KvSXrbmb4mAADn0rnKTTITALAaZMzNM55MGOhceu2KpA+klD51GssCAHDOtOC9HQAAmqYVczOcFNHWJvWUCivWZheq7nJdxZWXkaT2gn+USu1tbs3bDil+nvFTByeDauy58Sm39vzXpt3alcP9bu3Wmy53a9f87P/l1p7807e7td0vHndr81X/H2pcuaHPrUWCU6jL1/a6tXofH5OSfyan5ytubainw61Vqv5rWvA39tDJObeWJzOpkM/n6hyUtGXZ95uXfuZ5SNIf5bFiANl0FPyMiy4goraqErTxKfnLRe3OXNnPdynej03DXW5toLvo1va+5GfqxiH/NaP9Pza54Nb6u/zLn3//uT1u7UM/eqVb64yufYLjfXh83q1tGOx0a5I0X/HPVbw9/ms2+n6bmCn7L5qjnHKTzATOE9Va0sTsyu3H5Jx//b9+wG8fp4LlJmb9WtS0LFT89u/mLWv8BSWVin6ju6Zacmu/8Pq1bu0zzx5xa9dvGnZrH/7DX3Zru54+5NZ+/OqNbu3X//ALbu3T/+52txZdF6zt8s/vv/7rr7m1P73rO92aJJ0MsmpNr9+nPDHtX1MUg1DtCs79/3dgYsWfzwXZ3ojTyM1cJuE7Lk4pHTSzSyV91sy+nlJ67iytC1j9nL/S0b/UKwS1WjAWF5TCz4iuBmNxT70Yj9NescEfV4zGI2sNbuu+E34b373N71NFfaPRfj/fP/rrH3Vrn/34b7u1aMw8qrUH/ff54PpGivv+UbZUgzdOR1swLhKc4LKzrdF79HTlOEYL4DxQaDP1d63c54iajnUD/phhdK8pul86X/bb22j87uCJ+B5VNG4Y3TOLxilHe/wMGw76TKVgP16a8Mc+bwr608Ov/yG3tnXQHy/ett6/lmj0HuT8WJyZteBCJHpvRPcEouu3qB/6uW8c9V80R62am8GwOIBzwczqfmWwW9I2M7vEzDok3Slp5ynr2bbs2x+V9GxuOwEAwDmSQ26SmQCAC0YOuXm6kwm/TUrp4NL/90r6vKRrT28PAAA4N3IaowUA4ILQirnJx2cATZZHu5BSqpjZ3ZIellSQ9GBK6Qkzu1fSIymlnZLuNrNbJJUlnRCPAQcAtKAzzU0yEwBwIcmhv/nKZEItToa4U9Jd2dZtQ5JmUkrzZjYi6XskfeiMtwgAgLPgPLx3AwDAeasVc5NJEUATmeLH4JyOlNIuSbtO+dk9y/78rlxWBABAk+SVm2QmAOBCkEduZplMaGbXS/qkpCFJbzKz96eUrpZ0paT7zaymxSeVfiCl9OQZbRAAAGdBnmO0AACsdq2am0yKAJrpPH2EDAAA5yVyEwCA7HLKzQyTCXdr8WM1Tl3ui5Jec8YbAADA2UZfEwCA7HLMTTPbIen3tTgJ/09SSh84pf5uSe+UVJF0VNLPpJReWKq9TdKvLf3qb6eU/ixaF5MigCbjehsAgOzITQAAsiM3AQDIhswEACC7PHLTzAqS7pN0q6QDknab2c5TnjD4mKTtKaUZM/sFLX4k45vNbFjSb0jaLilJ+srSsie89TEpAmgik9TGFTcAAJmQmwAAZEduAgCQDZkJAEB2OebmDZL2pJT2SpKZPSTpdkmvTIpIKX1u2e9/SdJbl/78w5I+k1IaW1r2M5J2SPoLb2V1J0V4j79ob/OXaS/4B2JyruLW5so1t9YWHNuxqQW3Ntpf8heU9JWD7oQR9XX4h2ego+jWNgx2urUvPnbQrf3mv7nVrT370qRbu2HjGrfW0+nvw/7js26tUktu7cpNfW4t+YtpdqHqFyX1Bts6PlN2ayN9/jmO3hsjfR1u7aI1XW4tb23RmxtASzGTOpyArAUNZCFoB6pBexy1q10dBbeWgm1pr9MmRdsT5v+s345vHel2a9Fxe/zAhL++sn+98eoRP8fe9d1b3drJIIt6qv41TE/Jz7fo3JeD16y37FyD741jk/P++gr+xd+mYXITwOnrKLZp62jPirUT0/51/GC33xeL+g1///wxt7al18+ibet63dp0h583ktRZ9Nvcrxz2+6LdwXK3blvn1qLs+JEr1ru1Qyfn3Nqb7vuiW/v8B//HhrYlyrBrLhpwa//hLde6tTW9fv9OiscivnF4yq1dtnbl96gkHTzh96lfODHj1kY6V+7DtrcFAy0NIjeB1cNMKjrX5NH1f0eH37ZUq35/ayzI4u6gTxG5cqPfF5OkY8E43sy83457x0WKs/iHLxt1a1FWPXXIH6dd1++PC2vra93SL//N19za//FT293axiF/fcEwbd2+f7T/E7P+uejv8vu/8xX/2iB8DzvjLCncw9NHZgKrh5lUctqOqF/QW/IzoxaMiT5xeNytXbNx0K3NBq85Gty/kqSFoE2dDjJzoMvvTw/1+LWo7Y/GPv92z2G39r1bRtza2P4X3drv/91zbu016/z+5NrgfrGXNfVq9epf/uaYW7tmk7+tRyb8Mdr5sp+Z12z0XzNvGXNzxMweWfb9AymlB5Z9v0nS/mXfH5B0Y/B675D06WDZTdHG8KQIoInMeDQbAABZkZsAAGRHbgIAkA2ZCQBAdqeRm8dSSv7sztNap71Vix+VcXOjr8GkCKDJeDQbAADZkZsAAGRHbgIAkA2ZCQBAdjnl5kFJW5Z9v3npZ9/GzG6R9KuSbk4pzS9b9vtPWfbz0cqYFAE0GRfcAABkR24CAJAduQkAQDZkJgAA2eWUm7slbTOzS7Q4yeFOSXct/wUzu1bS/ZJ2pJReWlZ6WNLvmNnQ0vdvkPS+cJvz2GIAjTFJbVb/CwAAkJsAAJwOchMAgGzyzEwz22Fmz5jZHjN77wr1nzezr5vZV83sH83sqpx3BwCAsyqv3EwpVSTdrcUJDk9J+quU0hNmdq+Z3bb0ax+W1Cvp40vZuXNp2TFJv6XFiRW7Jd279DMXT4oAmslMxixkAACyITcBAMiO3AQAIJucMtPMCpLuk3SrpAOSdpvZzpTSk8t+7WMppT9e+v3bJP2epB1nvHIAAM6VHPuaKaVdknad8rN7lv35lmDZByU9mHVdTIoAmqyNf5oDAEBm5CYAANmRmwAAZJNTZt4gaU9Kaa8kmdlDkm6X9MqkiJTSxLLf75GU8lgxAADnUiv2NcNJEW1m6u4orFibXai6y1Wqfo5Hhyh+zZpbmyv7tYnZcrBG6VVr+tza9HzFrR2annVr88H2XH/NBrf2yS8fcGu6cbNb+uw3/aeB/NyNF7u19oJ/Nj71zBG3Vkv++X3tRQNurdQez8HZf9w/pusHO4Pt8V9z83CXWzsf/sHMy4+YAbA6mPyLgVqQjdWgIYvauE4noyWpo93/hKyJWT/fuoLXlOLsiEI+iI7wNWvJr33XpWvc2lSQ4Y/uP+HWNvV3u7Xo2NQ7bp51/SW3Vg6ufaT4vVEq+ud/vuK/bl9X0a1F76kUneAckZvA6mKSik4GDATt0eSc38ZHGbepx+8bXLd10K09f3TGrW0I+imSVAzazuvWD7m1x474WfWqjX4fthrk5lBPh1trDxrXH9u+ya1duq7HrUX9++j8VoKAG+z2lxubXnBrUnwtsnXUz/+DJ/x+arHgn9+rN/SH27OSzvbGric85CawupikgvOXOrpWj8bxqkE/tVT026SBoD2OcjrsT0oa6fWzSkFtz5Ept/YHX3zerX3gR690a2NTfq48+Ig/hvvoU/6Y6j995M1u7dK1fqZ6512Snn5x0q1ducm/ZmgPMkyKc7yn5L83FoL+Zi3I+J6SP25cdd7DFt5pOD05ZuYmSfuXfX9A0o3/3frMflHSuyV1SPrBXNYM4BVtZup22qpozHBq3m/7OqO+3Ra/bxdl5pHxebcWZbsU95vWDfj91ONBvu056ufpZSO9bu2lCX8/fmDrWrf2h196wa396b23u7Vrg750lCdf+OZRt3bTJaNurVLz3xeS9MIxf8zg8lH/uDWatVG/+Fz1/1q1rxn/rQJw1tnSY2aiLwAAsIjcBAAgO3ITAIBsMmbmiJk9suzrZxtZV0rpvpTSZZL+raRfy3M/AAA4F1qxr8nHZwBNdv41CwAAnL/ITQAAsiM3AQDIJmNmHkspbQ/qByVtWfb95qWfeR6S9EfZVg0AwPmjFfuaTIoAmsgsfgQeAAD4FnITAIDsyE0AALLJMTN3S9pmZpdocTLEnZLu+vZ12baU0rNL3/6opGcFAEALadW+JpMigCY7Hx8hAwDA+YrcBAAgO3ITAIBs8sjMlFLFzO6W9LCkgqQHU0pPmNm9kh5JKe2UdLeZ3SKpLOmEpLed8YoBADjHWrGvyaQIoMlasN0AAKBpyE0AALIjNwEAyCavzEwp7ZK065Sf3bPsz+/KZ00AADRPK/Y1mRQBNJGZteQjZgAAaAZyEwCA7MhNAACyITMBAMiuVXMznBRRrSVNzlVWrEX7emhszq2Vim1ubWZ+5XVJ0tbRHrcWHfiXJubdmiR97fBJt7ahu8ut3XTZqFt7+sVJt7ZuoNOt/fFPXuPWnj867dZuvO19bu3Nn/odt3bJWv+Yvm/95W7tyLh/TCdm/XM42F10a5LUXvDPY7lSc2uFgv+eKrb7tUitlhparhGt+IgZACtLklJauf1o9O96EJsy84vedkhSX6cf//Vav3LVb49nF6pubainw61VgteMMv7IuH+98V+ePeLWfvkXf9etveu3/5Vbe+8P+NkYnd+jk35u9nf52Vitk0VRbrYF21MsRK/ZWG4Gb7fckZvA6lKprtyA1IKGZSBoO6PapUH/Z7bsZ9jmYb9fWK+tjvq4f/bVg27tfUHmHDg+69a2jna7tUdf8Pu+0fF++3Vb3NreI34/dSY4ppcF5+Lxg/52Xram163VG5DpCPqGUX+zGGTjljX+eyPKq5PTC84y7iINIzeB1SPJ7ztFf9WjqIrGaaPafNlvNzuCdnMuWE6SFoL2eLrBceOP3H61W4vyf1OQ/x/4sSvd2uUfedCt1d55o1tTcA6jq411AyW3dnKm7NbW9Pp9dCnOzWjcNMqd7lLQGQ1YA+/7htZDZgKrRrWWdGJ65TawNxgXjVqBaMwsGqN79vCUWzsy5Y9tbl7w+3ZSnJlRho/2+bnxHRsH3NqLJ/x+aHhMg7b19Rf1ubVr1g66taiPvhCMMw+X/Oz7wt6X3NrNl651a5LUXfL3v6vDz75oPDUav28L+r6Ts372560Vc5MnRQBN1tjtJwAALkzkJgAA2ZGbAABkQ2YCAJBdK+YmkyKAJjK15mwqAACagdwEACA7chMAgGzITAAAsmvV3GRSBNBkDX7CBwAAFyRyEwCA7MhNAACyITMBAMiuFXOTSRFAE5m15mwqAACagdwEACA7chMAgGzITAAAsmvV3GRSBNBkba3XbgAA0DTkJgAA2ZGbAABkQ2YCAJBdK+YmkyKAJjJJhVZsOQAAaAJyEwCA7MhNAACyITMBAMiuVXMznBRRaDN1dxRWrM2Vq+5ytZTcWnfJX+Xa/pJbC15Sx6YW3Np9bWQ7FgAAIABJREFUX3rBX1DSmm5/e7o3rrzvkjQ17+//qzb0ubXvm5lzawfGZt1aR/DhLMe+/FG39sSBCbf2Lx963K395huucGu9nf4xG+guurVnDk26NUmq1vyTvGm4y621BY9oqVRrbm12wT+HJ2fKbi1vLfixOwAcJv+xUdUG26Oekp9F0ROqakGbWg1CdWK24r+o4oudviAfGvXckSm3Nl/2j+nOrx5xawf+4X9zax/+wnNu7dce/oZb+1ffdbFbWzfgX990FPwUqPcEsmj/I9E1Vbxc8J4K3m95IzeB1aXNyZVvHp5xl1kftKtRa9QeZNhMkMWdRT+Lo36xJC1U/Lb6nddtcWtRm3vZuh639tRBv8/VF/TF7/iDf3Rr/+kXvtutRf3U3/j0U27t2kvXuLVfuelStxYdz9FgPEGK+3je+1CK+7/R9kT91HYn/8/G00fJTWD1MPntx+RsY21clI1Rpkbt1Ylpf1s6i3GrFK2zGPSdxoM2PuqP7Qn6m9E49eHxebe2/wv/q1uLxkajMdzBYLx166h/XRDdq5ici/v+Ub+xWPBfOLo2iPqN0bXPuDNOkXc/lMwEVg8zqd1pq6LxtBeO+f3QNb0dbi3qF0b3zKI2s95HExQbbG9ng22N+jD/51dfdGuv3dDt1l63dtCtDZX8Y/r5F466te+q+P3J6QU/327YOuzWonNY7+b/02PTbq2/a8CtRfcEupx785JUDfqhu1844dby1oq5yZMigCZrwY/dAQCgachNAACyIzcBAMiGzAQAILtWzE0mRQBNZGYt+YgZAACagdwEACA7chMAgGzITAAAsmvV3GRSBNBkLdhuAADQNOQmAADZkZsAAGRDZgIAkF0r5mYrfuQHsGqYFj+nqd4XAAAgNwEAOB3kJgAA2ZCZAABkl2dumtkOM3vGzPaY2XtXqN9kZo+aWcXM7jilVjWzry597ay3Lp4UATQZ19MAAGRHbgIAkB25CQBANmQmAADZ5ZGbZlaQdJ+kWyUdkLTbzHamlJ5c9mv7JL1d0ntWeInZlNLrsq6PSRFAM5lU4IobAIBsyE0AALIjNwEAyIbMBAAgu/xy8wZJe1JKeyXJzB6SdLukVyZFpJSeX6rVznRl4aSIlKRaSivWKrWVfy5JQz0dbq0WLHdsciHaHNcv/9//7NbefN2GcNlSoeDWvnPLkFt78cScW5ss+p9K0hmsrxB8AMtFa7rc2uxC1a2NzfrH9MZLB93anuNTbu31W9e4tam5iluL3heS9PiLJ91aV4d/3NoL/vGOtmfjUKdbO3TSP795WnzEzDlZFYBzIEmqVFfOZi9PJamn5Ldxlaq/XLG9sQakXPGvH6IskqTOIOM62v1atM6DJ2bdWjFo4zcO+dn4n37qOrfmH1Hp52682K3tOTrp1nYfHHNr39M+4tb6Ov1Lseh9IUnF4HhH77eo5r1/pThvJ+fKbi1PeeWmme2Q9PuSCpL+JKX0gVPq75b0TkkVSUcl/UxK6YUzXzOA5ebLNT13ZOV+R9QGVoM+5fisf/0/0OW3uQtBTkWinJakUtHfj4tHSg2tM9rWv3n6sFt79Wi3W7vnzu9wa3NVv7950Yj/mn/wE9e4tf/5/3nSrf3dniNu7Q3b1rm1evkQZW7k2Ny8WysW/JX2Bus7PrXy+7R2xkM8347+JrC6VFPS9PzK7UfUj4tqFgxmV4O+QTQWGfVTDoz5fT9Jumxdr1trL/nb2h60x9HYYKQnaMdf3eDY92Vre9xaNBYZ9bUj0XVKbynOxbFpf0zZeRtKkkrBOqP9iMZ+vbzNcw4DmQmsPl7+zZX9fIvuGQXDaeoO+q9PH/bHE7cM+f2pydl4rG39oL+tUd8nysV/99k9bu1fXL3erW0L8jtqW08c9LNmpNvP2mic+aqN/Q1ty+MH/fuT2y8a9heUVA46cjPz/jVTNEZ7Msjh0X5/LOHV6/rcWp5OIzdHzOyRZd8/kFJ6YNn3myTtX/b9AUk3nsamdC69fkXSB1JKn4p+mSdFAE2W1wU3N3gAABeCM83NjI9le0zS9pTSjJn9gqQPSXrzma0ZAIBzjxs8AABkQ2YCAJBdxtw8llLafhY34+KU0kEzu1TSZ83s6yml57xfbmyaKYBcmBZnLNb7qvs637rB80ZJV0l6i5lddcqvvXyD5xpJn9DiDR4AAFpGTrn5ymPZUkoLkl5+LNsrUkqfSynNLH37JUmb894XAADOthz7mzvM7Bkz22Nm712hfpOZPWpmFTO745Ta28zs2aWvt+W3dwAA5CevzAQA4EKQY24elLRl2febl36WSUrp4NL/90r6vKRro99nUgTQTLb4qLd6XxlwgwcAsPrlk5srPZZtU/D775D06TPbcAAAmiCH3Mw4AX+fpLdL+tgpyw5L+g0tPv70Bkm/YWb+55QCANAs+Y3RAgCw+uWXm7slbTOzS8ysQ9KdknZm2gSzITMrLf15RNL3SPI/q1N8fAbQdG3ZWoa8P3eHGzwAgJaUU25mYmZvlbRd0s2nuywAAOeDjLkZeWUCviSZ2csT8F8ZbEopPb9UO/XDdH9Y0mdSSmNL9c9I2iHpL850owAAyFsOmQkAwAUjj9xMKVXM7G5JD0sqSHowpfSEmd0r6ZGU0k4zu17SJyUNSXqTmb0/pXS1pCsl3b/UD22T9IFTPh75v8OkCKCJFh8xk+lXc/vcHW7wAABaVU65memxbGZ2i6RflXRzSmn+9LYUAIDmO43cjCYTnu4E/OVO9+lMAAA0xWlkJgAAF7w8czOltEvSrlN+ds+yP+/WCk++Tyl9UdJrTmddTIoAmsrUplxmIXODBwBwAcglN195LJsWs/JOSXd921rMrpV0v6QdKaWXznSFAAA0R+bczG0SPgAArSm3MVoAAC4ArZmb4aQIM6nQtvJODXYX3eXGpstuzXk5SVKSP63kn54/6tY++GOnfpzlt1SqyV+hpJG+DrfWXfIPz8Uj3W5t70vTbu21mwbcWqlYcGsp2I3oeA91+udpx2Vr3VpXh78txYJ/Evcdn3Vr3nvpZa8a7QvrnnX9Jbc2PV9p6DU3DnU1tNzpMuX2eXTc4AHOAyap3Zki2e43q6rW/EY+Wq4WLHdyxs+GzqKft6X2OlM8gzw6Mj7n1g6f9GtR5mwa9tvjKBtrQXG2fOqTnb+lXPFrr79kTUPre2nCn4PWERzv4PRKkmaCjOsJrmGCy62G34u90fpylEduZnksm6QPS+qV9HFbXOG+lNJtZ7ZmAKcqFdt0yWjPirWo73AoyJSBLr89mg/a+N5Of7mXxv12fDToi0hSe9B3Kgd91c88c8StPX540q396OV+H+9v9/p96k0Dfr+4t8M/NlNzfhatH+x0a594x/Vu7e+CfY/6zFGGSfF1StT/6+/y+9Tz5apbsyCwvG1N0YVWA3Lqb2aagB8s+/2nLPv5M94i4ALVZqYupx1sC3JzLmirUr1Oh8Pr90pSpern7ZWb+sPXjdrVaJ0LQcb/88EJt3b9JUP+tgSvGe1jdESPTy24tXUDfm5OBnkb9WGj66lKnXPfHiw7ENwXiI5bozHn9Zstx5sxOY7RAjgPtJm5Y5zdwTjkWNBOj8/6bfHsgp9fG/r99n0uWO7VG+P7ZdF9yKgPMxOs8+7XX+zWonuiTxzwszZq+i8a8l/zPX/0uFv73Ttf69Z2PX3Irf3Iqze4te8Kxn2/eXTGrUnScKc/LrAh6Bc/vm/crW0c8peLRGMbeWrV3ORJEUAzWTxRKCtu8AAALgj55Wa9x7LdcuZrAQCgyfLJzboT8AMPS/odM3v5ruMbJL3vjLcIAIC85dTXBADggtCiucmkCKCJFj93J5+Wgxs8AIDVLs/cBABgtcsjN7NMwDez6yV9UtKQpDeZ2ftTSlenlMbM7Le0OLFCku5NKY2d0QYBAHAW0NcEACC7Vs1NJkUATdbWis+YAQCgSchNAACyyyM3M0zA363Fj8ZYadkHJT14xhsBAMBZRl8TAIDsWjE3mRQBNFkLthsAADQNuQkAQHbkJgAA2ZCZAABk14q5yaQIoInMpEIrthwAADQBuQkAQHbkJgAA2eSZmWa2Q9Lva/Fjp/4kpfSBU+rvlvROSRVJRyX9TErphVxWDgDAOdCqfc22Zm8AcKGzDF8AAGARuQkAQHbkJgAA2eSRmWZWkHSfpDdKukrSW8zsqlN+7TFJ21NK10j6hKQP5bIDAACcQ63Y1wyfFFGtJU3PV1esVao1d7kT02W3NtRTdGtf3nfcrX3H2kG3Fn1uyfHpebcmSWsHSm6tWkturRzs/6bhLrcWvQm6OwoNbctAl38ax6YW3Np8xd+HQpu/pcGm6Ir1vW5trrzye+ll0XmcXvCXDTZHk3OVcJ3NZmrNz90BsLIkqRY1ko4oU4oFf/7ifNCuDnb7eTsTtKmTs36GS9Ka3o6G1jkQ1GaD7Sm1+/vfwKGWFOdftH/RedpzZLqhbdk45F8zRNdaktQVXDdUgoMTRLymnOs+KT7e0XnKE7kJrC61lLTgtHUTU/51fNQer+nrdGvzZX+5UtFvxyZm/W2Zno/7G+sH/e2J+lzXrB9wa08d9TPn6RMTbu2u125ya/tOzLi1Q1Nzbi3Kouga5sUT/ms+fczflmeOP+/WfmjriFuTpMEeP+MPnfS3Z6TPX+7EdNBPnfTHIgadcZHoPdEIchNYfWpp5YvycpBxUR/1pQm/rRrt88dMezv9schoffX6OJ1FP1ecXZckjQdZfdloj1uL+qLlqr/CaHy7EizX6LVINLzdXvDb+SgDvPfSy6I+fDROHfUNx2fi8QaPl495RlyOmXmDpD0ppb2SZGYPSbpd0pMv/0JK6XPLfv9Lkt6ax4oBfEstJc05bW50n2oquJ8UXatH9/aiPmHUtz0wNuvWJGk4GMOM9uNkcP92tN/P/ig1OoK2//LgnuHX9427tdu++yK3Ntzl73tUizKqGO3DOv9aQor3P8rM113s9/uffnHSrUXXKFH/NU+t2tfk4zOAJmu9ZgMAgOYhNwEAyI7cBAAgm4yZOWJmjyz7/oGU0gPLvt8kaf+y7w9IujF4vXdI+nTGTQQA4LzRin1NJkUATWVqy/lfAwEAsHqRmwAAZEduAgCQTebMPJZS2p7LGs3eKmm7pJvzeD0AAM6d1uxrMikCaCKTdG4eOA4AQOsjNwEAyI7cBAAgmxwz86CkLcu+37z0s29fn9ktkn5V0s0ppfjzvwEAOM+0al+TSRFAk1kLfu4OAADNQm4CAJAduQkAQDY5ZeZuSdvM7BItToa4U9Jdp6znWkn3S9qRUnopj5UCAHCutWJfk0kRQDOZ1NaCDQcAAE1BbgIAkB25CQBANjllZkqpYmZ3S3pYUkHSgymlJ8zsXkmPpJR2SvqwpF5JH1+6obQvpXTbGa8cAIBzpUX7mkyKAJqoVR8xAwBAM5CbAABkR24CAJBNnpmZUtoladcpP7tn2Z9vyWlVAAA0Rav2NRueFBE9FqOzo+DWxmcrbu2q0X63VqkmtzY977/mpaM9bq3e66ZUc2tdRX8f2wv+sfn0U4fd2us2DLq1aMZNtL7eTv8U95b8fegKzmG15h+zctU/ZnNlv1Zv2c7geM8uVN1ad8nf/57g2IxNLbi1vLXiI2YA+GrJbyM9URsftbkLQbvZaHvcF7SNklQMMicStXWlbv8SKmrjoyMd5XuUKVPBNUV3kI2XjHa7tYVgWwpt/nGp1eJjHb03ovdUJViuJ7g26Cj452mu7J+nvJGbwOrixWap3W9z1vR2uLUoN6Jr/Cg31w10urWoHZek9qAeLTnc4+/jO6+/yK2dnCm7teOT/v4fmZlzax/59B639tM/sNWt3dG/ya11B3nzc6/3X3Nyzs/prx484dYkacNQl1s7Oe0fm+l5/z3VEbxPo2uqp1+cXPHn83X6zI0gN4HVo5aSZpyci9qjqLYpaBvj/pbfXhWDfkO5TjsXdNUUNWcjwbVB1Ec/dNLPv3944ahb+/Gr/Yx7fP+4W9scHO+oD7e2v+TWov7dTNC/jcaTpfhaLDzHQf83ei8Wg5o39t/I+EuEzARWj5SkhcrKmRM0m1of9P2icbioLfa2o55ofVLcF913bMatbRj09zFqB6PXjO7RPndkyq39v3v8TxD6haBf+MLRabcWHZfD437uR8sNdBfdmiS9cNjfx+h4R9YHy0Vj2998yT82eWvF3ORJEUCT1RlHBQAAy5CbAABkR24CAJANmQkAQHatmJtMigCaaPERMy3YcgAA0ATkJgAA2ZGbAABkQ2YCAJBdq+YmkyKAJmvBJ8wAANA05CYAANmRmwAAZENmAgCQXSvmJpMigKYyWQvOpgIAoDnITQAAsiM3AQDIhswEACC71szNtmZvAHAhM0kFs7pfAACA3AQA4HSQmwAAZENmAgCQXZ65aWY7zOwZM9tjZu9doX6TmT1qZhUzu+OU2tvM7Nmlr7fVWxdPigCayVrzETMAADQFuQkAQHbkJgAA2ZCZAABkl1NumllB0n2SbpV0QNJuM9uZUnpy2a/tk/R2Se85ZdlhSb8habukJOkrS8ue8NYXToqo1ZImZssr1samFtzlOosFt9Zd8muD3R1ubXq+6tZG+vzlvrZv3K1J0ovTs27tijV9bu3ydb1u7XhwbG66dNStecdaktYPdrq1Wi25tb7OmltL/mIKSpoMtvOx/VNu7dotg8GrSin5f4PKVX8/1vT65//giTm31lHwH5TSHtTyxgU3sHokSV6THLWd/V1Ft1YNGuuOdr+tKgW1wR6/dnRi3q1JUleHn+O1YFtL7X5jF2V8Cl6zGOxjd7Cd1SA3zfxLo2g7o+M9NVdxaz3BdrYX4oBoCx5RFuV4dJ02V/b3sa3NX1/bOQwzchNYPaq1pJMzK+fjzLzfdgbNkft6klQIFhzu9ftbA91+TtfLzagtPz7pL9vf5efRN474fa6u9sZyZfvGYbd2xcV+P+66dX7tmy9Nu7XeTn//xoNzONpfcmvtFvfh+oN1dgTHJtrWySDjoz6ld+0XvUcbRW4Cq4vXTkRjfBY2BI0NDi5Ug75f0V9f1Iets0rNLQT9saCPMxssF/Vjbr9qk1vrLvnZcP0lQ24tEp2nqF88X/HHTKNYifqpkjQf5NhQj39tVO8ce85GBp4uMhNYPWrJbx+jv+rR2Fc07lkJ7l/NBDk03NPYPVFJWuPfotSm4S63FuVNlJn/+dkjbu3Nr/EzM8qMny5tcWvRfc8NQ/7+RZkZ9TWPTvv983r3Nr9y2L0HrzcNbXRrPcH98kMn/XubA8G9hDV9fp85bznl5g2S9qSU9i6+pj0k6XZJr0yKSCk9v1Q79S/aD0v6TEppbKn+GUk7JP2FtzKeFAE00cuPmAEAAPWRmwAAZEduAgCQDZkJAEB2p5GbI2b2yLLvH0gpPbDs+02S9i/7/oCkGzNuxkrL+jN0xKQIoOksnJsIAACWIzcBAMiO3AQAIBsyEwCA7DLm5rGU0vazvS1ZMSkCaDImIQMAkB25CQBAduQmAADZkJkAAGSXU24elLT8c1Q2L/0s67Lff8qyn48WaOxDvgDkxjL8BwAAFpGbAABkR24CAJANmQkAQHY55eZuSdvM7BIz65B0p6SdGTfhYUlvMLMhMxuS9Ialn7l4UgTQRCbj8+oAAMiI3AQAIDtyEwCAbMhMAACyyys3U0oVM7tbi5MZCpIeTCk9YWb3SnokpbTTzK6X9ElJQ5LeZGbvTyldnVIaM7Pf0uLECkm6N6U0Fq2PSRFAMxmPZgMAIDNyEwCA7MhNAACyITMBAMgux9xMKe2StOuUn92z7M+7tfjRGCst+6CkB7OuK5wUUU1JU3OVFWubh7vc5WYWqm5tTW+HW0vBtgx0F93aniPTbu2qzf3Bq0rDJ/3t6S75h2eu7O9jR7v/qSQ9pYJbm55f+VjXq03M+rVoW0pBrdDmv5vX9pfc2j+9cNStlavxuRifKQfL+u+OmXn/XHQHx/vo5Lxbmw3ew3njehu4METtapQpXR1+OxYFZ1twVXJyesGtDfX4eStJh07OubXRPj8fasG2Fgv+tpr5WeVdo0jSXNCOB5Gi3iA3ou2cr9Tc2qahTrc2NuWfi/6u+FxUg4NaTX6tFiwXvU9ngmuRk0GG543cBFYPM1NnceV2fsa/VA/bnMGg3xj1Kfo6/b7fQtDGR9kgSR/9x2+6tes3Dri11272a1vXdLu1KOMefvawW1vf42f4v7hmvVsb6vH708eC/lZ0nTIcjBlEufldl65xa5L0lRdOuLX1/X5Wz5b985+CvD0y7u//SN/K+xjlcKPITWD1aDMLx/I8larfjkXtTjRO2x30U6P+htUZPY/GBqMxvqg9LhX95U4cm3FrncE+tgX9zagpj453dJ6KBf+8R5kajdEP1OlvRuOm3vWbJLUX/HMRXTdF4yLe+z7vj7MgM4HVo9AmDXSt3MeL7v1MzPo5FOXbZDBGGfVRo77t+kG/jyJJU8F9segeXrSt0f3EH9m2zq1Fbfh8ObhHN+FnzWiwD5HeoG8fiTJzIepoS9pxhd9njt5T88Fxi/rF48FrNnKt2KhWzE2eFAE0kUk8mg0AgIzITQAAsiM3AQDIhswEACC7Vs1NJkUAzdZ67QYAAM1DbgIAkB25CQBANmQmAADZtWBuMikCaLK8H/UGAMBqRm4CAJAduQkAQDZkJgAA2bVibjIpAmiys/CxsQAArFrkJgAA2ZGbAABkQ2YCAJBdK+YmkyKAZmvBhgMAgKYhNwEAyI7cBAAgGzITAIDsWjA325q9AcCFzLT4iJl6/wEAgPxy08x2mNkzZrbHzN67Qv0mM3vUzCpmdsfZ2BcAAM42+psAAGRDZgIAkF2r5iaTIoBmMskyfGV6KW7wAABWuxxy08wKku6T9EZJV0l6i5lddcqv7ZP0dkkfy38nAAA4R3Lqb2boa5bM7C+X6l82s61LP99qZrNm9tWlrz/OexcBAMhFjmO0AACsei2am+HHZxTa2jTY07Fibb5Sc5cb6Su5tfly1a31dvqbMzFbcWtr+/31TQbLSVJPyV9ntD3Fgn82u0v+XJNK1T9uA11Ff7lacmtdRX99He1+rauj4K+v6q9vdsE/h2989Qa3Vgq2RYqftBK939qCv1nR+R/o9o+3rfy2PyvyaBiW3eC5VdIBSbvNbGdK6cllv/byDZ73nPkaAaykVkuamV+53SkV/Ta3PfgArrmy3/51Bu3/bJC3hWB9Ud5K0miQuYWgQSsH+RflQ5Q5UU4fOjnn1qLrhvaCvy0DXX5tocGc6guyP8pwKc7GYlv+816LwbGJjmnecsjNGyTtSSntXXw9e0jS7ZJeycyU0vNLNf8gAzhjlWpNR8bnV6wN9fjtY5RjY9Nlt3bRmi63dnxqwa2Nz/iveWx65e1/2RUj/jpfs7HfrUXXBhbkw0sT/vb8xHdscmufeuJFt7ZtqNetRe1/lI1r+vwOV3fQT20LOmq1oM8sSRev6XFrxyf949YXXG/0dTfWcTw2ufL7LeqHN+pMczNjX/Mdkk6klC43szslfVDSm5dqz6WUXndmWwFAkmopuX2AqC8W5WbUdEb9kWh8sxy0ZVG+SfFYZdRvjPajTcH2hP0/v/2P9tFr46U4N+v1/zzRcRnu9XOq3rkYDcb3TwbXRqVgnCLK1Ebew3nfbDkfb94AaEyt5o9xFoN2sxi0jePzftsXje/NBGOb0bhvt1tZ1B/kVHQftjPMUz/fqjU/o/uD9j3q4XSX/NeMRNcL0fGOsvbydX6/t05k6qlDk25tW/C6jd6HjfYjytO8tWJu8qQIoKmyPGAmU8vyyg2elNKCpJdv8LwipfR8SulrkrjBAwBoUbnk5iZJ+5d9f2DpZwAArDK55GbdvubS93+29OdPSPohs1YcIgMAXLhyG6MFAOAC0Jq5yaQIoMkyPmJmxMweWfb1s6e8DDd4AAAXhJxyEwCAC0IOuZmlr/nK76SUKpLGJa1Zql1iZo+Z2RfM7PvOxj4CAJCHVnwMOAAAzdKKuRl+fAaAs8uUuWE4llLafna3BgCA81tOuXlQ0pZl329e+hkAAKvKedDfPCTpopTScTO7TtKnzOzqlNLEWVgXAAANO43MBADggtequcmTIoAmy+kRM9zgAQBcEHLIzd2StpnZJWbWIelOSTvP+oYDANAEOeRmlr7mK79jZu2SBiQdTynNp5SOS1JK6SuSnpN0RQ67BQBA7vJ6DLiZ7TCzZ8xsj5m9d4X6TWb2qJlVzOyO3HcEAIBzgI/PAHDacnrEDDd4AAAXhDPNzaXHet8t6WFJT0n6q5TSE2Z2r5ndtrgOu97MDkj6SUn3m9kTZ3evAAA4O3Lob2bpa+6U9LalP98h6bMppWRmo2ZWWNwOu1TSNkl789o3AADylMcY7VLu3SfpjZKukvQWM7vqlF/bJ+ntkj6W7x4AAHDu8PEZAE5bHu1CSqliZi/f4ClIevDlGzySHkkp7TSz6yV9UtKQpDeZ2ftTSlfnsHoAAM6ZnHJzl6Rdp/zsnmV/3q3FfwkLAEBLO9PczNLXlPSnkv7czPZIGtPixAlJuknSvWZWllST9PMppbEz3CQAAM6KnO7d3CBpT0pprySZ2UOSbpf05Mu/kFJ6fqlWy2eVAACce+fhnIe6wkkRJqm9beXdmq8kd7m5ctWtdXUU3Nrh8Xm31tHuP9Sir9PfjcnZsluTpJq/G6olv9hm/vbUohcNVILlvPMgSc8fnXVrawc63drYtH9s1g2U/G0p+NsSHDJVqvFx6e8qurVq8MLR6472+/tRqfrXnSdnztE1qUmW03QpbvAAzddmplJx5ZwrBm1nOWjHOouNPdQpyqIoUwttcVsd5VE1WGeUD1Et2tbI+iDH5sp+G79Q8WuFYN+j49LYVYE0u+BfT0lSqcH3xuRcxa0NdvtZHJ2n+eC45SrH3ATQfIW2Ng33dqxYG+ieJJriAAAPjklEQVTy+3hHgn7jRWu63Fpb0H5MBP3GqN189sSUW5Ok267a6NaitjPKqu6gTx31qXpK/jH97i1r3Nr//uV9bq1c87cz6G6pGOR7lDfRuUjRgoqPzZoNfW7tC88edWs3XT7i1qJxihNOX7zRawZXTrmZoa85p8WnK5263F9L+usz3gAAkhYfUexlWdRXifppUX8rGjeLxjCjvm/U35Li/Yj6R51OP1yKt3XraLdbmw+yOOqLHZv0r1PWD/rjtNGxaS/464vOb3TtEy0nSd0l/5hG4/vRfkTvt2h7ojH63GTPzBEze2TZ9w+klB5Y9v0mSfuXfX9A0o05bCGA02DmtznlKN+CobhojLYUtG9BtGmoZ+X+sFS/nY7Gfo9PLbi1TcN+n3ly2u9vjfb52xplbUeQYVFORVkTHZuoJY+O2dFpP78He/y+pCRdNtrj1qK+fUdwzRT136N9HK9zTzw3LTpGy5MigCYynZ+PkAEA4HxEbgIAkB25CQBANqeRmcdSStvP7tYAAHB+a9W+JpMigCZrwXYDAICmITcBAMiO3AQAIJucMvOgpC3Lvt+89DMAAFaVVuxrMikCaLJWfMQMAADNQm4CAJAduQkAQDY5ZeZuSdvM7BItToa4U9JdebwwAADnk1bsazb2YdQAcmNW/wsAACwiNwEAyI7cBAAgmzwyM6VUkXS3pIclPSXpr1JKT5jZvWZ22+J67HozOyDpJyXdb2ZPnL29AgDg7Mirr2lmO8zsGTPbY2bvXaFeMrO/XKp/2cy2Lv18q5nNmtlXl77+uN66eFIE0GSMQQEAkB25CQBAduQmAADZ5JWZKaVdknad8rN7lv15txY/VgMAgJaVR26aWUHSfZJulXRA0m4z25lSenLZr71D0omU0uVmdqekD0p681LtuZTS67Kuj0kRQBOZWvMRMwAANAO5CQBAduQmAADZkJkAAGSXY27eIGlPSmmvFl/zIUm3S1o+KeJ2Sb+59OdPSPpDa3Dl4aSIpKRytbZirbdUcJfr6vBrteSvb7in6NYKbf7+nZguu7VKtEJJI70dbm1yruLWuor+PsZr9EXbmpJf27ym261FxzTav4kZ/5hG7zXv/bK4Lf6xXnxdv9YWzDnqaPdr1fCY+uvrDt7DueJxpcCq48XVQsVvHzva/U+zavTiIsriY5MLbm2w28+NetuzUKm6tVKQm21BxlvQVk/M+jlWLPivWSr6x7uj4Neic9hoWx7lVGewnZLUHmxrLXjdgS7/HEfZeF4gN4FVxeT386K+yrqBkv+aQSNxYGzWrY0H/Z/jc35u/vC2dW5N8q8LJGly1l/nSJ+/j1HGR/3m516admuVoB/3Y68acWvfuWXIre07PuPWTkz5xzQ6vyem/e0cDY6ZJHUGxy3qx956pX+Op+f992kUVxsGO1f8eXT90hByE1hdzG/nF4J2LOrjRH2xWtXvHBSD1yw32PeV4v5ROdiejna/FvVFo37TbNXv30bbMhiMf0Y5nVJU89cXXWvMR+ciOIf11ILtifI2Ot7RNdw5iTIyE1hVkvy2Kur7RTkV9Tei9qNY8NvFseDeZn9X/G/a29v8bY3G9+bLfjas7Y/7VJ6obxuO+wbHrTu4B10Ncjjq20XXGdG2RPeDpfh988Ixv188Ourf243OYaP7mKvsuTliZo8s+/6BlNIDy77fJGn/su8PSLrxlNd45XdSShUzG5e0Zql2iZk9JmlC0q+llP4h2hieFAE0GdfbAABkR24CAJAduQkAQDZkJgAA2WXMzWMppe1naRMOSboopXTczK6T9CkzuzqlNOEt0PiUUAD5sAxfAABgEbkJAEB25CYAANmQmQAAZJdPbh6UtGXZ95uXfrbi75hZu6QBScdTSvMppeOSlFL6iqTnJF0RrYwnRQBNZWrj2WwAAGREbgIAkB25CQBANmQmAADZ5ZabuyVtM7NLtDj54U5Jd53yOzslvU3Sf5N0h6TPppSSmY1KGkspVc3sUknbJO2NVsakCKCJmGQMAEB25CYAANmRmwAAZENmAgCQXV65mVKqmNndkh6WVJD0YErpCTO7V9IjKaWdkv5U0p+b2R5JY1qcOCFJN0m618zKkmqSfj6lNBatj0kRQLNxxQ0AQHbkJgD8/+3dy25kRxkH8Dput9t2+zKeGSdoMoFEbFlGYseaF+AReA3egQfJhkXEggUSO5BYIYSEFBiFJBMmM449vrX7clhwi0h/nytNh6PT/H5SpChfzqVO9dT/VHWlA/XkJgDUkZkAUG9Nudm27QellA/+45/95Et/f1tK+dGS494vpbz/da5lUwR0zE+zAUA9uQkA9eQmANSRmQBQr4+5mW6KGGw15Xh/uLzYxsedXU3D2mwRH/ggulYp5eOXN2Ftd7gV1gZbeadsD+L64V78eLaS87Zt3MZ58twWybM5SO4la+P1ZB7Wsg/sw4OdsPbq8i6sJU0vt9P4XkopZdjG/TjajmuT2SKs3dzF1zwZx208u44/w+vWv2EDiDTN/bmz/Lj4mGwcy/LvKhn/T8Zx3t53/1l1Jxmrs9qqtrMsTo6bJ2F8s4if23CwWvtm8zin/hvz5L0h66dFEtZZjg+TNq7yuV+V3ITNsbVVysFosLR2fjMLjxsNlx9TSimfnMXzxmzu987pOKy928S1LBtKycfHB8l8ZJRk/KrXe/N4FNZ+8yz+hclffHgW1p4c7oW18Siew757Gt/Lqu8a8yzESil3ybwxc3Ubfxaz553d69Vk+Zz6niasRG7C5mhKPAdq2/hPe7ZenY2N2fpmFn9Z3s6yhdF7js3mf6vKnk22jpll3FsnuyudM3unyPJmsuI5s7XtUvJMytaUp9lnKrneMOn7afC5WXdsykzYHIOtphztLV//jMaUfx4XeXUVfy+W5UKWp8fJ937b98w1z5L7WXV8zHIxW4fMIiVbv/7WcZyZLy4mYe1gN35ue8l6wfPz2/i4nfi4+77bHCQZ9vRRPGf+9Iv4frLvy/eTz9tl8p3AuvUxN/1SBHSpyYMGAPgSuQkA9eQmANSRmQBQr6e5aVMEdKgp+X8hDgD8m9wEgHpyEwDqyEwAqNfX3LQpAjrWv2EDALojNwGgntwEgDoyEwDq9TE3bYqAjvVwMxUAdEZuAkA9uQkAdWQmANTrY27aFAEda3q5nwoAuiE3AaCe3ASAOjITAOr1MTdtioCO9XE3FQB0RW4CQD25CQB1ZCYA1OtjbqabIpqmKcPB1tLa3WwRHvfq8i6sHe7Fl8yOu57MwtqTk4Owtr8zCGullDJftGk9MpvH7V/Vxc00rB3vD8Pa1WQe1rYH8adyec/+3fl1fC/7o7gPXyZ92NzzJ2Q2j/si68abu7j9J+Od9JqRxwerHfd1NU0/Bw4gFo11izYe47JxbCsZI7IsniY5NR7Fg+p2kPv/Om9yzUF2sytKHlsZJBm3ar6PtuP2z5JzTqZxH2bPZZi0IevfUkrZSe41C5esHbvD/L0pkrVjneQmbJatpgnHsr1hPMb95dVNWLtI5jFPTnbD2h8/uwxr33t6FNbum2++vo3nsdtJPgySwS57p7hbcZ6axeaP33s7rD0cx/PUs6u4L66Sd59srn24G89FP3oZfy5KKeXpo72wlmXqbZLx2Tw9exM5CZ5bNn9fhdyEzRONLdmcYzKNx9V03TAZQLK1yGy+uXtPbmZjbjZWZ2uObZKbWf5l659ZFk+myfpm0obsPrM+zPo+W2u9b86cnTd7h8nOms0bsz6MpqnrjDiZCZtlPm/D7xuztc1FMr6Nk1zIxvfrZO4Tff9aSimjZE5cSj6Ov3k8Cms7yTWzc2ZrtHkOx8c9/+I2rJ0exW1Iczjp39PD+JyfXUzC2uie9dJsDTfv4/i82XtIJpujr1Nfc9MvRUDH+vgTMwDQFbkJAPXkJgDUkZkAUK+PuWlTBHSsj7upAKArchMA6slNAKgjMwGgXh9z06YI6FgfBw4A6IrcBIB6chMA6shMAKjXx9y0KQI61fTyJ2YAoBtyEwDqyU0AqCMzAaBeP3PTpgjoUFP6uZsKALogNwGgntwEgDoyEwDq9TU3bYqAjvVx4ACArshNAKgnNwGgjswEgHp9zM10U0Tzj7+W+fOLq/C4hwc7Ye3mbh5fL3mCp0ejsHZxMwtre8NBWCullOvkfgbJ/dwmx91MF/H1JvG9Zu2/ncbXm83j641Hw+S4Nq4t4tr0Nm7D8d7q+2yydlxO4vvZ24n7uG3j464m8TP99bNXYW3d+vgTM0BsEYyf2UvC7nArrE1m8dg4T8bqwVZ8wayWjZullPLy8i6sPdiPM6eUuB3TJI9eJ5mzM4jbMRzEzzTLuPhtI38vWCTP7exqGtayd6Y2eWallHKZPJus/VkPt9vJu0HST9n71LrJTdgci7Ytd0HO7Sbv+Fk2ZMdlmfrGYZwA20luZmNjKaX87PefhLXvv/UorD0+jPPhtx+dhbVvPxiHteMkp58e7YW1LG8ejuNzPr+4DWtZbmbvN+fXcaY+TtYMSinl41c3Ye3JSdz+F6/jd5/9UTz/bZokN4O5aNb2VclN2CxRJN0leZRlYz6niMeP8Wi1vM3WPksp5Q8fvw5r330zzrjtZP6Tja3ZXDxbp9zZjo/Lnml2XBYB2XM7T9bFD5J+mib9VEopd/Fpyyhpx3byucmWG+aL+H6iNq47N2UmbI6mKWUUjPHDZAwbrzifTKaMadZka7QvLibxSUspv/zTi7D2g3dOw9pR8h3ezVW8vvdwHM9Rs1yI+qGU/Hvf7JzZ+nTmd8/Pw9p7b5+Etb+ex3PbUkq5Tb4Tzt6n3jjeDWufv477/2gvnof/9FcfhrV162Nu+qUI6FLTz91UANAJuQkA9eQmANSRmQBQr6e5aVMEdKiv/98dAOiC3ASAenITAOrITACo19fctCkCOtbHn5gBgK7ITQCoJzcBoI7MBIB6fcxNmyKgY33cTQUAXZGbAFBPbgJAHZkJAPX6mJs2RUDH+jhwAEBX5CYA1JObAFBHZgJAvT7mpk0R0LE+/sQMAHRFbgJAPbkJAHVkJgDU62NuNm3bxsWmeVFKefa/ux3ohe+0bXu6jhM1TfPzUsrjin/187Ztf7iOawLfHLkJS8lNYCm5CV+xtswsRW7CppGb8BXmmsBSMhOW+r/PzXRTBAAAAAAAAABAX211fQMAAAAAAAAAAN8EmyIAAAAAAAAAgI1kUwQAAAAAAAAAsJFsigAAAAAAAAAANpJNEQAAAAAAAADARrIpAgAAAAAAAADYSDZFAAAAAAAAAAAbyaYIAAAAAAAAAGAj2RQBAAAAAAAAAGykvwHr/xU6TEe9RwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 2880x576 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 5  # how many digits we will display\n",
    "plt.figure(figsize=(40, 8))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    #original = x_test[i].reshape(round(input_size/2), 2)\n",
    "    #plt.scatter(original[:, 0], original[:, 1], cmap='Blues')\n",
    "    original = x_test[i].reshape((resX, resY))\n",
    "    plt.imshow(original, interpolation='nearest', cmap='Blues', extent=(0.5,np.shape(original)[0]+0.5,0.5,np.shape(original)[1]+0.5))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    #reconstruction = decoded_imgs[i].reshape(round(input_size/2), 2)\n",
    "    #plt.scatter(reconstruction[:, 0], reconstruction[:, 1], cmap='Blues')\n",
    "    reconstruction = decoded_imgs[i].reshape((resX, resY))\n",
    "    plt.imshow(reconstruction, interpolation='nearest', cmap='Blues', extent=(0.5,np.shape(reconstruction)[0]+0.5,0.5,np.shape(reconstruction)[1]+0.5))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7ad0e6f940c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ggplot'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_reset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "with plt.style.context(['default','ggplot'], after_reset=True):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test, cmap='Blues')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = ny = 20\n",
    "x_values = np.linspace(-3, 3, nx)\n",
    "y_values = np.linspace(-3, 3, ny)\n",
    "\n",
    "#z_mu_base = np.random.rand(1, encoding_dim);\n",
    "\n",
    "z_select = np.arange(encoding_dim)\n",
    "np.random.shuffle(z_select)\n",
    "\n",
    "z_mu_all = encoder.predict(x_test)\n",
    "z_mu_base0 = np.reshape(z_mu_all[z_select[0]], (1, encoding_dim))\n",
    "z_mu_base1 = np.reshape(z_mu_all[z_select[1]], (1, encoding_dim))\n",
    "z_mu_base2 = np.reshape(z_mu_all[z_select[2]], (1, encoding_dim))\n",
    "z_mu_base3 = np.reshape(z_mu_all[z_select[3]], (1, encoding_dim))\n",
    "\n",
    "canvas = np.empty((28*ny, 28*nx))\n",
    "for i, yi in enumerate(x_values):\n",
    "    for j, xi in enumerate(y_values):\n",
    "        #z_mu = np.array([[xi, yi]]) # only for 2 dim\n",
    "        # Show interpolations between four randomly chosen digits from the test set \n",
    "        z_mu = z_mu_base0 * ((xi + 3)/6)*(1-(yi + 3)/6) + z_mu_base1 * (1-(xi + 3)/6)*((yi + 3)/6)\n",
    "        z_mu += z_mu_base2 * ((xi + 3)/6)*((yi + 3)/6) + z_mu_base3 * (1-(xi + 3)/6)*(1-(yi + 3)/6)\n",
    "        x_mean = decoder.predict(z_mu)\n",
    "        canvas[(nx-i-1)*28:(nx-i)*28, j*28:(j+1)*28] = x_mean[0].reshape(28, 28)\n",
    "\n",
    "plt.figure(figsize=(8, 10))        \n",
    "Xi, Yi = np.meshgrid(x_values, y_values)\n",
    "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The density of the layer with latent variables (not sparse!)\n",
    "encoded_imgs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
