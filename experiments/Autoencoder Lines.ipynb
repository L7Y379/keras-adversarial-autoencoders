{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Lines\n",
    "\n",
    "Let's use an autoencoder for the Lines dataset. \n",
    "\n",
    "The first experiment imports the 2D data points and casts them into a 1D vector. The (x,y) coordinates are mapped into successive inputs. They are hence correlated... This apparently is tremendously difficult to reconstruct for the autoencoder. Why? In the end it is just a difference between input and output. \n",
    "\n",
    "The nonnegative sparse constraints are not gonna help much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 400)\n",
      "(20, 400)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import h5py\n",
    "#from matplotlib.mlab import griddata\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "filename = '/home/anne/data/h5/lines.h5'\n",
    "f = h5py.File(filename, 'r')\n",
    "#for key in f.keys():\n",
    "#    print(key)\n",
    "data = np.array(f[\"data\"])\n",
    "\n",
    "map_to_grid = 0\n",
    "\n",
    "def pnts2grid(x, y, resX=28, resY=28):\n",
    "    xrange = np.linspace(0, resX - 1, 1)\n",
    "    yrange = np.linspace(0, resY - 1, 1)\n",
    "    Z = np.zeros((resX, resY))\n",
    "    for i in range(len(x)):\n",
    "        xpixel = round((x[i] - min(x)) / (max(x) - min(x)) * resX - 1).astype('int')\n",
    "        ypixel = round((y[i] - min(y)) / (max(y) - min(y)) * resY - 1).astype('int')\n",
    "        Z[xpixel,ypixel] = 1\n",
    "    return Z\n",
    "\n",
    "# Training and test set are 2D points\n",
    "N = 100\n",
    "\n",
    "if map_to_grid:\n",
    "    resX = resY = 28\n",
    "    z = np.zeros((N, resX, resY))\n",
    "    for i in range(N):\n",
    "        x = data[i,:,0]\n",
    "        y = data[i,:,1]\n",
    "        z[i,:,:] = pnts2grid(x, y, resX, resY)\n",
    "    x_train = z[0:80]\n",
    "    x_test  = z[80:N]\n",
    "else:\n",
    "    x_train = data[0:80]\n",
    "    x_test  = data[80:N]\n",
    "\n",
    "\n",
    "if map_to_grid:\n",
    "    # Print one particular input to check if the discretization to a 28x28 grid went okay\n",
    "    zfig=x_train[1]\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(zfig, interpolation='nearest', cmap='Blues',\n",
    "        extent=(0.5,np.shape(zfig)[0]+0.5,0.5,np.shape(zfig)[1]+0.5))\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "# Make them into 32-bit floats\n",
    "#x_train = x_train.astype('float32') / 255.\n",
    "#x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32') \n",
    "\n",
    "# Create 1-dimensional vector of input, the coordinates are just fed into subsequent input nodes\n",
    "# The (...,-1) is to indicate to Keras that we want to flatten the remaining dimensions to one (product under the hood)\n",
    "# The layers are fully connected, so that is why we map to 1D. More dimensions only make sense if the layers\n",
    "# are not densely connected.\n",
    "x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "input_size = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                12832     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               13200     \n",
      "=================================================================\n",
      "Total params: 26,032\n",
      "Trainable params: 26,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "\n",
    "# this is the size of our encoded representations \n",
    "encoding_dim = 32\n",
    "batch_size = 100\n",
    "epochs = 1000\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(input_size,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu',bias_initializer=initializers.Constant(0.1))(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(input_size, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of optimizers:\n",
    "#  adam\n",
    "#  adagrad\n",
    "#  sgd\n",
    "# types of losses:\n",
    "# - mean_absolute_error (gets all zeros)\n",
    "# - hinge, squared_hinge\n",
    "# - logcosh\n",
    "# - kullback_leibler_divergence (gets all ones)\n",
    "# - binary_crossentropy (adam/adagrad gets somewhere when cast to a grid)\n",
    "# - poisson (as well)\n",
    "# - cosine_proximity (vague)\n",
    "# - mean_squared_logarithmic_error\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1000\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0861 - val_loss: 1.1670\n",
      "Epoch 2/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.9096 - val_loss: 1.0810\n",
      "Epoch 3/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.7407 - val_loss: 0.9984\n",
      "Epoch 4/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: 0.5802 - val_loss: 0.9179\n",
      "Epoch 5/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: 0.4207 - val_loss: 0.8361\n",
      "Epoch 6/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: 0.2606 - val_loss: 0.7540\n",
      "Epoch 7/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.0984 - val_loss: 0.6683\n",
      "Epoch 8/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -0.0674 - val_loss: 0.5807\n",
      "Epoch 9/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -0.2383 - val_loss: 0.4912\n",
      "Epoch 10/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -0.4152 - val_loss: 0.3988\n",
      "Epoch 11/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -0.5993 - val_loss: 0.3004\n",
      "Epoch 12/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -0.7905 - val_loss: 0.1970\n",
      "Epoch 13/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -0.9897 - val_loss: 0.0874\n",
      "Epoch 14/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -1.2000 - val_loss: -0.0296\n",
      "Epoch 15/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -1.4196 - val_loss: -0.1547\n",
      "Epoch 16/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -1.6472 - val_loss: -0.2878\n",
      "Epoch 17/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -1.8849 - val_loss: -0.4300\n",
      "Epoch 18/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: -2.1329 - val_loss: -0.5816\n",
      "Epoch 19/1000\n",
      "80/80 [==============================] - 0s 47us/step - loss: -2.3862 - val_loss: -0.7441\n",
      "Epoch 20/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -2.6489 - val_loss: -0.9186\n",
      "Epoch 21/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -2.9234 - val_loss: -1.1036\n",
      "Epoch 22/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -3.2029 - val_loss: -1.3030\n",
      "Epoch 23/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -3.4917 - val_loss: -1.5188\n",
      "Epoch 24/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -3.7921 - val_loss: -1.7498\n",
      "Epoch 25/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: -4.0981 - val_loss: -1.9937\n",
      "Epoch 26/1000\n",
      "80/80 [==============================] - 0s 79us/step - loss: -4.4130 - val_loss: -2.2524\n",
      "Epoch 27/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -4.7350 - val_loss: -2.5265\n",
      "Epoch 28/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: -5.0588 - val_loss: -2.8150\n",
      "Epoch 29/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -5.3895 - val_loss: -3.1162\n",
      "Epoch 30/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -5.7297 - val_loss: -3.4261\n",
      "Epoch 31/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -6.0787 - val_loss: -3.7459\n",
      "Epoch 32/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -6.4338 - val_loss: -4.0740\n",
      "Epoch 33/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: -6.7951 - val_loss: -4.4142\n",
      "Epoch 34/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -7.1604 - val_loss: -4.7604\n",
      "Epoch 35/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -7.5321 - val_loss: -5.1010\n",
      "Epoch 36/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -7.9098 - val_loss: -5.4429\n",
      "Epoch 37/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -8.2895 - val_loss: -5.7748\n",
      "Epoch 38/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -8.6750 - val_loss: -6.1050\n",
      "Epoch 39/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -9.0604 - val_loss: -6.4253\n",
      "Epoch 40/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -9.4467 - val_loss: -6.7458\n",
      "Epoch 41/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: -9.8334 - val_loss: -7.0578\n",
      "Epoch 42/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -10.2215 - val_loss: -7.3697\n",
      "Epoch 43/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -10.6098 - val_loss: -7.6795\n",
      "Epoch 44/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -10.9949 - val_loss: -7.9906\n",
      "Epoch 45/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -11.3782 - val_loss: -8.2965\n",
      "Epoch 46/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -11.7598 - val_loss: -8.5885\n",
      "Epoch 47/1000\n",
      "80/80 [==============================] - 0s 47us/step - loss: -12.1350 - val_loss: -8.8723\n",
      "Epoch 48/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -12.5063 - val_loss: -9.1511\n",
      "Epoch 49/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -12.8712 - val_loss: -9.4175\n",
      "Epoch 50/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -13.2295 - val_loss: -9.6733\n",
      "Epoch 51/1000\n",
      "80/80 [==============================] - 0s 46us/step - loss: -13.5818 - val_loss: -9.9245\n",
      "Epoch 52/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -13.9266 - val_loss: -10.1562\n",
      "Epoch 53/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -14.2694 - val_loss: -10.3739\n",
      "Epoch 54/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -14.6021 - val_loss: -10.5764\n",
      "Epoch 55/1000\n",
      "80/80 [==============================] - 0s 89us/step - loss: -14.9260 - val_loss: -10.7666\n",
      "Epoch 56/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -15.2422 - val_loss: -10.9458\n",
      "Epoch 57/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -15.5515 - val_loss: -11.1115\n",
      "Epoch 58/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -15.8506 - val_loss: -11.2644\n",
      "Epoch 59/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -16.1447 - val_loss: -11.4033\n",
      "Epoch 60/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -16.4315 - val_loss: -11.5337\n",
      "Epoch 61/1000\n",
      "80/80 [==============================] - 0s 89us/step - loss: -16.7145 - val_loss: -11.6574\n",
      "Epoch 62/1000\n",
      "80/80 [==============================] - 0s 85us/step - loss: -16.9896 - val_loss: -11.7753\n",
      "Epoch 63/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -17.2591 - val_loss: -11.8891\n",
      "Epoch 64/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -17.5181 - val_loss: -11.9946\n",
      "Epoch 65/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -17.7740 - val_loss: -12.0947\n",
      "Epoch 66/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -18.0240 - val_loss: -12.1885\n",
      "Epoch 67/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -18.2693 - val_loss: -12.2747\n",
      "Epoch 68/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -18.5073 - val_loss: -12.3563\n",
      "Epoch 69/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -18.7414 - val_loss: -12.4315\n",
      "Epoch 70/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -18.9700 - val_loss: -12.5056\n",
      "Epoch 71/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -19.1962 - val_loss: -12.5721\n",
      "Epoch 72/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -19.4182 - val_loss: -12.6325\n",
      "Epoch 73/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -19.6372 - val_loss: -12.6891\n",
      "Epoch 74/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -19.8551 - val_loss: -12.7414\n",
      "Epoch 75/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -20.0693 - val_loss: -12.7893\n",
      "Epoch 76/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -20.2820 - val_loss: -12.8350\n",
      "Epoch 77/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -20.4908 - val_loss: -12.8796\n",
      "Epoch 78/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -20.6975 - val_loss: -12.9199\n",
      "Epoch 79/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 60us/step - loss: -20.9005 - val_loss: -12.9590\n",
      "Epoch 80/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -21.1009 - val_loss: -12.9972\n",
      "Epoch 81/1000\n",
      "80/80 [==============================] - 0s 109us/step - loss: -21.2970 - val_loss: -13.0320\n",
      "Epoch 82/1000\n",
      "80/80 [==============================] - 0s 95us/step - loss: -21.4887 - val_loss: -13.0660\n",
      "Epoch 83/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -21.6793 - val_loss: -13.0964\n",
      "Epoch 84/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -21.8675 - val_loss: -13.1263\n",
      "Epoch 85/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -22.0545 - val_loss: -13.1553\n",
      "Epoch 86/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -22.2387 - val_loss: -13.1816\n",
      "Epoch 87/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -22.4243 - val_loss: -13.2090\n",
      "Epoch 88/1000\n",
      "80/80 [==============================] - 0s 126us/step - loss: -22.6074 - val_loss: -13.2335\n",
      "Epoch 89/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -22.7889 - val_loss: -13.2587\n",
      "Epoch 90/1000\n",
      "80/80 [==============================] - 0s 82us/step - loss: -22.9730 - val_loss: -13.2796\n",
      "Epoch 91/1000\n",
      "80/80 [==============================] - 0s 123us/step - loss: -23.1545 - val_loss: -13.2990\n",
      "Epoch 92/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -23.3343 - val_loss: -13.3178\n",
      "Epoch 93/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -23.5132 - val_loss: -13.3356\n",
      "Epoch 94/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -23.6897 - val_loss: -13.3496\n",
      "Epoch 95/1000\n",
      "80/80 [==============================] - 0s 94us/step - loss: -23.8659 - val_loss: -13.3613\n",
      "Epoch 96/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -24.0384 - val_loss: -13.3751\n",
      "Epoch 97/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -24.2122 - val_loss: -13.3874\n",
      "Epoch 98/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -24.3829 - val_loss: -13.4006\n",
      "Epoch 99/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -24.5500 - val_loss: -13.4124\n",
      "Epoch 100/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -24.7187 - val_loss: -13.4230\n",
      "Epoch 101/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -24.8845 - val_loss: -13.4312\n",
      "Epoch 102/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -25.0505 - val_loss: -13.4404\n",
      "Epoch 103/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -25.2134 - val_loss: -13.4514\n",
      "Epoch 104/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -25.3761 - val_loss: -13.4617\n",
      "Epoch 105/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -25.5395 - val_loss: -13.4706\n",
      "Epoch 106/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -25.6994 - val_loss: -13.4776\n",
      "Epoch 107/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -25.8578 - val_loss: -13.4844\n",
      "Epoch 108/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -26.0153 - val_loss: -13.4916\n",
      "Epoch 109/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -26.1719 - val_loss: -13.4980\n",
      "Epoch 110/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -26.3286 - val_loss: -13.5050\n",
      "Epoch 111/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -26.4828 - val_loss: -13.5121\n",
      "Epoch 112/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -26.6389 - val_loss: -13.5196\n",
      "Epoch 113/1000\n",
      "80/80 [==============================] - 0s 100us/step - loss: -26.7931 - val_loss: -13.5252\n",
      "Epoch 114/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -26.9459 - val_loss: -13.5294\n",
      "Epoch 115/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -27.0975 - val_loss: -13.5346\n",
      "Epoch 116/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -27.2507 - val_loss: -13.5392\n",
      "Epoch 117/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -27.4023 - val_loss: -13.5461\n",
      "Epoch 118/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -27.5521 - val_loss: -13.5542\n",
      "Epoch 119/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -27.7045 - val_loss: -13.5616\n",
      "Epoch 120/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -27.8539 - val_loss: -13.5702\n",
      "Epoch 121/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -28.0043 - val_loss: -13.5801\n",
      "Epoch 122/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -28.1547 - val_loss: -13.5888\n",
      "Epoch 123/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -28.3037 - val_loss: -13.5966\n",
      "Epoch 124/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -28.4526 - val_loss: -13.6034\n",
      "Epoch 125/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -28.6004 - val_loss: -13.6102\n",
      "Epoch 126/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -28.7481 - val_loss: -13.6186\n",
      "Epoch 127/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -28.8939 - val_loss: -13.6256\n",
      "Epoch 128/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -29.0401 - val_loss: -13.6322\n",
      "Epoch 129/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -29.1833 - val_loss: -13.6391\n",
      "Epoch 130/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -29.3264 - val_loss: -13.6469\n",
      "Epoch 131/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -29.4686 - val_loss: -13.6561\n",
      "Epoch 132/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -29.6088 - val_loss: -13.6669\n",
      "Epoch 133/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -29.7495 - val_loss: -13.6771\n",
      "Epoch 134/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -29.8894 - val_loss: -13.6866\n",
      "Epoch 135/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: -30.0288 - val_loss: -13.6958\n",
      "Epoch 136/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -30.1662 - val_loss: -13.7054\n",
      "Epoch 137/1000\n",
      "80/80 [==============================] - 0s 85us/step - loss: -30.3047 - val_loss: -13.7151\n",
      "Epoch 138/1000\n",
      "80/80 [==============================] - 0s 85us/step - loss: -30.4408 - val_loss: -13.7256\n",
      "Epoch 139/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -30.5772 - val_loss: -13.7349\n",
      "Epoch 140/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -30.7120 - val_loss: -13.7465\n",
      "Epoch 141/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -30.8454 - val_loss: -13.7568\n",
      "Epoch 142/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -30.9777 - val_loss: -13.7689\n",
      "Epoch 143/1000\n",
      "80/80 [==============================] - 0s 79us/step - loss: -31.1095 - val_loss: -13.7773\n",
      "Epoch 144/1000\n",
      "80/80 [==============================] - 0s 111us/step - loss: -31.2402 - val_loss: -13.7862\n",
      "Epoch 145/1000\n",
      "80/80 [==============================] - 0s 89us/step - loss: -31.3677 - val_loss: -13.7947\n",
      "Epoch 146/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -31.4946 - val_loss: -13.8018\n",
      "Epoch 147/1000\n",
      "80/80 [==============================] - 0s 91us/step - loss: -31.6213 - val_loss: -13.8082\n",
      "Epoch 148/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -31.7464 - val_loss: -13.8161\n",
      "Epoch 149/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -31.8710 - val_loss: -13.8230\n",
      "Epoch 150/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -31.9951 - val_loss: -13.8284\n",
      "Epoch 151/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -32.1171 - val_loss: -13.8344\n",
      "Epoch 152/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -32.2367 - val_loss: -13.8400\n",
      "Epoch 153/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -32.3559 - val_loss: -13.8441\n",
      "Epoch 154/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -32.4766 - val_loss: -13.8488\n",
      "Epoch 155/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -32.5951 - val_loss: -13.8532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: -32.7128 - val_loss: -13.8588\n",
      "Epoch 157/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -32.8288 - val_loss: -13.8647\n",
      "Epoch 158/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -32.9447 - val_loss: -13.8699\n",
      "Epoch 159/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -33.0602 - val_loss: -13.8725\n",
      "Epoch 160/1000\n",
      "80/80 [==============================] - 0s 79us/step - loss: -33.1754 - val_loss: -13.8765\n",
      "Epoch 161/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -33.2909 - val_loss: -13.8811\n",
      "Epoch 162/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -33.4045 - val_loss: -13.8869\n",
      "Epoch 163/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -33.5175 - val_loss: -13.8931\n",
      "Epoch 164/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -33.6310 - val_loss: -13.8984\n",
      "Epoch 165/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -33.7438 - val_loss: -13.9029\n",
      "Epoch 166/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -33.8545 - val_loss: -13.9084\n",
      "Epoch 167/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -33.9659 - val_loss: -13.9137\n",
      "Epoch 168/1000\n",
      "80/80 [==============================] - 0s 86us/step - loss: -34.0744 - val_loss: -13.9184\n",
      "Epoch 169/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: -34.1819 - val_loss: -13.9239\n",
      "Epoch 170/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -34.2890 - val_loss: -13.9279\n",
      "Epoch 171/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -34.3944 - val_loss: -13.9316\n",
      "Epoch 172/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -34.5000 - val_loss: -13.9365\n",
      "Epoch 173/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -34.6021 - val_loss: -13.9403\n",
      "Epoch 174/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -34.7043 - val_loss: -13.9441\n",
      "Epoch 175/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -34.8075 - val_loss: -13.9502\n",
      "Epoch 176/1000\n",
      "80/80 [==============================] - 0s 96us/step - loss: -34.9096 - val_loss: -13.9566\n",
      "Epoch 177/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -35.0103 - val_loss: -13.9646\n",
      "Epoch 178/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -35.1111 - val_loss: -13.9723\n",
      "Epoch 179/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -35.2108 - val_loss: -13.9782\n",
      "Epoch 180/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -35.3091 - val_loss: -13.9829\n",
      "Epoch 181/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -35.4077 - val_loss: -13.9863\n",
      "Epoch 182/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -35.5057 - val_loss: -13.9874\n",
      "Epoch 183/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -35.6020 - val_loss: -13.9886\n",
      "Epoch 184/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -35.6980 - val_loss: -13.9917\n",
      "Epoch 185/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -35.7923 - val_loss: -13.9941\n",
      "Epoch 186/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -35.8856 - val_loss: -13.9969\n",
      "Epoch 187/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: -35.9779 - val_loss: -14.0021\n",
      "Epoch 188/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -36.0682 - val_loss: -14.0033\n",
      "Epoch 189/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -36.1600 - val_loss: -14.0071\n",
      "Epoch 190/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -36.2507 - val_loss: -14.0086\n",
      "Epoch 191/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -36.3404 - val_loss: -14.0094\n",
      "Epoch 192/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -36.4280 - val_loss: -14.0113\n",
      "Epoch 193/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -36.5158 - val_loss: -14.0138\n",
      "Epoch 194/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -36.6026 - val_loss: -14.0174\n",
      "Epoch 195/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -36.6870 - val_loss: -14.0224\n",
      "Epoch 196/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -36.7728 - val_loss: -14.0293\n",
      "Epoch 197/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -36.8565 - val_loss: -14.0378\n",
      "Epoch 198/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: -36.9405 - val_loss: -14.0447\n",
      "Epoch 199/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -37.0227 - val_loss: -14.0535\n",
      "Epoch 200/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -37.1041 - val_loss: -14.0617\n",
      "Epoch 201/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -37.1849 - val_loss: -14.0697\n",
      "Epoch 202/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -37.2651 - val_loss: -14.0785\n",
      "Epoch 203/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -37.3451 - val_loss: -14.0832\n",
      "Epoch 204/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -37.4242 - val_loss: -14.0885\n",
      "Epoch 205/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -37.5029 - val_loss: -14.0928\n",
      "Epoch 206/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -37.5799 - val_loss: -14.0972\n",
      "Epoch 207/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -37.6562 - val_loss: -14.1005\n",
      "Epoch 208/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -37.7318 - val_loss: -14.1040\n",
      "Epoch 209/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -37.8063 - val_loss: -14.1046\n",
      "Epoch 210/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -37.8786 - val_loss: -14.1079\n",
      "Epoch 211/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -37.9520 - val_loss: -14.1148\n",
      "Epoch 212/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -38.0254 - val_loss: -14.1206\n",
      "Epoch 213/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -38.0970 - val_loss: -14.1280\n",
      "Epoch 214/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -38.1683 - val_loss: -14.1364\n",
      "Epoch 215/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -38.2398 - val_loss: -14.1422\n",
      "Epoch 216/1000\n",
      "80/80 [==============================] - 0s 135us/step - loss: -38.3086 - val_loss: -14.1490\n",
      "Epoch 217/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -38.3781 - val_loss: -14.1547\n",
      "Epoch 218/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -38.4461 - val_loss: -14.1603\n",
      "Epoch 219/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -38.5139 - val_loss: -14.1665\n",
      "Epoch 220/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -38.5803 - val_loss: -14.1722\n",
      "Epoch 221/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -38.6475 - val_loss: -14.1767\n",
      "Epoch 222/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -38.7140 - val_loss: -14.1822\n",
      "Epoch 223/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -38.7792 - val_loss: -14.1880\n",
      "Epoch 224/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: -38.8438 - val_loss: -14.1925\n",
      "Epoch 225/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -38.9078 - val_loss: -14.1977\n",
      "Epoch 226/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -38.9718 - val_loss: -14.2055\n",
      "Epoch 227/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -39.0349 - val_loss: -14.2100\n",
      "Epoch 228/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: -39.0970 - val_loss: -14.2157\n",
      "Epoch 229/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -39.1591 - val_loss: -14.2257\n",
      "Epoch 230/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -39.2209 - val_loss: -14.2337\n",
      "Epoch 231/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -39.2818 - val_loss: -14.2401\n",
      "Epoch 232/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -39.3421 - val_loss: -14.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -39.4018 - val_loss: -14.2526\n",
      "Epoch 234/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: -39.4621 - val_loss: -14.2589\n",
      "Epoch 235/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -39.5198 - val_loss: -14.2658\n",
      "Epoch 236/1000\n",
      "80/80 [==============================] - 0s 156us/step - loss: -39.5783 - val_loss: -14.2721\n",
      "Epoch 237/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -39.6346 - val_loss: -14.2771\n",
      "Epoch 238/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -39.6916 - val_loss: -14.2820\n",
      "Epoch 239/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -39.7478 - val_loss: -14.2856\n",
      "Epoch 240/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -39.8038 - val_loss: -14.2902\n",
      "Epoch 241/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -39.8590 - val_loss: -14.2936\n",
      "Epoch 242/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: -39.9147 - val_loss: -14.2966\n",
      "Epoch 243/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -39.9688 - val_loss: -14.3027\n",
      "Epoch 244/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -40.0230 - val_loss: -14.3077\n",
      "Epoch 245/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -40.0776 - val_loss: -14.3127\n",
      "Epoch 246/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -40.1298 - val_loss: -14.3186\n",
      "Epoch 247/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -40.1827 - val_loss: -14.3243\n",
      "Epoch 248/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -40.2348 - val_loss: -14.3294\n",
      "Epoch 249/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -40.2858 - val_loss: -14.3317\n",
      "Epoch 250/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -40.3370 - val_loss: -14.3358\n",
      "Epoch 251/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -40.3881 - val_loss: -14.3403\n",
      "Epoch 252/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: -40.4387 - val_loss: -14.3444\n",
      "Epoch 253/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: -40.4892 - val_loss: -14.3488\n",
      "Epoch 254/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -40.5388 - val_loss: -14.3552\n",
      "Epoch 255/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -40.5869 - val_loss: -14.3601\n",
      "Epoch 256/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -40.6369 - val_loss: -14.3647\n",
      "Epoch 257/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -40.6860 - val_loss: -14.3696\n",
      "Epoch 258/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -40.7343 - val_loss: -14.3752\n",
      "Epoch 259/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -40.7838 - val_loss: -14.3782\n",
      "Epoch 260/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -40.8311 - val_loss: -14.3793\n",
      "Epoch 261/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -40.8801 - val_loss: -14.3795\n",
      "Epoch 262/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -40.9279 - val_loss: -14.3827\n",
      "Epoch 263/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -40.9748 - val_loss: -14.3834\n",
      "Epoch 264/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -41.0208 - val_loss: -14.3870\n",
      "Epoch 265/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -41.0680 - val_loss: -14.3872\n",
      "Epoch 266/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -41.1147 - val_loss: -14.3875\n",
      "Epoch 267/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -41.1600 - val_loss: -14.3898\n",
      "Epoch 268/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -41.2049 - val_loss: -14.3937\n",
      "Epoch 269/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -41.2508 - val_loss: -14.4004\n",
      "Epoch 270/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -41.2948 - val_loss: -14.4045\n",
      "Epoch 271/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -41.3400 - val_loss: -14.4061\n",
      "Epoch 272/1000\n",
      "80/80 [==============================] - 0s 82us/step - loss: -41.3843 - val_loss: -14.4082\n",
      "Epoch 273/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -41.4285 - val_loss: -14.4098\n",
      "Epoch 274/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -41.4724 - val_loss: -14.4138\n",
      "Epoch 275/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -41.5160 - val_loss: -14.4154\n",
      "Epoch 276/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -41.5589 - val_loss: -14.4179\n",
      "Epoch 277/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -41.6016 - val_loss: -14.4219\n",
      "Epoch 278/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -41.6440 - val_loss: -14.4263\n",
      "Epoch 279/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -41.6861 - val_loss: -14.4297\n",
      "Epoch 280/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -41.7269 - val_loss: -14.4312\n",
      "Epoch 281/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -41.7684 - val_loss: -14.4340\n",
      "Epoch 282/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -41.8092 - val_loss: -14.4385\n",
      "Epoch 283/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -41.8491 - val_loss: -14.4430\n",
      "Epoch 284/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: -41.8883 - val_loss: -14.4478\n",
      "Epoch 285/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -41.9286 - val_loss: -14.4525\n",
      "Epoch 286/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -41.9678 - val_loss: -14.4533\n",
      "Epoch 287/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -42.0068 - val_loss: -14.4572\n",
      "Epoch 288/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -42.0450 - val_loss: -14.4613\n",
      "Epoch 289/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -42.0830 - val_loss: -14.4659\n",
      "Epoch 290/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -42.1213 - val_loss: -14.4695\n",
      "Epoch 291/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -42.1588 - val_loss: -14.4720\n",
      "Epoch 292/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: -42.1964 - val_loss: -14.4755\n",
      "Epoch 293/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -42.2335 - val_loss: -14.4785\n",
      "Epoch 294/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -42.2702 - val_loss: -14.4811\n",
      "Epoch 295/1000\n",
      "80/80 [==============================] - 0s 81us/step - loss: -42.3069 - val_loss: -14.4843\n",
      "Epoch 296/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: -42.3428 - val_loss: -14.4881\n",
      "Epoch 297/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: -42.3799 - val_loss: -14.4921\n",
      "Epoch 298/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: -42.4155 - val_loss: -14.4942\n",
      "Epoch 299/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -42.4512 - val_loss: -14.4984\n",
      "Epoch 300/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -42.4861 - val_loss: -14.5047\n",
      "Epoch 301/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: -42.5213 - val_loss: -14.5086\n",
      "Epoch 302/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -42.5546 - val_loss: -14.5112\n",
      "Epoch 303/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: -42.5884 - val_loss: -14.5135\n",
      "Epoch 304/1000\n",
      "80/80 [==============================] - 0s 85us/step - loss: -42.6218 - val_loss: -14.5162\n",
      "Epoch 305/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -42.6546 - val_loss: -14.5203\n",
      "Epoch 306/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -42.6873 - val_loss: -14.5260\n",
      "Epoch 307/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -42.7196 - val_loss: -14.5275\n",
      "Epoch 308/1000\n",
      "80/80 [==============================] - 0s 105us/step - loss: -42.7512 - val_loss: -14.5299\n",
      "Epoch 309/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -42.7831 - val_loss: -14.5342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310/1000\n",
      "80/80 [==============================] - 0s 91us/step - loss: -42.8149 - val_loss: -14.5383\n",
      "Epoch 311/1000\n",
      "80/80 [==============================] - 0s 81us/step - loss: -42.8459 - val_loss: -14.5412\n",
      "Epoch 312/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -42.8765 - val_loss: -14.5443\n",
      "Epoch 313/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -42.9066 - val_loss: -14.5474\n",
      "Epoch 314/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -42.9369 - val_loss: -14.5521\n",
      "Epoch 315/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -42.9666 - val_loss: -14.5561\n",
      "Epoch 316/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -42.9960 - val_loss: -14.5623\n",
      "Epoch 317/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -43.0255 - val_loss: -14.5684\n",
      "Epoch 318/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -43.0554 - val_loss: -14.5739\n",
      "Epoch 319/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -43.0838 - val_loss: -14.5822\n",
      "Epoch 320/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -43.1122 - val_loss: -14.5867\n",
      "Epoch 321/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: -43.1413 - val_loss: -14.5922\n",
      "Epoch 322/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -43.1708 - val_loss: -14.5933\n",
      "Epoch 323/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -43.1994 - val_loss: -14.5984\n",
      "Epoch 324/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -43.2272 - val_loss: -14.6058\n",
      "Epoch 325/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: -43.2553 - val_loss: -14.6109\n",
      "Epoch 326/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -43.2834 - val_loss: -14.6139\n",
      "Epoch 327/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -43.3112 - val_loss: -14.6210\n",
      "Epoch 328/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -43.3395 - val_loss: -14.6270\n",
      "Epoch 329/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -43.3673 - val_loss: -14.6303\n",
      "Epoch 330/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: -43.3944 - val_loss: -14.6348\n",
      "Epoch 331/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -43.4219 - val_loss: -14.6377\n",
      "Epoch 332/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -43.4486 - val_loss: -14.6407\n",
      "Epoch 333/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -43.4758 - val_loss: -14.6455\n",
      "Epoch 334/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -43.5019 - val_loss: -14.6504\n",
      "Epoch 335/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -43.5284 - val_loss: -14.6539\n",
      "Epoch 336/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -43.5546 - val_loss: -14.6578\n",
      "Epoch 337/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: -43.5804 - val_loss: -14.6609\n",
      "Epoch 338/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -43.6058 - val_loss: -14.6653\n",
      "Epoch 339/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -43.6311 - val_loss: -14.6673\n",
      "Epoch 340/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -43.6566 - val_loss: -14.6721\n",
      "Epoch 341/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -43.6815 - val_loss: -14.6779\n",
      "Epoch 342/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -43.7067 - val_loss: -14.6823\n",
      "Epoch 343/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -43.7314 - val_loss: -14.6848\n",
      "Epoch 344/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -43.7558 - val_loss: -14.6867\n",
      "Epoch 345/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -43.7796 - val_loss: -14.6888\n",
      "Epoch 346/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -43.8036 - val_loss: -14.6902\n",
      "Epoch 347/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -43.8273 - val_loss: -14.6950\n",
      "Epoch 348/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -43.8509 - val_loss: -14.6987\n",
      "Epoch 349/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -43.8737 - val_loss: -14.7008\n",
      "Epoch 350/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -43.8963 - val_loss: -14.7023\n",
      "Epoch 351/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -43.9185 - val_loss: -14.7024\n",
      "Epoch 352/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -43.9417 - val_loss: -14.7041\n",
      "Epoch 353/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -43.9636 - val_loss: -14.7075\n",
      "Epoch 354/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -43.9857 - val_loss: -14.7094\n",
      "Epoch 355/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -44.0072 - val_loss: -14.7114\n",
      "Epoch 356/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -44.0290 - val_loss: -14.7155\n",
      "Epoch 357/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -44.0508 - val_loss: -14.7198\n",
      "Epoch 358/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -44.0725 - val_loss: -14.7259\n",
      "Epoch 359/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -44.0935 - val_loss: -14.7311\n",
      "Epoch 360/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -44.1147 - val_loss: -14.7356\n",
      "Epoch 361/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -44.1363 - val_loss: -14.7373\n",
      "Epoch 362/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -44.1571 - val_loss: -14.7378\n",
      "Epoch 363/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -44.1781 - val_loss: -14.7403\n",
      "Epoch 364/1000\n",
      "80/80 [==============================] - 0s 47us/step - loss: -44.1992 - val_loss: -14.7436\n",
      "Epoch 365/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -44.2202 - val_loss: -14.7480\n",
      "Epoch 366/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: -44.2402 - val_loss: -14.7509\n",
      "Epoch 367/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -44.2606 - val_loss: -14.7519\n",
      "Epoch 368/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -44.2805 - val_loss: -14.7552\n",
      "Epoch 369/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -44.3001 - val_loss: -14.7576\n",
      "Epoch 370/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -44.3204 - val_loss: -14.7600\n",
      "Epoch 371/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -44.3397 - val_loss: -14.7624\n",
      "Epoch 372/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -44.3586 - val_loss: -14.7664\n",
      "Epoch 373/1000\n",
      "80/80 [==============================] - 0s 86us/step - loss: -44.3775 - val_loss: -14.7688\n",
      "Epoch 374/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -44.3967 - val_loss: -14.7714\n",
      "Epoch 375/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -44.4153 - val_loss: -14.7749\n",
      "Epoch 376/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: -44.4340 - val_loss: -14.7757\n",
      "Epoch 377/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -44.4519 - val_loss: -14.7766\n",
      "Epoch 378/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -44.4698 - val_loss: -14.7772\n",
      "Epoch 379/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -44.4889 - val_loss: -14.7810\n",
      "Epoch 380/1000\n",
      "80/80 [==============================] - 0s 100us/step - loss: -44.5071 - val_loss: -14.7838\n",
      "Epoch 381/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -44.5245 - val_loss: -14.7881\n",
      "Epoch 382/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -44.5428 - val_loss: -14.7910\n",
      "Epoch 383/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -44.5604 - val_loss: -14.7932\n",
      "Epoch 384/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -44.5775 - val_loss: -14.7942\n",
      "Epoch 385/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -44.5945 - val_loss: -14.7957\n",
      "Epoch 386/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -44.6118 - val_loss: -14.7995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 387/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -44.6291 - val_loss: -14.8020\n",
      "Epoch 388/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -44.6462 - val_loss: -14.8061\n",
      "Epoch 389/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -44.6624 - val_loss: -14.8101\n",
      "Epoch 390/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -44.6791 - val_loss: -14.8157\n",
      "Epoch 391/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -44.6950 - val_loss: -14.8192\n",
      "Epoch 392/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -44.7111 - val_loss: -14.8214\n",
      "Epoch 393/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -44.7270 - val_loss: -14.8219\n",
      "Epoch 394/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -44.7427 - val_loss: -14.8225\n",
      "Epoch 395/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -44.7581 - val_loss: -14.8233\n",
      "Epoch 396/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -44.7734 - val_loss: -14.8252\n",
      "Epoch 397/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -44.7882 - val_loss: -14.8273\n",
      "Epoch 398/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -44.8034 - val_loss: -14.8293\n",
      "Epoch 399/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -44.8194 - val_loss: -14.8329\n",
      "Epoch 400/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -44.8354 - val_loss: -14.8357\n",
      "Epoch 401/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -44.8510 - val_loss: -14.8426\n",
      "Epoch 402/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -44.8659 - val_loss: -14.8475\n",
      "Epoch 403/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -44.8811 - val_loss: -14.8494\n",
      "Epoch 404/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -44.8957 - val_loss: -14.8503\n",
      "Epoch 405/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -44.9107 - val_loss: -14.8511\n",
      "Epoch 406/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -44.9256 - val_loss: -14.8557\n",
      "Epoch 407/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -44.9397 - val_loss: -14.8564\n",
      "Epoch 408/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -44.9531 - val_loss: -14.8594\n",
      "Epoch 409/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -44.9677 - val_loss: -14.8652\n",
      "Epoch 410/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -44.9818 - val_loss: -14.8696\n",
      "Epoch 411/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -44.9952 - val_loss: -14.8730\n",
      "Epoch 412/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: -45.0084 - val_loss: -14.8766\n",
      "Epoch 413/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -45.0225 - val_loss: -14.8792\n",
      "Epoch 414/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -45.0356 - val_loss: -14.8818\n",
      "Epoch 415/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -45.0479 - val_loss: -14.8869\n",
      "Epoch 416/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -45.0606 - val_loss: -14.8900\n",
      "Epoch 417/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -45.0730 - val_loss: -14.8918\n",
      "Epoch 418/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -45.0857 - val_loss: -14.8960\n",
      "Epoch 419/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -45.0979 - val_loss: -14.9026\n",
      "Epoch 420/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -45.1106 - val_loss: -14.9057\n",
      "Epoch 421/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -45.1231 - val_loss: -14.9074\n",
      "Epoch 422/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -45.1353 - val_loss: -14.9093\n",
      "Epoch 423/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -45.1469 - val_loss: -14.9125\n",
      "Epoch 424/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -45.1590 - val_loss: -14.9130\n",
      "Epoch 425/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -45.1710 - val_loss: -14.9135\n",
      "Epoch 426/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -45.1827 - val_loss: -14.9155\n",
      "Epoch 427/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -45.1940 - val_loss: -14.9165\n",
      "Epoch 428/1000\n",
      "80/80 [==============================] - 0s 79us/step - loss: -45.2056 - val_loss: -14.9190\n",
      "Epoch 429/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -45.2175 - val_loss: -14.9210\n",
      "Epoch 430/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -45.2292 - val_loss: -14.9241\n",
      "Epoch 431/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -45.2399 - val_loss: -14.9243\n",
      "Epoch 432/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -45.2508 - val_loss: -14.9238\n",
      "Epoch 433/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -45.2617 - val_loss: -14.9257\n",
      "Epoch 434/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -45.2724 - val_loss: -14.9267\n",
      "Epoch 435/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -45.2835 - val_loss: -14.9278\n",
      "Epoch 436/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -45.2939 - val_loss: -14.9291\n",
      "Epoch 437/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -45.3047 - val_loss: -14.9327\n",
      "Epoch 438/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -45.3155 - val_loss: -14.9386\n",
      "Epoch 439/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -45.3251 - val_loss: -14.9435\n",
      "Epoch 440/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -45.3354 - val_loss: -14.9482\n",
      "Epoch 441/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -45.3449 - val_loss: -14.9497\n",
      "Epoch 442/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -45.3559 - val_loss: -14.9525\n",
      "Epoch 443/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -45.3658 - val_loss: -14.9552\n",
      "Epoch 444/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -45.3760 - val_loss: -14.9571\n",
      "Epoch 445/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -45.3855 - val_loss: -14.9590\n",
      "Epoch 446/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -45.3949 - val_loss: -14.9622\n",
      "Epoch 447/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -45.4050 - val_loss: -14.9641\n",
      "Epoch 448/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -45.4140 - val_loss: -14.9667\n",
      "Epoch 449/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -45.4241 - val_loss: -14.9698\n",
      "Epoch 450/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -45.4335 - val_loss: -14.9686\n",
      "Epoch 451/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -45.4425 - val_loss: -14.9719\n",
      "Epoch 452/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -45.4522 - val_loss: -14.9765\n",
      "Epoch 453/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -45.4611 - val_loss: -14.9806\n",
      "Epoch 454/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -45.4699 - val_loss: -14.9838\n",
      "Epoch 455/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -45.4787 - val_loss: -14.9833\n",
      "Epoch 456/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -45.4878 - val_loss: -14.9866\n",
      "Epoch 457/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -45.4969 - val_loss: -14.9907\n",
      "Epoch 458/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -45.5059 - val_loss: -14.9933\n",
      "Epoch 459/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -45.5146 - val_loss: -14.9956\n",
      "Epoch 460/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -45.5231 - val_loss: -14.9961\n",
      "Epoch 461/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -45.5317 - val_loss: -15.0001\n",
      "Epoch 462/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -45.5404 - val_loss: -15.0016\n",
      "Epoch 463/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -45.5490 - val_loss: -15.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 464/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -45.5572 - val_loss: -15.0034\n",
      "Epoch 465/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -45.5658 - val_loss: -15.0061\n",
      "Epoch 466/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -45.5746 - val_loss: -15.0068\n",
      "Epoch 467/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -45.5833 - val_loss: -15.0080\n",
      "Epoch 468/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -45.5919 - val_loss: -15.0100\n",
      "Epoch 469/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -45.5998 - val_loss: -15.0128\n",
      "Epoch 470/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -45.6083 - val_loss: -15.0184\n",
      "Epoch 471/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -45.6163 - val_loss: -15.0187\n",
      "Epoch 472/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -45.6243 - val_loss: -15.0186\n",
      "Epoch 473/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -45.6326 - val_loss: -15.0184\n",
      "Epoch 474/1000\n",
      "80/80 [==============================] - 0s 83us/step - loss: -45.6409 - val_loss: -15.0192\n",
      "Epoch 475/1000\n",
      "80/80 [==============================] - 0s 81us/step - loss: -45.6491 - val_loss: -15.0205\n",
      "Epoch 476/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -45.6567 - val_loss: -15.0209\n",
      "Epoch 477/1000\n",
      "80/80 [==============================] - 0s 92us/step - loss: -45.6648 - val_loss: -15.0224\n",
      "Epoch 478/1000\n",
      "80/80 [==============================] - 0s 131us/step - loss: -45.6727 - val_loss: -15.0228\n",
      "Epoch 479/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -45.6799 - val_loss: -15.0246\n",
      "Epoch 480/1000\n",
      "80/80 [==============================] - 0s 83us/step - loss: -45.6879 - val_loss: -15.0273\n",
      "Epoch 481/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -45.6956 - val_loss: -15.0280\n",
      "Epoch 482/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -45.7032 - val_loss: -15.0307\n",
      "Epoch 483/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: -45.7112 - val_loss: -15.0332\n",
      "Epoch 484/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -45.7184 - val_loss: -15.0346\n",
      "Epoch 485/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -45.7263 - val_loss: -15.0336\n",
      "Epoch 486/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -45.7337 - val_loss: -15.0333\n",
      "Epoch 487/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -45.7413 - val_loss: -15.0323\n",
      "Epoch 488/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -45.7483 - val_loss: -15.0324\n",
      "Epoch 489/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -45.7556 - val_loss: -15.0337\n",
      "Epoch 490/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -45.7633 - val_loss: -15.0341\n",
      "Epoch 491/1000\n",
      "80/80 [==============================] - 0s 86us/step - loss: -45.7702 - val_loss: -15.0343\n",
      "Epoch 492/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: -45.7771 - val_loss: -15.0348\n",
      "Epoch 493/1000\n",
      "80/80 [==============================] - 0s 82us/step - loss: -45.7846 - val_loss: -15.0361\n",
      "Epoch 494/1000\n",
      "80/80 [==============================] - 0s 125us/step - loss: -45.7919 - val_loss: -15.0363\n",
      "Epoch 495/1000\n",
      "80/80 [==============================] - 0s 79us/step - loss: -45.7992 - val_loss: -15.0377\n",
      "Epoch 496/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -45.8060 - val_loss: -15.0415\n",
      "Epoch 497/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -45.8133 - val_loss: -15.0409\n",
      "Epoch 498/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -45.8203 - val_loss: -15.0383\n",
      "Epoch 499/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -45.8273 - val_loss: -15.0368\n",
      "Epoch 500/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -45.8340 - val_loss: -15.0348\n",
      "Epoch 501/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -45.8405 - val_loss: -15.0366\n",
      "Epoch 502/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -45.8474 - val_loss: -15.0381\n",
      "Epoch 503/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -45.8539 - val_loss: -15.0392\n",
      "Epoch 504/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -45.8601 - val_loss: -15.0404\n",
      "Epoch 505/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -45.8659 - val_loss: -15.0408\n",
      "Epoch 506/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -45.8720 - val_loss: -15.0396\n",
      "Epoch 507/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -45.8784 - val_loss: -15.0389\n",
      "Epoch 508/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -45.8846 - val_loss: -15.0382\n",
      "Epoch 509/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -45.8908 - val_loss: -15.0373\n",
      "Epoch 510/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -45.8967 - val_loss: -15.0363\n",
      "Epoch 511/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -45.9029 - val_loss: -15.0373\n",
      "Epoch 512/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -45.9089 - val_loss: -15.0384\n",
      "Epoch 513/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -45.9150 - val_loss: -15.0409\n",
      "Epoch 514/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -45.9207 - val_loss: -15.0418\n",
      "Epoch 515/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -45.9262 - val_loss: -15.0431\n",
      "Epoch 516/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -45.9317 - val_loss: -15.0441\n",
      "Epoch 517/1000\n",
      "80/80 [==============================] - 0s 87us/step - loss: -45.9370 - val_loss: -15.0426\n",
      "Epoch 518/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -45.9428 - val_loss: -15.0435\n",
      "Epoch 519/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -45.9484 - val_loss: -15.0413\n",
      "Epoch 520/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -45.9532 - val_loss: -15.0405\n",
      "Epoch 521/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -45.9590 - val_loss: -15.0402\n",
      "Epoch 522/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -45.9643 - val_loss: -15.0400\n",
      "Epoch 523/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: -45.9693 - val_loss: -15.0396\n",
      "Epoch 524/1000\n",
      "80/80 [==============================] - 0s 46us/step - loss: -45.9747 - val_loss: -15.0395\n",
      "Epoch 525/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -45.9798 - val_loss: -15.0400\n",
      "Epoch 526/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -45.9846 - val_loss: -15.0403\n",
      "Epoch 527/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -45.9895 - val_loss: -15.0420\n",
      "Epoch 528/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -45.9948 - val_loss: -15.0426\n",
      "Epoch 529/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -45.9999 - val_loss: -15.0427\n",
      "Epoch 530/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.0050 - val_loss: -15.0433\n",
      "Epoch 531/1000\n",
      "80/80 [==============================] - 0s 132us/step - loss: -46.0101 - val_loss: -15.0411\n",
      "Epoch 532/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.0153 - val_loss: -15.0394\n",
      "Epoch 533/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.0200 - val_loss: -15.0386\n",
      "Epoch 534/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.0250 - val_loss: -15.0401\n",
      "Epoch 535/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.0301 - val_loss: -15.0387\n",
      "Epoch 536/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.0346 - val_loss: -15.0395\n",
      "Epoch 537/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.0395 - val_loss: -15.0398\n",
      "Epoch 538/1000\n",
      "80/80 [==============================] - 0s 91us/step - loss: -46.0442 - val_loss: -15.0363\n",
      "Epoch 539/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: -46.0489 - val_loss: -15.0367\n",
      "Epoch 540/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.0536 - val_loss: -15.0377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.0578 - val_loss: -15.0380\n",
      "Epoch 542/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -46.0624 - val_loss: -15.0395\n",
      "Epoch 543/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.0668 - val_loss: -15.0391\n",
      "Epoch 544/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.0715 - val_loss: -15.0386\n",
      "Epoch 545/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.0758 - val_loss: -15.0373\n",
      "Epoch 546/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.0800 - val_loss: -15.0360\n",
      "Epoch 547/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.0843 - val_loss: -15.0370\n",
      "Epoch 548/1000\n",
      "80/80 [==============================] - 0s 125us/step - loss: -46.0886 - val_loss: -15.0370\n",
      "Epoch 549/1000\n",
      "80/80 [==============================] - 0s 174us/step - loss: -46.0928 - val_loss: -15.0390\n",
      "Epoch 550/1000\n",
      "80/80 [==============================] - 0s 81us/step - loss: -46.0973 - val_loss: -15.0409\n",
      "Epoch 551/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.1016 - val_loss: -15.0391\n",
      "Epoch 552/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.1057 - val_loss: -15.0380\n",
      "Epoch 553/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.1096 - val_loss: -15.0368\n",
      "Epoch 554/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.1135 - val_loss: -15.0353\n",
      "Epoch 555/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.1177 - val_loss: -15.0343\n",
      "Epoch 556/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.1217 - val_loss: -15.0357\n",
      "Epoch 557/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -46.1258 - val_loss: -15.0374\n",
      "Epoch 558/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.1298 - val_loss: -15.0370\n",
      "Epoch 559/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.1334 - val_loss: -15.0367\n",
      "Epoch 560/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.1376 - val_loss: -15.0363\n",
      "Epoch 561/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.1414 - val_loss: -15.0392\n",
      "Epoch 562/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.1453 - val_loss: -15.0387\n",
      "Epoch 563/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.1489 - val_loss: -15.0372\n",
      "Epoch 564/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.1532 - val_loss: -15.0352\n",
      "Epoch 565/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: -46.1571 - val_loss: -15.0354\n",
      "Epoch 566/1000\n",
      "80/80 [==============================] - 0s 90us/step - loss: -46.1609 - val_loss: -15.0352\n",
      "Epoch 567/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.1643 - val_loss: -15.0379\n",
      "Epoch 568/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.1681 - val_loss: -15.0389\n",
      "Epoch 569/1000\n",
      "80/80 [==============================] - 0s 90us/step - loss: -46.1720 - val_loss: -15.0378\n",
      "Epoch 570/1000\n",
      "80/80 [==============================] - 0s 105us/step - loss: -46.1755 - val_loss: -15.0364\n",
      "Epoch 571/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.1794 - val_loss: -15.0333\n",
      "Epoch 572/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.1830 - val_loss: -15.0303\n",
      "Epoch 573/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.1863 - val_loss: -15.0302\n",
      "Epoch 574/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.1897 - val_loss: -15.0298\n",
      "Epoch 575/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.1935 - val_loss: -15.0293\n",
      "Epoch 576/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.1971 - val_loss: -15.0289\n",
      "Epoch 577/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.2005 - val_loss: -15.0282\n",
      "Epoch 578/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.2044 - val_loss: -15.0297\n",
      "Epoch 579/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.2079 - val_loss: -15.0295\n",
      "Epoch 580/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.2112 - val_loss: -15.0312\n",
      "Epoch 581/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.2149 - val_loss: -15.0331\n",
      "Epoch 582/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -46.2185 - val_loss: -15.0320\n",
      "Epoch 583/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.2218 - val_loss: -15.0321\n",
      "Epoch 584/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: -46.2255 - val_loss: -15.0318\n",
      "Epoch 585/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.2290 - val_loss: -15.0321\n",
      "Epoch 586/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.2323 - val_loss: -15.0345\n",
      "Epoch 587/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.2360 - val_loss: -15.0350\n",
      "Epoch 588/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.2395 - val_loss: -15.0347\n",
      "Epoch 589/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.2426 - val_loss: -15.0352\n",
      "Epoch 590/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.2458 - val_loss: -15.0345\n",
      "Epoch 591/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.2490 - val_loss: -15.0346\n",
      "Epoch 592/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.2522 - val_loss: -15.0318\n",
      "Epoch 593/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.2553 - val_loss: -15.0310\n",
      "Epoch 594/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.2585 - val_loss: -15.0331\n",
      "Epoch 595/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.2620 - val_loss: -15.0348\n",
      "Epoch 596/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.2652 - val_loss: -15.0357\n",
      "Epoch 597/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.2681 - val_loss: -15.0360\n",
      "Epoch 598/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.2710 - val_loss: -15.0378\n",
      "Epoch 599/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: -46.2743 - val_loss: -15.0401\n",
      "Epoch 600/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.2776 - val_loss: -15.0388\n",
      "Epoch 601/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.2805 - val_loss: -15.0385\n",
      "Epoch 602/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -46.2835 - val_loss: -15.0369\n",
      "Epoch 603/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.2864 - val_loss: -15.0361\n",
      "Epoch 604/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.2896 - val_loss: -15.0361\n",
      "Epoch 605/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.2923 - val_loss: -15.0366\n",
      "Epoch 606/1000\n",
      "80/80 [==============================] - 0s 85us/step - loss: -46.2954 - val_loss: -15.0362\n",
      "Epoch 607/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.2982 - val_loss: -15.0362\n",
      "Epoch 608/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.3009 - val_loss: -15.0377\n",
      "Epoch 609/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.3037 - val_loss: -15.0390\n",
      "Epoch 610/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.3067 - val_loss: -15.0377\n",
      "Epoch 611/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.3097 - val_loss: -15.0355\n",
      "Epoch 612/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.3123 - val_loss: -15.0345\n",
      "Epoch 613/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.3148 - val_loss: -15.0340\n",
      "Epoch 614/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -46.3176 - val_loss: -15.0321\n",
      "Epoch 615/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.3203 - val_loss: -15.0323\n",
      "Epoch 616/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -46.3228 - val_loss: -15.0370\n",
      "Epoch 617/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.3254 - val_loss: -15.0424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 618/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -46.3281 - val_loss: -15.0458\n",
      "Epoch 619/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.3308 - val_loss: -15.0473\n",
      "Epoch 620/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.3333 - val_loss: -15.0467\n",
      "Epoch 621/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.3357 - val_loss: -15.0463\n",
      "Epoch 622/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.3385 - val_loss: -15.0486\n",
      "Epoch 623/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.3409 - val_loss: -15.0509\n",
      "Epoch 624/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.3436 - val_loss: -15.0502\n",
      "Epoch 625/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.3460 - val_loss: -15.0498\n",
      "Epoch 626/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: -46.3485 - val_loss: -15.0506\n",
      "Epoch 627/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.3510 - val_loss: -15.0508\n",
      "Epoch 628/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.3534 - val_loss: -15.0496\n",
      "Epoch 629/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.3558 - val_loss: -15.0497\n",
      "Epoch 630/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.3583 - val_loss: -15.0509\n",
      "Epoch 631/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.3605 - val_loss: -15.0511\n",
      "Epoch 632/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.3629 - val_loss: -15.0518\n",
      "Epoch 633/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.3651 - val_loss: -15.0521\n",
      "Epoch 634/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.3678 - val_loss: -15.0522\n",
      "Epoch 635/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.3699 - val_loss: -15.0511\n",
      "Epoch 636/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.3722 - val_loss: -15.0489\n",
      "Epoch 637/1000\n",
      "80/80 [==============================] - 0s 202us/step - loss: -46.3747 - val_loss: -15.0479\n",
      "Epoch 638/1000\n",
      "80/80 [==============================] - 0s 80us/step - loss: -46.3771 - val_loss: -15.0445\n",
      "Epoch 639/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.3792 - val_loss: -15.0443\n",
      "Epoch 640/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.3814 - val_loss: -15.0436\n",
      "Epoch 641/1000\n",
      "80/80 [==============================] - 0s 47us/step - loss: -46.3838 - val_loss: -15.0466\n",
      "Epoch 642/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.3863 - val_loss: -15.0495\n",
      "Epoch 643/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.3886 - val_loss: -15.0505\n",
      "Epoch 644/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.3909 - val_loss: -15.0494\n",
      "Epoch 645/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.3934 - val_loss: -15.0484\n",
      "Epoch 646/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.3956 - val_loss: -15.0471\n",
      "Epoch 647/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: -46.3976 - val_loss: -15.0466\n",
      "Epoch 648/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.4000 - val_loss: -15.0478\n",
      "Epoch 649/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: -46.4022 - val_loss: -15.0492\n",
      "Epoch 650/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.4043 - val_loss: -15.0491\n",
      "Epoch 651/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.4063 - val_loss: -15.0501\n",
      "Epoch 652/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.4083 - val_loss: -15.0507\n",
      "Epoch 653/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.4103 - val_loss: -15.0533\n",
      "Epoch 654/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.4127 - val_loss: -15.0535\n",
      "Epoch 655/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.4145 - val_loss: -15.0501\n",
      "Epoch 656/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.4165 - val_loss: -15.0482\n",
      "Epoch 657/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.4187 - val_loss: -15.0490\n",
      "Epoch 658/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.4209 - val_loss: -15.0490\n",
      "Epoch 659/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.4228 - val_loss: -15.0474\n",
      "Epoch 660/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: -46.4247 - val_loss: -15.0485\n",
      "Epoch 661/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.4265 - val_loss: -15.0468\n",
      "Epoch 662/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.4286 - val_loss: -15.0475\n",
      "Epoch 663/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.4307 - val_loss: -15.0480\n",
      "Epoch 664/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.4326 - val_loss: -15.0486\n",
      "Epoch 665/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.4348 - val_loss: -15.0488\n",
      "Epoch 666/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.4364 - val_loss: -15.0490\n",
      "Epoch 667/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.4384 - val_loss: -15.0497\n",
      "Epoch 668/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.4405 - val_loss: -15.0496\n",
      "Epoch 669/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.4421 - val_loss: -15.0491\n",
      "Epoch 670/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.4441 - val_loss: -15.0451\n",
      "Epoch 671/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.4457 - val_loss: -15.0432\n",
      "Epoch 672/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.4477 - val_loss: -15.0415\n",
      "Epoch 673/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -46.4495 - val_loss: -15.0398\n",
      "Epoch 674/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.4511 - val_loss: -15.0377\n",
      "Epoch 675/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.4531 - val_loss: -15.0362\n",
      "Epoch 676/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.4548 - val_loss: -15.0354\n",
      "Epoch 677/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.4567 - val_loss: -15.0360\n",
      "Epoch 678/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -46.4585 - val_loss: -15.0381\n",
      "Epoch 679/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.4600 - val_loss: -15.0378\n",
      "Epoch 680/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.4619 - val_loss: -15.0369\n",
      "Epoch 681/1000\n",
      "80/80 [==============================] - 0s 105us/step - loss: -46.4636 - val_loss: -15.0363\n",
      "Epoch 682/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.4656 - val_loss: -15.0331\n",
      "Epoch 683/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.4672 - val_loss: -15.0306\n",
      "Epoch 684/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.4690 - val_loss: -15.0287\n",
      "Epoch 685/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.4706 - val_loss: -15.0278\n",
      "Epoch 686/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.4726 - val_loss: -15.0268\n",
      "Epoch 687/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.4743 - val_loss: -15.0257\n",
      "Epoch 688/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.4757 - val_loss: -15.0252\n",
      "Epoch 689/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: -46.4776 - val_loss: -15.0265\n",
      "Epoch 690/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: -46.4791 - val_loss: -15.0257\n",
      "Epoch 691/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.4809 - val_loss: -15.0232\n",
      "Epoch 692/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.4822 - val_loss: -15.0216\n",
      "Epoch 693/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -46.4842 - val_loss: -15.0206\n",
      "Epoch 694/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -46.4857 - val_loss: -15.0156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 695/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.4872 - val_loss: -15.0134\n",
      "Epoch 696/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.4886 - val_loss: -15.0140\n",
      "Epoch 697/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: -46.4903 - val_loss: -15.0119\n",
      "Epoch 698/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.4918 - val_loss: -15.0109\n",
      "Epoch 699/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.4933 - val_loss: -15.0084\n",
      "Epoch 700/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.4947 - val_loss: -15.0068\n",
      "Epoch 701/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.4964 - val_loss: -15.0046\n",
      "Epoch 702/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.4979 - val_loss: -15.0061\n",
      "Epoch 703/1000\n",
      "80/80 [==============================] - 0s 84us/step - loss: -46.4992 - val_loss: -15.0063\n",
      "Epoch 704/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.5010 - val_loss: -15.0054\n",
      "Epoch 705/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.5024 - val_loss: -15.0046\n",
      "Epoch 706/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.5038 - val_loss: -15.0038\n",
      "Epoch 707/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.5054 - val_loss: -15.0015\n",
      "Epoch 708/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.5068 - val_loss: -14.9973\n",
      "Epoch 709/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.5082 - val_loss: -14.9954\n",
      "Epoch 710/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.5096 - val_loss: -14.9963\n",
      "Epoch 711/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.5112 - val_loss: -14.9956\n",
      "Epoch 712/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -46.5124 - val_loss: -14.9952\n",
      "Epoch 713/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.5138 - val_loss: -14.9941\n",
      "Epoch 714/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.5150 - val_loss: -14.9930\n",
      "Epoch 715/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.5164 - val_loss: -14.9935\n",
      "Epoch 716/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -46.5180 - val_loss: -14.9932\n",
      "Epoch 717/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.5194 - val_loss: -14.9918\n",
      "Epoch 718/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.5209 - val_loss: -14.9910\n",
      "Epoch 719/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.5222 - val_loss: -14.9900\n",
      "Epoch 720/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.5234 - val_loss: -14.9875\n",
      "Epoch 721/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.5247 - val_loss: -14.9830\n",
      "Epoch 722/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.5262 - val_loss: -14.9807\n",
      "Epoch 723/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.5276 - val_loss: -14.9807\n",
      "Epoch 724/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.5287 - val_loss: -14.9807\n",
      "Epoch 725/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.5302 - val_loss: -14.9810\n",
      "Epoch 726/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.5314 - val_loss: -14.9771\n",
      "Epoch 727/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.5327 - val_loss: -14.9744\n",
      "Epoch 728/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.5337 - val_loss: -14.9745\n",
      "Epoch 729/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.5349 - val_loss: -14.9751\n",
      "Epoch 730/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.5360 - val_loss: -14.9734\n",
      "Epoch 731/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.5372 - val_loss: -14.9730\n",
      "Epoch 732/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.5386 - val_loss: -14.9716\n",
      "Epoch 733/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.5397 - val_loss: -14.9713\n",
      "Epoch 734/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.5409 - val_loss: -14.9714\n",
      "Epoch 735/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.5420 - val_loss: -14.9726\n",
      "Epoch 736/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.5431 - val_loss: -14.9733\n",
      "Epoch 737/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.5443 - val_loss: -14.9734\n",
      "Epoch 738/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.5452 - val_loss: -14.9742\n",
      "Epoch 739/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.5465 - val_loss: -14.9751\n",
      "Epoch 740/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.5474 - val_loss: -14.9737\n",
      "Epoch 741/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: -46.5487 - val_loss: -14.9741\n",
      "Epoch 742/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.5499 - val_loss: -14.9742\n",
      "Epoch 743/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.5510 - val_loss: -14.9736\n",
      "Epoch 744/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.5520 - val_loss: -14.9745\n",
      "Epoch 745/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.5530 - val_loss: -14.9755\n",
      "Epoch 746/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.5540 - val_loss: -14.9750\n",
      "Epoch 747/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.5553 - val_loss: -14.9744\n",
      "Epoch 748/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.5564 - val_loss: -14.9751\n",
      "Epoch 749/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.5574 - val_loss: -14.9747\n",
      "Epoch 750/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.5584 - val_loss: -14.9751\n",
      "Epoch 751/1000\n",
      "80/80 [==============================] - 0s 113us/step - loss: -46.5596 - val_loss: -14.9739\n",
      "Epoch 752/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.5606 - val_loss: -14.9733\n",
      "Epoch 753/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.5620 - val_loss: -14.9714\n",
      "Epoch 754/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.5629 - val_loss: -14.9695\n",
      "Epoch 755/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.5640 - val_loss: -14.9694\n",
      "Epoch 756/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.5650 - val_loss: -14.9690\n",
      "Epoch 757/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.5662 - val_loss: -14.9684\n",
      "Epoch 758/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.5673 - val_loss: -14.9690\n",
      "Epoch 759/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.5683 - val_loss: -14.9718\n",
      "Epoch 760/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.5693 - val_loss: -14.9736\n",
      "Epoch 761/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.5704 - val_loss: -14.9749\n",
      "Epoch 762/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.5713 - val_loss: -14.9750\n",
      "Epoch 763/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.5726 - val_loss: -14.9736\n",
      "Epoch 764/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.5737 - val_loss: -14.9717\n",
      "Epoch 765/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.5745 - val_loss: -14.9704\n",
      "Epoch 766/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.5753 - val_loss: -14.9683\n",
      "Epoch 767/1000\n",
      "80/80 [==============================] - 0s 106us/step - loss: -46.5766 - val_loss: -14.9689\n",
      "Epoch 768/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.5776 - val_loss: -14.9711\n",
      "Epoch 769/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -46.5785 - val_loss: -14.9707\n",
      "Epoch 770/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.5797 - val_loss: -14.9712\n",
      "Epoch 771/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.5806 - val_loss: -14.9734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 772/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.5818 - val_loss: -14.9750\n",
      "Epoch 773/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.5829 - val_loss: -14.9756\n",
      "Epoch 774/1000\n",
      "80/80 [==============================] - 0s 123us/step - loss: -46.5839 - val_loss: -14.9737\n",
      "Epoch 775/1000\n",
      "80/80 [==============================] - 0s 47us/step - loss: -46.5849 - val_loss: -14.9716\n",
      "Epoch 776/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.5858 - val_loss: -14.9705\n",
      "Epoch 777/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.5865 - val_loss: -14.9707\n",
      "Epoch 778/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.5878 - val_loss: -14.9726\n",
      "Epoch 779/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.5888 - val_loss: -14.9739\n",
      "Epoch 780/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.5897 - val_loss: -14.9722\n",
      "Epoch 781/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.5906 - val_loss: -14.9700\n",
      "Epoch 782/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.5914 - val_loss: -14.9715\n",
      "Epoch 783/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.5922 - val_loss: -14.9729\n",
      "Epoch 784/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.5930 - val_loss: -14.9728\n",
      "Epoch 785/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.5939 - val_loss: -14.9735\n",
      "Epoch 786/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.5951 - val_loss: -14.9715\n",
      "Epoch 787/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.5960 - val_loss: -14.9708\n",
      "Epoch 788/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.5968 - val_loss: -14.9703\n",
      "Epoch 789/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.5978 - val_loss: -14.9695\n",
      "Epoch 790/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.5984 - val_loss: -14.9690\n",
      "Epoch 791/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.5992 - val_loss: -14.9700\n",
      "Epoch 792/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.6002 - val_loss: -14.9713\n",
      "Epoch 793/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.6010 - val_loss: -14.9727\n",
      "Epoch 794/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6018 - val_loss: -14.9733\n",
      "Epoch 795/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6027 - val_loss: -14.9750\n",
      "Epoch 796/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -46.6035 - val_loss: -14.9766\n",
      "Epoch 797/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6043 - val_loss: -14.9776\n",
      "Epoch 798/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.6052 - val_loss: -14.9766\n",
      "Epoch 799/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.6059 - val_loss: -14.9758\n",
      "Epoch 800/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6065 - val_loss: -14.9736\n",
      "Epoch 801/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -46.6073 - val_loss: -14.9711\n",
      "Epoch 802/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.6081 - val_loss: -14.9684\n",
      "Epoch 803/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6089 - val_loss: -14.9664\n",
      "Epoch 804/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.6098 - val_loss: -14.9673\n",
      "Epoch 805/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.6104 - val_loss: -14.9678\n",
      "Epoch 806/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.6113 - val_loss: -14.9677\n",
      "Epoch 807/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.6119 - val_loss: -14.9679\n",
      "Epoch 808/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.6123 - val_loss: -14.9668\n",
      "Epoch 809/1000\n",
      "80/80 [==============================] - 0s 134us/step - loss: -46.6131 - val_loss: -14.9680\n",
      "Epoch 810/1000\n",
      "80/80 [==============================] - 0s 79us/step - loss: -46.6140 - val_loss: -14.9689\n",
      "Epoch 811/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6146 - val_loss: -14.9664\n",
      "Epoch 812/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.6153 - val_loss: -14.9632\n",
      "Epoch 813/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6159 - val_loss: -14.9637\n",
      "Epoch 814/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.6165 - val_loss: -14.9632\n",
      "Epoch 815/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.6172 - val_loss: -14.9611\n",
      "Epoch 816/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.6179 - val_loss: -14.9615\n",
      "Epoch 817/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.6184 - val_loss: -14.9595\n",
      "Epoch 818/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -46.6192 - val_loss: -14.9609\n",
      "Epoch 819/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6199 - val_loss: -14.9601\n",
      "Epoch 820/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.6204 - val_loss: -14.9590\n",
      "Epoch 821/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.6211 - val_loss: -14.9603\n",
      "Epoch 822/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6216 - val_loss: -14.9611\n",
      "Epoch 823/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6222 - val_loss: -14.9618\n",
      "Epoch 824/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6228 - val_loss: -14.9610\n",
      "Epoch 825/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6235 - val_loss: -14.9583\n",
      "Epoch 826/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.6241 - val_loss: -14.9576\n",
      "Epoch 827/1000\n",
      "80/80 [==============================] - 0s 76us/step - loss: -46.6246 - val_loss: -14.9587\n",
      "Epoch 828/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.6251 - val_loss: -14.9592\n",
      "Epoch 829/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.6257 - val_loss: -14.9600\n",
      "Epoch 830/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -46.6264 - val_loss: -14.9610\n",
      "Epoch 831/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.6271 - val_loss: -14.9619\n",
      "Epoch 832/1000\n",
      "80/80 [==============================] - 0s 46us/step - loss: -46.6277 - val_loss: -14.9616\n",
      "Epoch 833/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.6283 - val_loss: -14.9615\n",
      "Epoch 834/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6289 - val_loss: -14.9616\n",
      "Epoch 835/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.6294 - val_loss: -14.9616\n",
      "Epoch 836/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: -46.6299 - val_loss: -14.9604\n",
      "Epoch 837/1000\n",
      "80/80 [==============================] - 0s 92us/step - loss: -46.6307 - val_loss: -14.9588\n",
      "Epoch 838/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.6311 - val_loss: -14.9597\n",
      "Epoch 839/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.6317 - val_loss: -14.9585\n",
      "Epoch 840/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.6322 - val_loss: -14.9566\n",
      "Epoch 841/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -46.6328 - val_loss: -14.9558\n",
      "Epoch 842/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.6332 - val_loss: -14.9569\n",
      "Epoch 843/1000\n",
      "80/80 [==============================] - 0s 98us/step - loss: -46.6339 - val_loss: -14.9587\n",
      "Epoch 844/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.6346 - val_loss: -14.9603\n",
      "Epoch 845/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.6349 - val_loss: -14.9625\n",
      "Epoch 846/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.6355 - val_loss: -14.9618\n",
      "Epoch 847/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.6361 - val_loss: -14.9602\n",
      "Epoch 848/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.6365 - val_loss: -14.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 849/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.6369 - val_loss: -14.9563\n",
      "Epoch 850/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.6374 - val_loss: -14.9549\n",
      "Epoch 851/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.6379 - val_loss: -14.9540\n",
      "Epoch 852/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.6383 - val_loss: -14.9537\n",
      "Epoch 853/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.6388 - val_loss: -14.9540\n",
      "Epoch 854/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6394 - val_loss: -14.9555\n",
      "Epoch 855/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.6399 - val_loss: -14.9551\n",
      "Epoch 856/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.6404 - val_loss: -14.9545\n",
      "Epoch 857/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.6409 - val_loss: -14.9534\n",
      "Epoch 858/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.6413 - val_loss: -14.9526\n",
      "Epoch 859/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.6418 - val_loss: -14.9522\n",
      "Epoch 860/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.6425 - val_loss: -14.9520\n",
      "Epoch 861/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6429 - val_loss: -14.9519\n",
      "Epoch 862/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.6435 - val_loss: -14.9533\n",
      "Epoch 863/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.6441 - val_loss: -14.9529\n",
      "Epoch 864/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6445 - val_loss: -14.9542\n",
      "Epoch 865/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.6450 - val_loss: -14.9550\n",
      "Epoch 866/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.6455 - val_loss: -14.9547\n",
      "Epoch 867/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.6459 - val_loss: -14.9551\n",
      "Epoch 868/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6464 - val_loss: -14.9583\n",
      "Epoch 869/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6470 - val_loss: -14.9582\n",
      "Epoch 870/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.6476 - val_loss: -14.9561\n",
      "Epoch 871/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.6480 - val_loss: -14.9565\n",
      "Epoch 872/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.6485 - val_loss: -14.9582\n",
      "Epoch 873/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.6489 - val_loss: -14.9606\n",
      "Epoch 874/1000\n",
      "80/80 [==============================] - 0s 77us/step - loss: -46.6495 - val_loss: -14.9611\n",
      "Epoch 875/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.6499 - val_loss: -14.9612\n",
      "Epoch 876/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6503 - val_loss: -14.9595\n",
      "Epoch 877/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6506 - val_loss: -14.9608\n",
      "Epoch 878/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.6513 - val_loss: -14.9611\n",
      "Epoch 879/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.6517 - val_loss: -14.9601\n",
      "Epoch 880/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6521 - val_loss: -14.9580\n",
      "Epoch 881/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.6525 - val_loss: -14.9561\n",
      "Epoch 882/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.6528 - val_loss: -14.9522\n",
      "Epoch 883/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.6533 - val_loss: -14.9492\n",
      "Epoch 884/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: -46.6538 - val_loss: -14.9479\n",
      "Epoch 885/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.6543 - val_loss: -14.9474\n",
      "Epoch 886/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.6547 - val_loss: -14.9481\n",
      "Epoch 887/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6552 - val_loss: -14.9484\n",
      "Epoch 888/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.6555 - val_loss: -14.9480\n",
      "Epoch 889/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.6559 - val_loss: -14.9490\n",
      "Epoch 890/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.6562 - val_loss: -14.9512\n",
      "Epoch 891/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.6567 - val_loss: -14.9526\n",
      "Epoch 892/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6571 - val_loss: -14.9516\n",
      "Epoch 893/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6575 - val_loss: -14.9519\n",
      "Epoch 894/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -46.6579 - val_loss: -14.9534\n",
      "Epoch 895/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.6584 - val_loss: -14.9548\n",
      "Epoch 896/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.6588 - val_loss: -14.9545\n",
      "Epoch 897/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.6591 - val_loss: -14.9543\n",
      "Epoch 898/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.6595 - val_loss: -14.9536\n",
      "Epoch 899/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.6598 - val_loss: -14.9531\n",
      "Epoch 900/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.6603 - val_loss: -14.9534\n",
      "Epoch 901/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.6608 - val_loss: -14.9525\n",
      "Epoch 902/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.6611 - val_loss: -14.9521\n",
      "Epoch 903/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6614 - val_loss: -14.9518\n",
      "Epoch 904/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.6617 - val_loss: -14.9491\n",
      "Epoch 905/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.6620 - val_loss: -14.9491\n",
      "Epoch 906/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.6623 - val_loss: -14.9490\n",
      "Epoch 907/1000\n",
      "80/80 [==============================] - 0s 83us/step - loss: -46.6627 - val_loss: -14.9515\n",
      "Epoch 908/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.6631 - val_loss: -14.9550\n",
      "Epoch 909/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -46.6635 - val_loss: -14.9566\n",
      "Epoch 910/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.6639 - val_loss: -14.9559\n",
      "Epoch 911/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.6642 - val_loss: -14.9562\n",
      "Epoch 912/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6647 - val_loss: -14.9559\n",
      "Epoch 913/1000\n",
      "80/80 [==============================] - 0s 61us/step - loss: -46.6650 - val_loss: -14.9551\n",
      "Epoch 914/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.6653 - val_loss: -14.9548\n",
      "Epoch 915/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.6658 - val_loss: -14.9546\n",
      "Epoch 916/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.6662 - val_loss: -14.9542\n",
      "Epoch 917/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.6665 - val_loss: -14.9527\n",
      "Epoch 918/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6669 - val_loss: -14.9539\n",
      "Epoch 919/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6673 - val_loss: -14.9558\n",
      "Epoch 920/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.6676 - val_loss: -14.9574\n",
      "Epoch 921/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6679 - val_loss: -14.9583\n",
      "Epoch 922/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.6683 - val_loss: -14.9596\n",
      "Epoch 923/1000\n",
      "80/80 [==============================] - 0s 71us/step - loss: -46.6688 - val_loss: -14.9597\n",
      "Epoch 924/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.6692 - val_loss: -14.9588\n",
      "Epoch 925/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6696 - val_loss: -14.9594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 926/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.6699 - val_loss: -14.9592\n",
      "Epoch 927/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.6702 - val_loss: -14.9600\n",
      "Epoch 928/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.6707 - val_loss: -14.9607\n",
      "Epoch 929/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.6710 - val_loss: -14.9599\n",
      "Epoch 930/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.6714 - val_loss: -14.9594\n",
      "Epoch 931/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.6718 - val_loss: -14.9593\n",
      "Epoch 932/1000\n",
      "80/80 [==============================] - 0s 78us/step - loss: -46.6722 - val_loss: -14.9595\n",
      "Epoch 933/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.6726 - val_loss: -14.9582\n",
      "Epoch 934/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -46.6730 - val_loss: -14.9570\n",
      "Epoch 935/1000\n",
      "80/80 [==============================] - 0s 65us/step - loss: -46.6733 - val_loss: -14.9568\n",
      "Epoch 936/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.6737 - val_loss: -14.9577\n",
      "Epoch 937/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6741 - val_loss: -14.9582\n",
      "Epoch 938/1000\n",
      "80/80 [==============================] - 0s 59us/step - loss: -46.6744 - val_loss: -14.9577\n",
      "Epoch 939/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.6748 - val_loss: -14.9572\n",
      "Epoch 940/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.6750 - val_loss: -14.9572\n",
      "Epoch 941/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.6753 - val_loss: -14.9566\n",
      "Epoch 942/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6756 - val_loss: -14.9560\n",
      "Epoch 943/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6760 - val_loss: -14.9567\n",
      "Epoch 944/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.6763 - val_loss: -14.9566\n",
      "Epoch 945/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.6766 - val_loss: -14.9575\n",
      "Epoch 946/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.6770 - val_loss: -14.9591\n",
      "Epoch 947/1000\n",
      "80/80 [==============================] - 0s 53us/step - loss: -46.6773 - val_loss: -14.9591\n",
      "Epoch 948/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.6776 - val_loss: -14.9596\n",
      "Epoch 949/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.6780 - val_loss: -14.9605\n",
      "Epoch 950/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.6783 - val_loss: -14.9611\n",
      "Epoch 951/1000\n",
      "80/80 [==============================] - 0s 50us/step - loss: -46.6786 - val_loss: -14.9602\n",
      "Epoch 952/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.6789 - val_loss: -14.9599\n",
      "Epoch 953/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.6791 - val_loss: -14.9622\n",
      "Epoch 954/1000\n",
      "80/80 [==============================] - 0s 75us/step - loss: -46.6795 - val_loss: -14.9643\n",
      "Epoch 955/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: -46.6797 - val_loss: -14.9643\n",
      "Epoch 956/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.6801 - val_loss: -14.9643\n",
      "Epoch 957/1000\n",
      "80/80 [==============================] - 0s 57us/step - loss: -46.6803 - val_loss: -14.9646\n",
      "Epoch 958/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.6806 - val_loss: -14.9652\n",
      "Epoch 959/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.6809 - val_loss: -14.9638\n",
      "Epoch 960/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6812 - val_loss: -14.9634\n",
      "Epoch 961/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.6814 - val_loss: -14.9633\n",
      "Epoch 962/1000\n",
      "80/80 [==============================] - 0s 68us/step - loss: -46.6817 - val_loss: -14.9642\n",
      "Epoch 963/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6821 - val_loss: -14.9643\n",
      "Epoch 964/1000\n",
      "80/80 [==============================] - 0s 55us/step - loss: -46.6823 - val_loss: -14.9634\n",
      "Epoch 965/1000\n",
      "80/80 [==============================] - 0s 56us/step - loss: -46.6826 - val_loss: -14.9616\n",
      "Epoch 966/1000\n",
      "80/80 [==============================] - 0s 51us/step - loss: -46.6828 - val_loss: -14.9606\n",
      "Epoch 967/1000\n",
      "80/80 [==============================] - 0s 49us/step - loss: -46.6832 - val_loss: -14.9617\n",
      "Epoch 968/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.6835 - val_loss: -14.9617\n",
      "Epoch 969/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6837 - val_loss: -14.9615\n",
      "Epoch 970/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.6840 - val_loss: -14.9613\n",
      "Epoch 971/1000\n",
      "80/80 [==============================] - 0s 95us/step - loss: -46.6841 - val_loss: -14.9610\n",
      "Epoch 972/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.6844 - val_loss: -14.9600\n",
      "Epoch 973/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6847 - val_loss: -14.9612\n",
      "Epoch 974/1000\n",
      "80/80 [==============================] - 0s 52us/step - loss: -46.6849 - val_loss: -14.9638\n",
      "Epoch 975/1000\n",
      "80/80 [==============================] - 0s 48us/step - loss: -46.6853 - val_loss: -14.9652\n",
      "Epoch 976/1000\n",
      "80/80 [==============================] - 0s 54us/step - loss: -46.6856 - val_loss: -14.9663\n",
      "Epoch 977/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.6857 - val_loss: -14.9661\n",
      "Epoch 978/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.6859 - val_loss: -14.9672\n",
      "Epoch 979/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.6862 - val_loss: -14.9661\n",
      "Epoch 980/1000\n",
      "80/80 [==============================] - 0s 69us/step - loss: -46.6864 - val_loss: -14.9657\n",
      "Epoch 981/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6866 - val_loss: -14.9648\n",
      "Epoch 982/1000\n",
      "80/80 [==============================] - 0s 88us/step - loss: -46.6867 - val_loss: -14.9653\n",
      "Epoch 983/1000\n",
      "80/80 [==============================] - 0s 88us/step - loss: -46.6871 - val_loss: -14.9668\n",
      "Epoch 984/1000\n",
      "80/80 [==============================] - 0s 70us/step - loss: -46.6873 - val_loss: -14.9683\n",
      "Epoch 985/1000\n",
      "80/80 [==============================] - 0s 74us/step - loss: -46.6875 - val_loss: -14.9696\n",
      "Epoch 986/1000\n",
      "80/80 [==============================] - 0s 86us/step - loss: -46.6878 - val_loss: -14.9705\n",
      "Epoch 987/1000\n",
      "80/80 [==============================] - 0s 85us/step - loss: -46.6881 - val_loss: -14.9706\n",
      "Epoch 988/1000\n",
      "80/80 [==============================] - 0s 72us/step - loss: -46.6883 - val_loss: -14.9697\n",
      "Epoch 989/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.6885 - val_loss: -14.9686\n",
      "Epoch 990/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6889 - val_loss: -14.9676\n",
      "Epoch 991/1000\n",
      "80/80 [==============================] - 0s 60us/step - loss: -46.6891 - val_loss: -14.9695\n",
      "Epoch 992/1000\n",
      "80/80 [==============================] - 0s 63us/step - loss: -46.6894 - val_loss: -14.9707\n",
      "Epoch 993/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6896 - val_loss: -14.9714\n",
      "Epoch 994/1000\n",
      "80/80 [==============================] - 0s 67us/step - loss: -46.6900 - val_loss: -14.9718\n",
      "Epoch 995/1000\n",
      "80/80 [==============================] - 0s 64us/step - loss: -46.6901 - val_loss: -14.9714\n",
      "Epoch 996/1000\n",
      "80/80 [==============================] - 0s 58us/step - loss: -46.6903 - val_loss: -14.9717\n",
      "Epoch 997/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -46.6905 - val_loss: -14.9731\n",
      "Epoch 998/1000\n",
      "80/80 [==============================] - 0s 62us/step - loss: -46.6906 - val_loss: -14.9756\n",
      "Epoch 999/1000\n",
      "80/80 [==============================] - 0s 66us/step - loss: -46.6909 - val_loss: -14.9773\n",
      "Epoch 1000/1000\n",
      "80/80 [==============================] - 0s 73us/step - loss: -46.6910 - val_loss: -14.9781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faeb770b278>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the autoencoder should learn to map the x_train input data to x_train as output\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode lines\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x576 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 8  # how many samples we will display\n",
    "plt.figure(figsize=(40, 8))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    if map_to_grid:\n",
    "        original = x_test[i].reshape((resX, resY))\n",
    "        plt.imshow(original, interpolation='nearest', cmap='Blues', extent=(0.5,np.shape(original)[0]+0.5,0.5,np.shape(original)[1]+0.5))\n",
    "    else:\n",
    "        original = x_test[i].reshape(round(input_size/2), 2)\n",
    "        plt.scatter(original[:, 0], original[:, 1], cmap='Blues')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    if map_to_grid:\n",
    "        reconstruction = decoded_imgs[i].reshape((resX, resY))\n",
    "        plt.imshow(reconstruction, interpolation='nearest', cmap='Blues', extent=(0.5,np.shape(reconstruction)[0]+0.5,0.5,np.shape(reconstruction)[1]+0.5))\n",
    "    else:\n",
    "        reconstruction = decoded_imgs[i].reshape(round(input_size/2), 2)\n",
    "        plt.scatter(reconstruction[:, 0], reconstruction[:, 1], cmap='Blues')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAARCAYAAAB+ZX8UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAAMhJREFUeJzt2zFqAkEYhuGZ4AWETevYprTIrTyOHslCEExnHVvtTTdewBkE9V8kz9NO8xXLy7Ds5lprAiDGx9gDAP4T0QUIJLoAgUQXIJDoAgSa9A6HYailzIOmwP32h+PYE6CpXk7nWuvnrbNudEuZp81295pV8IDp93LsCdD097P+bZ11o7s/HD3cAE/Uje7ia5Y221XUFribywDvKvf+SMs5n1JKzWsyADeV1jvdbnQBeC6fjAEEEl2AQKILEEh0AQKJLkCgK9jmJk0F//TtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_reconstruction = reconstruction>0.05;\n",
    "norm_fig = plt.imshow(norm_reconstruction, interpolation='nearest', cmap='Blues', extent=(0.5,np.shape(norm_reconstruction)[0]+0.5,0.5,np.shape(norm_reconstruction)[1]+0.5))\n",
    "norm_fig.axes.get_xaxis().set_visible(False)\n",
    "norm_fig.axes.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 22 is out of bounds for axis 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d7602ec0a35c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mz_mu_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mz_mu_base0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_mu_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_select\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mz_mu_base1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_mu_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_select\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mz_mu_base2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_mu_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_select\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 22 is out of bounds for axis 0 with size 20"
     ]
    }
   ],
   "source": [
    "nx = ny = 20\n",
    "x_values = np.linspace(-3, 3, nx)\n",
    "y_values = np.linspace(-3, 3, ny)\n",
    "\n",
    "#z_mu_base = np.random.rand(1, encoding_dim);\n",
    "\n",
    "z_select = np.arange(encoding_dim)\n",
    "np.random.shuffle(z_select)\n",
    "\n",
    "z_mu_all = encoder.predict(x_test)\n",
    "z_mu_base0 = np.reshape(z_mu_all[z_select[0]], (1, encoding_dim))\n",
    "z_mu_base1 = np.reshape(z_mu_all[z_select[1]], (1, encoding_dim))\n",
    "z_mu_base2 = np.reshape(z_mu_all[z_select[2]], (1, encoding_dim))\n",
    "z_mu_base3 = np.reshape(z_mu_all[z_select[3]], (1, encoding_dim))\n",
    "\n",
    "canvas = np.empty((28*ny, 28*nx))\n",
    "for i, yi in enumerate(x_values):\n",
    "    for j, xi in enumerate(y_values):\n",
    "        #z_mu = np.array([[xi, yi]]) # only for 2 dim\n",
    "        # Show interpolations between four randomly chosen digits from the test set \n",
    "        z_mu = z_mu_base0 * ((xi + 3)/6)*(1-(yi + 3)/6) + z_mu_base1 * (1-(xi + 3)/6)*((yi + 3)/6)\n",
    "        z_mu += z_mu_base2 * ((xi + 3)/6)*((yi + 3)/6) + z_mu_base3 * (1-(xi + 3)/6)*(1-(yi + 3)/6)\n",
    "        x_mean = decoder.predict(z_mu)\n",
    "        canvas[(nx-i-1)*28:(nx-i)*28, j*28:(j+1)*28] = x_mean[0].reshape(28, 28)\n",
    "\n",
    "plt.figure(figsize=(8, 10))        \n",
    "Xi, Yi = np.meshgrid(x_values, y_values)\n",
    "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.79425"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The density of the layer with latent variables (not sparse!)\n",
    "encoded_imgs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 3\n",
      "[[ 0.03388221  0.10955057  0.0736325  ...  0.16900167  0.04594845\n",
      "   0.26352867]\n",
      " [ 0.21277002  0.22894491  0.25271934 ... -0.1786572   0.16298749\n",
      "   0.22988069]\n",
      " [ 0.2533826   0.05217274  0.04544273 ...  0.09988996 -0.2047565\n",
      "  -0.1228686 ]\n",
      " ...\n",
      " [ 0.24806689 -0.05300307 -0.02689626 ... -0.00732307 -0.02101317\n",
      "  -0.30315536]\n",
      " [ 0.09297731  0.25394177 -0.10752632 ...  0.06786896  0.16373947\n",
      "   0.10750473]\n",
      " [-0.02477356 -0.1367      0.2053124  ...  0.16467123  0.24538676\n",
      "  -0.25966063]]\n",
      "[ 1.10256881e-01 -1.32695541e-01 -1.34413287e-01  1.33497670e-01\n",
      " -6.31325543e-02  3.56235588e-03 -4.35589813e-02  3.23263817e-02\n",
      " -1.02669299e-02  8.36004596e-03 -1.34691164e-01 -3.81592959e-02\n",
      "  2.93433052e-02  6.67212307e-02 -1.23187050e-01 -1.34887919e-01\n",
      " -1.04054064e-01 -1.06255591e-01 -8.91876817e-02  1.96360782e-01\n",
      " -2.31309906e-02 -2.76789013e-02 -1.11116007e-01 -1.42535726e-02\n",
      " -8.52111727e-02 -1.07177004e-01 -8.42325091e-02 -4.91375960e-02\n",
      " -9.08282548e-02 -7.60511532e-02 -1.24403633e-01 -6.59258571e-03\n",
      " -6.23948537e-02 -9.44658648e-03 -1.29493564e-01  1.65038824e-01\n",
      " -1.40877917e-01 -1.19710248e-02 -7.50495344e-02 -3.06892674e-02\n",
      "  1.83068831e-02 -8.78357813e-02 -1.23608649e-01  4.58599888e-02\n",
      "  1.55753773e-02 -1.14959016e-01 -1.26990881e-02 -2.16493867e-02\n",
      " -2.20334362e-02 -7.51978997e-03 -1.45567730e-01 -7.75555372e-02\n",
      " -7.16882816e-04 -7.85979182e-02 -2.18441077e-02  8.06670263e-02\n",
      " -9.09679830e-02  6.37960956e-02 -2.42348462e-01  1.12144751e-02\n",
      " -9.92941409e-02  3.99912708e-02 -1.35787502e-01  6.04301356e-02\n",
      "  1.77239940e-01  2.38003016e-01  5.62454984e-02 -3.19869667e-02\n",
      " -4.76492867e-02  2.00432226e-01  8.59661698e-02 -1.23745471e-01\n",
      " -4.41284478e-03 -1.19701950e-02 -6.39207959e-02 -8.94041955e-02\n",
      " -7.05588162e-02 -1.20491199e-01 -1.90361395e-01  8.56008381e-02\n",
      "  1.52044222e-01 -7.30191320e-02 -5.69829531e-02  8.75468776e-02\n",
      "  3.89849320e-02  1.14864931e-01 -1.38169840e-01 -4.82227132e-02\n",
      " -1.85551681e-02 -1.10130727e-01 -2.25149486e-02 -1.02871321e-01\n",
      " -6.88784346e-02  2.76349625e-03  1.49724241e-02  1.74841255e-01\n",
      "  3.53905074e-02 -1.58228636e-01 -9.63122547e-02 -9.99589413e-02\n",
      " -9.94741470e-02 -1.72155350e-01 -8.18006992e-02 -4.14323658e-02\n",
      " -1.25480279e-01  1.01230428e-01  2.95238122e-02 -5.29890396e-02\n",
      " -7.81782568e-02  4.43185903e-02 -8.33982676e-02  7.74399862e-02\n",
      "  1.21154092e-01 -1.09925136e-01 -1.26016706e-01 -9.33396220e-02\n",
      "  7.76010379e-02  3.91086526e-02 -4.08210792e-02  1.16104651e-02\n",
      "  7.27917207e-03  8.42620954e-02 -1.30049663e-03  6.09379783e-02\n",
      " -4.58313944e-03  3.94515507e-02 -1.08675800e-01 -6.16142079e-02\n",
      " -5.48764989e-02 -6.19991906e-02  1.16616925e-02  2.15651654e-02\n",
      "  1.72460780e-01 -9.52177346e-02  1.44455522e-01 -9.88293514e-02\n",
      " -1.54578283e-01  7.88295567e-02 -5.20514175e-02 -4.65209894e-02\n",
      " -4.23203185e-02  1.34238169e-01 -1.17520010e-02 -1.31472543e-01\n",
      "  6.20295443e-02 -1.11173816e-01  5.10085039e-02 -8.53756219e-02\n",
      " -4.59115431e-02 -8.12119171e-02  7.38174049e-03 -5.11373654e-02\n",
      " -7.49079064e-02 -1.82497010e-01 -6.21269904e-02 -1.83590576e-01\n",
      " -6.08863421e-02 -1.86923407e-02 -1.18024498e-02 -7.45395571e-02\n",
      " -7.75443390e-02  1.26761973e-01 -2.11190552e-01  9.62459818e-02\n",
      " -1.18968293e-01  2.49893725e-01  1.10695779e-01  8.82939473e-02\n",
      " -6.46649003e-02 -1.68306962e-01 -9.67032555e-03  1.50343915e-02\n",
      " -1.18216261e-01  2.53788456e-02 -4.70390096e-02 -2.77638901e-02\n",
      " -3.89786959e-02 -3.80281359e-02 -1.36803314e-01  1.54120624e-01\n",
      " -4.05949578e-02  7.04547688e-02 -8.35446045e-02 -6.90217316e-02\n",
      " -9.56008956e-02 -2.41307840e-01 -6.44254833e-02 -1.46608606e-01\n",
      "  1.09876290e-01  1.29726231e-01  7.37971021e-03 -1.60127461e-01\n",
      "  5.13652712e-02 -5.31740040e-02  1.13709517e-01  1.07090259e-02\n",
      " -1.06871895e-01  1.14100084e-01  1.06279023e-01 -1.40622362e-01\n",
      " -1.14432886e-01 -2.08157226e-01 -3.36707719e-02  6.44138008e-02\n",
      " -2.23964509e-02 -8.92801136e-02 -1.45610362e-01  6.85638636e-02\n",
      "  7.20268786e-02 -4.20911759e-02  2.12916136e-01  1.36823654e-01\n",
      "  1.23847738e-01 -1.67794749e-01 -2.44416837e-02 -1.08895479e-02\n",
      "  1.82510599e-01 -1.47744820e-01 -6.18268400e-02 -2.77824253e-02\n",
      " -1.89559776e-02 -2.04608887e-01 -3.10450912e-01 -1.17388949e-01\n",
      "  4.94316518e-02 -2.13570073e-01 -7.89517537e-02 -8.40686709e-02\n",
      "  1.44080982e-01  1.09350912e-01 -6.97907656e-02  8.25547893e-03\n",
      "  6.14727959e-02  1.50418773e-01 -4.65228558e-02 -1.04306012e-01\n",
      " -1.47310104e-02  1.10649727e-01 -8.59116390e-03 -1.33741930e-01\n",
      " -2.06737950e-01 -1.51681140e-01 -2.79298816e-02 -6.98846728e-02\n",
      " -1.00969367e-01  5.61517328e-02  1.40144406e-02 -2.97661554e-02\n",
      " -2.72129141e-02 -9.82694477e-02 -7.31839240e-02  1.01522535e-01\n",
      "  7.77280033e-02 -6.79939166e-02 -2.25328747e-02 -1.28654853e-01\n",
      "  2.04845935e-01 -1.20107114e-01 -5.37716858e-02  1.97253060e-02\n",
      "  2.28332188e-02  2.06680857e-02 -8.54814425e-02 -2.37920377e-02\n",
      "  6.28846958e-02  2.42615193e-02 -1.46910235e-01 -2.05219463e-01\n",
      " -5.51187284e-02 -1.04263037e-01 -1.01582639e-01  5.07790642e-03\n",
      " -9.92079526e-02 -1.49130359e-01 -6.45569116e-02  1.67896792e-01\n",
      " -1.97094023e-01 -2.09190859e-03 -6.77539483e-02  4.59536836e-02\n",
      " -2.89553665e-02 -2.91498512e-01 -5.92526570e-02 -3.27443592e-02\n",
      " -2.05970615e-01  1.68568566e-02 -1.05002023e-01 -2.11166695e-01\n",
      " -2.38738898e-02  1.56939477e-01 -4.99853455e-02  7.55436346e-02\n",
      "  2.72476971e-02 -9.03264657e-02 -2.12283619e-02  3.54846828e-02\n",
      "  6.28455281e-02 -6.16921261e-02  1.13901213e-01  3.61661576e-02\n",
      " -4.89210784e-02  4.91906218e-02 -5.93849691e-03 -6.86305985e-02\n",
      " -1.47942320e-01  1.81426376e-01 -2.57411242e-01 -1.24792330e-01\n",
      " -6.44554794e-02  3.49219628e-02 -3.06595992e-02 -4.06942479e-02\n",
      " -5.94000856e-04 -7.05204383e-02 -8.67403969e-02 -1.34640202e-01\n",
      "  2.63504975e-04 -7.32532069e-02 -2.74129119e-02 -1.21653095e-01\n",
      " -1.14062898e-01 -1.63208619e-01 -1.33651599e-01 -6.24802373e-02\n",
      " -1.22615874e-01  2.82501634e-02 -1.08796492e-01 -3.28500271e-02\n",
      "  1.52987719e-01 -1.36237636e-01  2.15427414e-01 -1.64987128e-02\n",
      " -1.66340843e-01 -3.23807374e-02 -7.01030567e-02  1.20445386e-01\n",
      "  1.04766734e-01  8.20343867e-02  3.52011845e-02 -5.39704487e-02\n",
      "  5.52583672e-02 -3.37522998e-02  1.74386382e-01 -4.62163389e-02\n",
      " -4.85030189e-02 -4.65906672e-02 -9.66127589e-02 -3.35832052e-02\n",
      " -1.02157474e-01 -1.83029950e-01 -3.25132981e-02 -9.81375277e-02\n",
      " -7.05554709e-02 -1.57808334e-01 -6.16536215e-02  6.85052574e-02\n",
      " -1.31197870e-01  1.45713046e-01  1.03315553e-02 -1.24584856e-02\n",
      " -1.32728592e-01  8.02779011e-03  1.38455495e-01 -9.66476798e-02\n",
      "  1.35219768e-02 -4.66467105e-02 -6.51752353e-02  2.75828298e-02\n",
      " -6.26615435e-02 -4.13764268e-02 -8.55206326e-02 -5.27552441e-02\n",
      " -4.41437475e-02 -5.99372126e-02 -2.69320369e-01  4.70100865e-02\n",
      " -1.73106920e-02 -2.19791085e-01 -5.10850847e-02 -1.86035827e-01\n",
      " -6.40249625e-02 -3.17970552e-02  1.64062023e-01  7.90540054e-02\n",
      " -7.69425109e-02  9.03519429e-03 -1.45120129e-01 -1.91328049e-01\n",
      " -1.65638551e-01 -7.46750180e-03 -1.29834667e-01 -3.85111943e-02\n",
      " -1.81162149e-01  4.97912429e-03 -1.35133728e-01  5.08666374e-02\n",
      " -5.56017347e-02  1.64867640e-02  7.76646808e-02 -9.75730494e-02]\n"
     ]
    }
   ],
   "source": [
    "# First layer [0] is empty array []. Second [1] and third [2] layers have weights and biases.\n",
    "print(\"Number of layers:\", len(autoencoder.layers))\n",
    "weights = autoencoder.layers[2].get_weights()[0]\n",
    "biases = autoencoder.layers[2].get_weights()[1]\n",
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
